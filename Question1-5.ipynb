{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import nltk\n",
    "import tqdm\n",
    "import itertools\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import os.path\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_wiki = os.listdir(\"data_files/wiki-pages/wiki-pages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/109 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/109 [00:03<07:11,  4.00s/it]\u001b[A\n",
      "  2%|▏         | 2/109 [00:08<07:15,  4.07s/it]\u001b[A\n",
      "  3%|▎         | 3/109 [00:11<06:57,  3.94s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "#Counting frequency of every term\n",
    "word_count = {}\n",
    "\n",
    "brackets = ['lrd', 'rrb', 'lsb', 'rsb']\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "    with open('wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "            for iline, line in enumerate(openfile.readlines()):\n",
    "                text = json.loads(line)['text']\n",
    "                text = text.lower()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "\n",
    "                for token in tokens:\n",
    "                    if token not in brackets:\n",
    "                        if token in word_count:\n",
    "                            word_count[token] += 1\n",
    "\n",
    "                        else:\n",
    "                            word_count[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_count, open(\"pkl_files/word_count.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pickle.load( open( \"pkl_files/word_count.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457271337"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell to calculate total number of words\n",
    "\n",
    "word_ranks = list(word_count.items())\n",
    "word_ranks.sort(key=lambda tup: tup[1], reverse = True) \n",
    "word_ranks\n",
    "\n",
    "total = sum(word_count.values())\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6.728126937026888),\n",
       " ('of', 3.5237113495263754),\n",
       " ('in', 3.1868513989102274),\n",
       " ('and', 2.7821435481752053),\n",
       " ('a', 2.385506179233797)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell to calculate proportion of most frequent words\n",
    "\n",
    "prop_list = []\n",
    "\n",
    "for word, count in word_ranks[:5]:\n",
    "    prop = (count/total) * 100\n",
    "    prop_list.append((word, prop))\n",
    "    \n",
    "prop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting curve for zipfs law\n",
    "counts = list(word_count.values())\n",
    "counts.sort(reverse = True)\n",
    "\n",
    "#Making a log version of lists\n",
    "log_freq = []\n",
    "log_ranks = []\n",
    "\n",
    "for count in counts:\n",
    "    log_freq.append( math.log(count) )\n",
    "    \n",
    "for rank in range(1,len(log_freq) + 1):\n",
    "    log_ranks.append(math.log(rank))\n",
    " \n",
    "\n",
    "f = plt.figure()\n",
    "plt.plot(log_ranks, log_freq)\n",
    "plt.xlabel('Log(Rank)')\n",
    "plt.ylabel('Log(Frequency)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.977259523130932"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating value of constant K\n",
    "\n",
    "log_sums = []\n",
    "\n",
    "for idx, val in enumerate(log_freq):\n",
    "\n",
    "    item_sum = log_freq[idx] + log_ranks[idx]\n",
    "    log_sums.append(item_sum)\n",
    "    \n",
    "avg_k = sum(log_sums)/len(log_sums)\n",
    "avg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3195517.239955811"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(avg_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3300000, 0, 30]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting K across range of values\n",
    "\n",
    "plt.plot(log_sums)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('K')\n",
    "plt.tick_params(\n",
    "    axis='x',          \n",
    "    which='both',      \n",
    "    bottom=False,      \n",
    "    top=False,         \n",
    "    labelbottom=False) \n",
    "plt.axis([0, 3300000, 0, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Making a list of all words in the claims and stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "\n",
    "ten_claims =  [75397,150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253]\n",
    "claim_string = []\n",
    "\n",
    "with open('data_files/train.jsonl') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "        \n",
    "            claim_dic = json.loads(line)\n",
    "            ID = claim_dic['id']\n",
    "            \n",
    "            \n",
    "            if ID in ten_claims:\n",
    "                claim_string.append(claim_dic['claim'])\n",
    "                text = claim_dic['claim'].lower()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                \n",
    "                claims.append(tokens)\n",
    "                \n",
    "#All claim words\n",
    "all_words = []\n",
    "\n",
    "for claim in claims:\n",
    "    for word in claim:\n",
    "        all_words.append(word)\n",
    "        \n",
    "#Removing stop words from claims\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_words = [word for word in all_words if not word in stop_words] \n",
    "all_words = [stemmer.stem(word) for word in all_words]\n",
    "\n",
    "all_words = list(set(all_words))\n",
    "all_words.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing TF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_matrix = np.zeros((total_doc, len(all_words)))\n",
    "doc_lengths = []\n",
    "ids = []\n",
    "current_doc = 0\n",
    "\n",
    "brackets = ['lrb', 'rrb', 'lsb', 'rsb', 'rcb', 'lcb']\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    \n",
    "                    text = json.loads(line)['text']\n",
    "                    text = text.lower()\n",
    "                    tokens = tokenizer.tokenize(text)\n",
    "                    \n",
    "                    #Removing stop words\n",
    "                    tokens = [word for word in tokens if not word in stop_words] \n",
    "                    \n",
    "                    #Removing brackets manually\n",
    "                    tokens = [word for word in tokens if not word in brackets] \n",
    "                    \n",
    "                    #Stemming\n",
    "                    tokens = [stemmer.stem(word) for word in tokens]\n",
    "                    \n",
    "                    #Appending doc_lengths\n",
    "                    doc_lengths.append(len(tokens))\n",
    "                    \n",
    "                    #Count the words in documents\n",
    "                    for ind,word in enumerate(all_words):\n",
    "                        if word in set(tokens):\n",
    "                            word_tf = tokens.count(word)/len(tokens)\n",
    "                            tf_matrix[current_doc][ind] = word_tf\n",
    "                            \n",
    "                            \n",
    "                    ids.append(json.loads(line)['id'])\n",
    "                    current_doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading in files to save time in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ids, open(\"pkl_files/ids.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pickle.load( open( \"pkl_files/ids.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(doc_lengths, open(\"pkl_files/doc_lengths.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = pickle.load( open( \"pkl_files/doc_lengths.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pkl_files/tf_matrix.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_out = pickle.dumps(tf_matrix)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, len(bytes_out), max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pkl_files/tf_matrix.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_in = bytearray(0)\n",
    "input_size = os.path.getsize(file_path)\n",
    "with open(file_path, 'rb') as f_in:\n",
    "    for _ in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "tf_matrix = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_doc = tf_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making IDF matrix\n",
    "\n",
    "#Counting number of documents with each term in it\n",
    "idf_matrix = np.zeros(len(all_words))\n",
    "\n",
    "for index in range(0, len(all_words)):\n",
    "    doc_count = np.count_nonzero(tf_matrix[:,index])\n",
    "    idf_matrix[index] = math.log10(total_doc/doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_matrix*idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing TF-IDF representation of claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_claims = []\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for claim in claims:\n",
    "    clean_claim = [word for word in claim if not word in stop_words] \n",
    "    clean_claim = [stemmer.stem(word) for word in clean_claim]\n",
    "    clean_claims.append(clean_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf for every claim/norm adjusted\n",
    "claims_tf_idfs = []\n",
    "\n",
    "for claim in clean_claims:\n",
    "    claim_tf = np.zeros(len(all_words))\n",
    "    \n",
    "    for idx, word in enumerate(all_words):\n",
    "        claim_tf[idx] = claim.count(word)/len(claim)\n",
    "            \n",
    "    claim_tf_idf = claim_tf*idf_matrix\n",
    "    claims_tf_idfs.append(claim_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Norm adjusting the matrices\n",
    "norm_rows_docs = []\n",
    "norm_rows_claims = []\n",
    "\n",
    "for row in tf_idf:\n",
    "    norm_doc = np.linalg.norm(row)\n",
    "    norm_adj_row = row\n",
    "    \n",
    "    if norm_doc!= 0:\n",
    "        norm_adj_row = norm_adj_row/norm_doc\n",
    "    \n",
    "    norm_rows_docs.append(norm_adj_row)\n",
    "        \n",
    "for claim in claims_tf_idfs:\n",
    "    norm_claim = np.linalg.norm(claim)\n",
    "    norm_adj_claim = claim/norm_claim\n",
    "    norm_rows_claims.append(norm_adj_claim)\n",
    "    \n",
    "doc_norm_tf_idf = np.array(norm_rows_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving 5 highest for each claim\n",
    "top5_list = []\n",
    "\n",
    "for claim in norm_rows_claims:\n",
    "    cosine_similarity = doc_norm_tf_idf@claim \n",
    "    highest_5_row = cosine_similarity.argsort()[-5:][::1]\n",
    "    \n",
    "    highest_5 = []\n",
    "    for ind in highest_5_row:\n",
    "        highest_5.append(ids[ind])\n",
    "        \n",
    "    highest_5.reverse()    \n",
    "    top5_list.append(highest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_disp_list = list(zip(claim_string, top5_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Likelihood Model\n",
    "\n",
    "unigram_similar = []\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        vec = tf_matrix[:, position]\n",
    "        probabs = probabs*vec\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    non_zero_unigram = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        if probabs[ind]!=0:\n",
    "            non_zero_unigram.append(ids[ind])\n",
    "    \n",
    "    non_zero_unigram.reverse()\n",
    "    unigram_similar.append((claim, non_zero_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 5, 5, 1, 1, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating returned unigrams\n",
    "returned_unigram = []\n",
    "\n",
    "for tup in unigram_similar:\n",
    "    returned_unigram.append(len(tup[1]))\n",
    "    \n",
    "returned_unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_similar = []\n",
    "#Creating array of doc lengths to adjust tf matrix back to just count\n",
    "doc_length_array = np.array(doc_lengths)\n",
    "\n",
    "#Creating new denominator to divide by\n",
    "adj_denom = doc_length_array + len(counts)\n",
    "adj_denom = np.reciprocal(adj_denom.astype(np.float32))\n",
    "\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        \n",
    "        #Required modification to tf matrix\n",
    "        vec = tf_matrix[:, position]\n",
    "        term_freq = vec*doc_length_array\n",
    "        \n",
    "        term_freq_plus1 = term_freq + 1\n",
    "        term_freq_div = term_freq_plus1*adj_denom\n",
    "        \n",
    "        #Calculating probabilities like with mle\n",
    "        probabs = probabs*term_freq_div\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    #Retrieving top5 artilces\n",
    "    top5_laplace = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        top5_laplace.append(ids[ind])\n",
    "    \n",
    "    top5_laplace.reverse()\n",
    "    laplace_similar.append((claim, top5_laplace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building necessary vector to represent entire corpus\n",
    "\n",
    "#Total words\n",
    "total_words = sum(counts)\n",
    "\n",
    "#Making a stemmed word dictionary\n",
    "word_count_list = list(word_count.items())\n",
    "\n",
    "stem_word_count = {}\n",
    "\n",
    "for word_tuple in word_count_list:\n",
    "    word = word_tuple[0]\n",
    "    count = word_tuple[1]\n",
    "    \n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    \n",
    "    if stemmed_word in stem_word_count:\n",
    "        stem_word_count[stemmed_word] += count\n",
    "        \n",
    "    else:\n",
    "        stem_word_count[stemmed_word] = count\n",
    "        \n",
    "claim_word_counts = []\n",
    "\n",
    "for word in all_words:\n",
    "    claim_word_counts.append(stem_word_count[word])\n",
    "    \n",
    "claim_words_array = np.array(claim_word_counts)\n",
    "claim_words_array_div = claim_words_array/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting lambda value\n",
    "lambda_val = 0.5\n",
    "\n",
    "#Calculating jelinek mercer similarity\n",
    "jelinek_similar = []\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        vec = tf_matrix[:, position]\n",
    "        \n",
    "        #jelinek mercer smoothing\n",
    "        lam_vec = lambda_val*vec\n",
    "        corpus_vec = lam_vec + ((1-lambda_val)*claim_words_array_div[all_words.index(word)])\n",
    "        \n",
    "        probabs = probabs*corpus_vec\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    jelinek_similar_claim = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        jelinek_similar_claim.append(ids[ind])\n",
    "    \n",
    "    jelinek_similar_claim.reverse()\n",
    "    jelinek_similar.append((claim, jelinek_similar_claim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building modified lambda value for dirichlet\n",
    "avg_length = total_words/ tf_matrix.shape[0]\n",
    "\n",
    "dirich_denom = doc_length_array + avg_length\n",
    "dirich_denom = np.reciprocal(dirich_denom.astype(np.float32))\n",
    "\n",
    "dirich_constant = doc_length_array*dirich_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_similar = []\n",
    "\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        vec = tf_matrix[:, position]\n",
    "        \n",
    "        \n",
    "        dirich_vec = dirich_constant*vec\n",
    "        corpus_dirich_vec = dirich_vec + ((1-dirich_constant)*claim_words_array_div[all_words.index(word)])\n",
    "        \n",
    "        probabs = probabs*corpus_dirich_vec\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    dirich_similar_claim = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        dirich_similar_claim.append(ids[ind])\n",
    "    \n",
    "    dirich_similar_claim.reverse()\n",
    "    dirichlet_similar.append((claim, dirich_similar_claim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending output to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_claims =  [75397,150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253]\n",
    "\n",
    "def csv_sender(results, ten_claims):\n",
    "    \n",
    "    list_of_lists =[]\n",
    "    \n",
    "    for result in results:\n",
    "        list_of_lists.append(result[1])\n",
    "        \n",
    "    df = pd.DataFrame(list_of_lists)\n",
    "    \n",
    "    df.columns = ['doc id_1', 'doc id_2', 'doc id_3', 'doc id_4','doc id_5' ]\n",
    "    df.index = ten_claims\n",
    "    df.index.name = 'claim id'\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "csv_sender(claim_disp_list, ten_claims).to_csv('csv_files/cosine.csv')\n",
    "csv_sender(unigram_similar, ten_claims).to_csv('csv_files/unigram.csv')\n",
    "csv_sender(laplace_similar, ten_claims).to_csv('csv_files/laplace.csv')\n",
    "csv_sender(jelinek_similar, ten_claims).to_csv('csv_files/jelinek.csv')\n",
    "csv_sender(dirichlet_similar, ten_claims).to_csv('csv_files/dirichlet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "#Loading in the glove model\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel('glove.6B.50D.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in document lines for specific claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['boston', 'celtic', 'play', 'home', 'game', 'td', 'garden'],\n",
       " ['TD_Garden',\n",
       "  'Boston_Garden',\n",
       "  'List_of_Boston_Celtics_head_coaches',\n",
       "  'Boston_Celtics',\n",
       "  'The_Sports_Museum'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting the claim to check\n",
    "claim_to_check = dirichlet_similar[5]\n",
    "claim_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:51<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#Loading in lines from relevant documents\n",
    "\n",
    "lines_from_doc = []\n",
    "\n",
    "bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    iD = json.loads(line)['id']\n",
    "                    \n",
    "                    if iD in claim_to_check[1]:\n",
    "                        doc_lines = json.loads(line)['lines']\n",
    "                        doc_line_list = doc_lines.split('\\n')\n",
    "                        \n",
    "            \n",
    "                        for doc_line in doc_line_list:\n",
    "                            doc_line = doc_line.lower()\n",
    "                            #Isolating cleaned up version of line\n",
    "                            doc_line = doc_line.split('\\t')[1]\n",
    "                            doc_line = doc_line.split('.')[0]\n",
    "                            \n",
    "                            doc_line = doc_line.split(' ')\n",
    "                            \n",
    "                            doc_line = doc_line[:-1]\n",
    "                            \n",
    "                            #Sorting out the brackets\n",
    "                            doc_line_brac = []\n",
    "                            for word in doc_line:\n",
    "                                \n",
    "                                if word in set(list(bracket_dict.keys())):\n",
    "                                    word = bracket_dict[word]\n",
    "                                \n",
    "                                doc_line_brac.append(word)\n",
    "                                \n",
    "                        #doc_tokens = nltk.word_tokenize(doc_lines)\n",
    "                            lines_from_doc.append(doc_line_brac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing empty lines\n",
    "clean_doc_lines = []\n",
    "\n",
    "for line in lines_from_doc:\n",
    "    \n",
    "    if len(line) != 0:\n",
    "        clean_doc_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector representation of all sentences and claim (1 claim and 51 sentences)\n",
    "vector_representations = []\n",
    "\n",
    "the_claim = claim_to_check[0]\n",
    "\n",
    "sum_count_vec = np.zeros(50)\n",
    "for word in the_claim:\n",
    "    vec_rep = model[word]\n",
    "    sum_count_vec += vec_rep\n",
    "\n",
    "vector_representations.append(sum_count_vec)\n",
    "\n",
    "for sentence in clean_doc_lines:\n",
    "    \n",
    "    sent_count_vec = np.zeros(50)\n",
    "    for word in sentence:\n",
    "\n",
    "        try:\n",
    "            vec_rep = model[word]\n",
    "\n",
    "        except:\n",
    "            vec_rep = 0\n",
    "\n",
    "        sent_count_vec += vec_rep\n",
    "\n",
    "    vector_representations.append(sent_count_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in training claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_claims = []\n",
    "veri_evid = []\n",
    "veri_label = []\n",
    "\n",
    "binary_claim_dic = {'SUPPORTS':1, 'REFUTES':0}\n",
    "\n",
    "with open('data_files/train.jsonl') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "            \n",
    "            claim_dic = json.loads(line)\n",
    "            veri_check = claim_dic['verifiable']\n",
    "            \n",
    "            if veri_check == 'VERIFIABLE':\n",
    "                claim = claim_dic['claim']\n",
    "                evidence = claim_dic['evidence']\n",
    "                label = binary_claim_dic[claim_dic['label']]\n",
    "                \n",
    "                veri_claims.append(claim)\n",
    "                veri_evid.append(evidence)\n",
    "                veri_label.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolating subset which can be verified by just one line\n",
    "one_line_claims = []\n",
    "one_line_evidence = []\n",
    "one_line_labels = []\n",
    "\n",
    "for idx, evid in enumerate(veri_evid):\n",
    "    if len(evid[0]) == 1:\n",
    "        one_line_evidence.append(evid)\n",
    "        one_line_claims.append(veri_claims[idx])\n",
    "        one_line_labels.append(veri_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Roman_Atwood', 1), ('Roman_Atwood', 3)],\n",
       " [('History_of_art', 2)],\n",
       " [('Adrienne_Bailon', 0)],\n",
       " [('Boston_Celtics', 3), ('Boston_Celtics', 3)],\n",
       " [('The_Ten_Commandments_-LRB-1956_film-RRB-', 0),\n",
       "  ('The_Ten_Commandments_-LRB-1956_film-RRB-', 20)],\n",
       " [('Tetris', 18)],\n",
       " [('Cyndi_Lauper', 2)],\n",
       " [('The_Hunger_Games_-LRB-film-RRB-', 0),\n",
       "  ('The_Hunger_Games_-LRB-film-RRB-', 1),\n",
       "  ('The_Hunger_Games_-LRB-film-RRB-', 2),\n",
       "  ('The_Hunger_Games_-LRB-film-RRB-', 16)],\n",
       " [('Stranger_Things', 5)],\n",
       " [('Ryan_Seacrest', 0),\n",
       "  ('Ryan_Seacrest', 1),\n",
       "  ('Ryan_Seacrest', 2),\n",
       "  ('Ryan_Seacrest', 5)],\n",
       " [('Puerto_Rico', 0)],\n",
       " [('Michael_Giacchino', 1)],\n",
       " [('Stranger_than_Fiction_-LRB-2006_film-RRB-', 0)],\n",
       " [('Chris_Hemsworth', 2)],\n",
       " [('Selena', 8),\n",
       "  ('Selena', 0),\n",
       "  ('Selena', 2),\n",
       "  ('Selena', 4),\n",
       "  ('Selena', 8),\n",
       "  ('Selena', 21),\n",
       "  ('Selena', 0),\n",
       "  ('Selena', 9),\n",
       "  ('Selena', 10),\n",
       "  ('Selena', 11),\n",
       "  ('Selena', 14),\n",
       "  ('Selena', 16),\n",
       "  ('Selena', 18),\n",
       "  ('Selena', 19),\n",
       "  ('Selena', 21),\n",
       "  ('Selena', 32)],\n",
       " [(\"Robert_J._O'Neill_-LRB-U.S._Navy_SEAL-RRB-\", 0)],\n",
       " [('The_Silence_of_the_Lambs_-LRB-film-RRB-', 0),\n",
       "  ('The_Silence_of_the_Lambs_-LRB-film-RRB-', 0)],\n",
       " [('Andy_Roddick', 7)],\n",
       " [('Bed-In', 0)],\n",
       " [('Tupac_Shakur', 0), ('Tupac_Shakur', 0)],\n",
       " [('The_Jim_Henson_Company', 5)],\n",
       " [('Midwestern_United_States', 4)],\n",
       " [('Saratoga_-LRB-film-RRB-', 0)],\n",
       " [('Grace_Jones', 0)],\n",
       " [('Lisbon', 0), ('Lisbon', 2)],\n",
       " [('Willie_Nelson', 9)],\n",
       " [('Malcolm_Young', 0)],\n",
       " [('Newfoundland_and_Labrador', 4)],\n",
       " [('Furia_-LRB-film-RRB-', 0)],\n",
       " [('United_States', 3)],\n",
       " [('Sophie_Turner', 0)],\n",
       " [('Mother_Teresa', 11)],\n",
       " [('The_Smurfs_-LRB-film-RRB-', 10),\n",
       "  ('The_Smurfs_-LRB-film-RRB-', 0),\n",
       "  ('The_Smurfs_-LRB-film-RRB-', 10),\n",
       "  ('The_Smurfs_-LRB-film-RRB-', 15),\n",
       "  ('The_Smurfs_-LRB-film-RRB-', 12)],\n",
       " [('C._S._Forester', 0)],\n",
       " [('Kong-COLON-_Skull_Island', 0)],\n",
       " [('The_Challenge_-LRB-TV_series-RRB-', 0)],\n",
       " [('Berlin', 0)],\n",
       " [('South_Korea', 27)],\n",
       " [('Hacksaw_Ridge', 5)],\n",
       " [('Michael_Fassbender', 0),\n",
       "  ('Michael_Fassbender', 1),\n",
       "  ('Michael_Fassbender', 2),\n",
       "  ('Michael_Fassbender', 3)],\n",
       " [('Red_Headed_Stranger', 0), ('Red_Headed_Stranger', 10)],\n",
       " [('Uzbekistan', 14)],\n",
       " [('Charles,_Prince_of_Wales', 16)],\n",
       " [('Chester_Bennington', 0),\n",
       "  ('Chester_Bennington', 1),\n",
       "  ('Chester_Bennington', 2),\n",
       "  ('Chester_Bennington', 4),\n",
       "  ('Chester_Bennington', 7)],\n",
       " [('Vincent_Cassel', 1)],\n",
       " [('Pope_John_Paul_II_-LRB-miniseries-RRB-', 11)],\n",
       " [('David_Beckham', 0)],\n",
       " [('Indiana_Jones', 7)],\n",
       " [('Brian_Wilson', 0),\n",
       "  ('Brian_Wilson', 5),\n",
       "  ('Brian_Wilson', 6),\n",
       "  ('Brian_Wilson', 7),\n",
       "  ('Brian_Wilson', 8),\n",
       "  ('Brian_Wilson', 9),\n",
       "  ('Brian_Wilson', 19),\n",
       "  ('Brian_Wilson', 23)],\n",
       " [('The_Great_Gatsby', 1)],\n",
       " [('Iron_Man_3', 2)],\n",
       " [('La_La_Anthony', 0)],\n",
       " [('Led_Zeppelin', 0),\n",
       "  ('Led_Zeppelin', 2),\n",
       "  ('Led_Zeppelin', 1),\n",
       "  ('Led_Zeppelin', 13),\n",
       "  ('Led_Zeppelin', 20),\n",
       "  ('Led_Zeppelin', 23),\n",
       "  ('Led_Zeppelin', 24),\n",
       "  ('Led_Zeppelin', 5),\n",
       "  ('Led_Zeppelin', 6),\n",
       "  ('Led_Zeppelin', 7),\n",
       "  ('Led_Zeppelin', 12),\n",
       "  ('Led_Zeppelin', 15),\n",
       "  ('Led_Zeppelin', 18)],\n",
       " [('Richard_Harris', 0)],\n",
       " [('The_Fosters_-LRB-2013_TV_series-RRB-', 0),\n",
       "  ('The_Fosters_-LRB-2013_TV_series-RRB-', 4)],\n",
       " [('Jonathan_Hensleigh', 0)],\n",
       " [('Filmfare', 0)],\n",
       " [('Buckingham_Palace', 15)],\n",
       " [('Sarah_Paulson', 2)],\n",
       " [('David_Harbour', 0)],\n",
       " [('Kazakhstan', 12)],\n",
       " [('José_María_Chacón', 0)],\n",
       " [('Liverpool_F.C.', 11)],\n",
       " [('Land_Rover', 0), ('Land_Rover', 11)],\n",
       " [('Sully_-LRB-film-RRB-', 0)],\n",
       " [('Brock_Lesnar', 18)],\n",
       " [('Khmer_Empire', 0)],\n",
       " [('On_the_Road_-LRB-film-RRB-', 5),\n",
       "  ('On_the_Road_-LRB-film-RRB-', 6),\n",
       "  ('On_the_Road_-LRB-film-RRB-', 7)],\n",
       " [('Paramore_-LRB-album-RRB-', 2)],\n",
       " [('Peter_Davison', 1), ('Peter_Davison', 2)],\n",
       " [('21_Jump_Street_-LRB-film-RRB-', 4)],\n",
       " [('Philadelphia_Museum_of_Art', 2)],\n",
       " [('Deadpool_-LRB-film-RRB-', 1)],\n",
       " [('Stephon_Marbury', 0)],\n",
       " [('Cerebral_palsy', 1)],\n",
       " [('Alfred_Hitchcock', 0)],\n",
       " [('Julius_Caesar_-LRB-1953_film-RRB-', 0)],\n",
       " [('Absolute_Beginners_-LRB-film-RRB-', 1)],\n",
       " [('No_Reservations_-LRB-film-RRB-', 5)],\n",
       " [('Dangerous_Liaisons', 0)],\n",
       " [('Alex_Rodriguez', 3)],\n",
       " [('Friendship', 2), ('Friendship', 2)],\n",
       " [('Judd_Apatow', 0), ('Judd_Apatow', 1)],\n",
       " [('Massachusetts', 1)],\n",
       " [('Giancarlo_Stanton', 4)],\n",
       " [('Chris_Pine', 2)],\n",
       " [('Trevor_Noah', 9)],\n",
       " [('Ryan_Phillippe', 1)],\n",
       " [('Sean_Connery', 0), ('Sean_Connery', 5)],\n",
       " [('Venom_-LRB-comics-RRB-', 2)],\n",
       " [('Lily_James', 1),\n",
       "  ('Lily_James', 4),\n",
       "  ('Lily_James', 6),\n",
       "  ('Lily_James', 0),\n",
       "  ('Lily_James', 1),\n",
       "  ('Lily_James', 4),\n",
       "  ('Lily_James', 6),\n",
       "  ('Lily_James', 1),\n",
       "  ('Lily_James', 4),\n",
       "  ('Lily_James', 4),\n",
       "  ('Lily_James', 1),\n",
       "  ('Lily_James', 4),\n",
       "  ('Lily_James', 6)],\n",
       " [('Jing_Tian', 0)],\n",
       " [('Jennifer_Aniston', 1)],\n",
       " [('Sparta', 6),\n",
       "  ('Sparta', 6),\n",
       "  ('Sparta', 6),\n",
       "  ('Sparta', 6),\n",
       "  ('Sparta', 6),\n",
       "  ('Sparta', 6)],\n",
       " [('Isis', 15)],\n",
       " [('International_relations', 0),\n",
       "  ('International_relations', 1),\n",
       "  ('International_relations', 2),\n",
       "  ('International_relations', 9),\n",
       "  ('International_relations', 10)],\n",
       " [('Régine_Chassagne', 0), ('Régine_Chassagne', 0)],\n",
       " [('John_McCain', 0),\n",
       "  ('John_McCain', 1),\n",
       "  ('John_McCain', 0),\n",
       "  ('John_McCain', 13),\n",
       "  ('John_McCain', 15),\n",
       "  ('John_McCain', 14),\n",
       "  ('John_McCain', 19),\n",
       "  ('John_McCain', 26),\n",
       "  ('John_McCain', 25),\n",
       "  ('John_McCain', 0),\n",
       "  ('John_McCain', 13),\n",
       "  ('John_McCain', 0),\n",
       "  ('John_McCain', 1),\n",
       "  ('John_McCain', 14),\n",
       "  ('John_McCain', 15),\n",
       "  ('John_McCain', 19),\n",
       "  ('John_McCain', 22),\n",
       "  ('John_McCain', 23)],\n",
       " [('Farrah_Fawcett', 16)],\n",
       " [('The_Who', 6)],\n",
       " [('Planet_of_the_Apes_-LRB-1968_film-RRB-', 12)],\n",
       " [('Star_vs._the_Forces_of_Evil', 0),\n",
       "  ('Star_vs._the_Forces_of_Evil', 1),\n",
       "  ('Star_vs._the_Forces_of_Evil', 2),\n",
       "  ('Star_vs._the_Forces_of_Evil', 3),\n",
       "  ('Star_vs._the_Forces_of_Evil', 4),\n",
       "  ('Star_vs._the_Forces_of_Evil', 5),\n",
       "  ('Star_vs._the_Forces_of_Evil', 6)],\n",
       " [('Galaxy_Quest', 0)],\n",
       " [('Friends_with_Benefits_-LRB-film-RRB-', 0),\n",
       "  ('Friends_with_Benefits_-LRB-film-RRB-', 1)],\n",
       " [('Zooey_Deschanel', 0), ('Zooey_Deschanel', 1)],\n",
       " [('Winnipeg', 16)],\n",
       " [('Rick_and_Morty', 8), ('Rick_and_Morty', 9)],\n",
       " [('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 3),\n",
       "  ('Paul_Walker', 4),\n",
       "  ('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 3),\n",
       "  ('Paul_Walker', 4),\n",
       "  ('Paul_Walker', 13),\n",
       "  ('Paul_Walker', 13),\n",
       "  ('Paul_Walker', 4),\n",
       "  ('Paul_Walker', 3),\n",
       "  ('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 3),\n",
       "  ('Paul_Walker', 4),\n",
       "  ('Paul_Walker', 13),\n",
       "  ('Paul_Walker', 2),\n",
       "  ('Paul_Walker', 3),\n",
       "  ('Paul_Walker', 4)],\n",
       " [('Clint_Eastwood', 5)],\n",
       " [('Gilmore_Girls', 0)],\n",
       " [('Honest_-LRB-Future_album-RRB-', 6),\n",
       "  ('Honest_-LRB-Future_album-RRB-', 6),\n",
       "  ('Honest_-LRB-Future_album-RRB-', 6)],\n",
       " [('Pranab_Mukherjee', 0), ('Pranab_Mukherjee', 23)],\n",
       " [('Anonymous_-LRB-group-RRB-', 5),\n",
       "  ('Anonymous_-LRB-group-RRB-', 5),\n",
       "  ('Anonymous_-LRB-group-RRB-', 5),\n",
       "  ('Anonymous_-LRB-group-RRB-', 5)],\n",
       " [('San_Francisco', 0)],\n",
       " [('Laadla_-LRB-1994_film-RRB-', 0)],\n",
       " [('Lily_Collins', 4)],\n",
       " [('Bangladesh', 31),\n",
       "  ('Bangladesh', 31),\n",
       "  ('Bangladesh', 31),\n",
       "  ('Bangladesh', 31),\n",
       "  ('Bangladesh', 31)],\n",
       " [('Janet_Jackson', 5)],\n",
       " [('Transformers-COLON-_The_Last_Knight', 2)],\n",
       " [('Naturi_Naughton', 0)],\n",
       " [('Paul_Pogba', 4)],\n",
       " [('Mel_Gibson', 0)],\n",
       " [('WALL-E', 13)],\n",
       " [('Generation_X', 1)],\n",
       " [('Islam', 2), ('Islam', 3), ('Islam', 10)],\n",
       " [('Jackie_Chan', 8)],\n",
       " [('Earth', 23)],\n",
       " [('Larry_Junstrom', 0)],\n",
       " [('Sophie_Turner', 5)],\n",
       " [('Renaissance', 1), ('Renaissance', 0)],\n",
       " [('Pluto', 13)],\n",
       " [('Steve_Buscemi', 2)],\n",
       " [('Twenty_One_Pilots', 1)],\n",
       " [('Selena_Gomez', 20)],\n",
       " [('Priyanka_Chopra', 0)],\n",
       " [('Chris_Pérez', 0), ('Chris_Pérez', 24)],\n",
       " [('Quentin_Tarantino', 0)],\n",
       " [('Tate_Ellington', 4), ('Tate_Ellington', 4)],\n",
       " [('Mr._Sunshine_-LRB-2011_TV_series-RRB-', 0)],\n",
       " [('Dolly_Parton', 17), ('Dolly_Parton', 0)],\n",
       " [('The_Amanda_Show', 0)],\n",
       " [('19th_G7_summit', 4)],\n",
       " [('Ethiopia', 29)],\n",
       " [('Ted_Kaczynski', 5), ('Ted_Kaczynski', 6)],\n",
       " [('Keith_Urban', 18)],\n",
       " [('Eminem', 2),\n",
       "  ('Eminem', 3),\n",
       "  ('Eminem', 4),\n",
       "  ('Eminem', 8),\n",
       "  ('Eminem', 9),\n",
       "  ('Eminem', 12),\n",
       "  ('Eminem', 13),\n",
       "  ('Eminem', 11),\n",
       "  ('Eminem', 10)],\n",
       " [('Jennifer_Aniston', 0), ('Jennifer_Aniston', 10)],\n",
       " [('Ilkhanate', 4), ('Ilkhanate', 4)],\n",
       " [('Homeland_-LRB-TV_series-RRB-', 0)],\n",
       " [('Jordan_Knight', 0)],\n",
       " [('Adam_Driver', 1)],\n",
       " [('Thor-COLON-_The_Dark_World', 0),\n",
       "  ('Thor-COLON-_The_Dark_World', 1),\n",
       "  ('Thor-COLON-_The_Dark_World', 2),\n",
       "  ('Thor-COLON-_The_Dark_World', 9),\n",
       "  ('Thor-COLON-_The_Dark_World', 17),\n",
       "  ('Thor-COLON-_The_Dark_World', 0),\n",
       "  ('Thor-COLON-_The_Dark_World', 0),\n",
       "  ('Thor-COLON-_The_Dark_World', 0),\n",
       "  ('Thor-COLON-_The_Dark_World', 0),\n",
       "  ('Thor-COLON-_The_Dark_World', 1),\n",
       "  ('Thor-COLON-_The_Dark_World', 2),\n",
       "  ('Thor-COLON-_The_Dark_World', 15),\n",
       "  ('Thor-COLON-_The_Dark_World', 16),\n",
       "  ('Thor-COLON-_The_Dark_World', 17)],\n",
       " [('Loki', 1)],\n",
       " [('Amsterdam', 23)],\n",
       " [('Chester_Bennington', 2),\n",
       "  ('Chester_Bennington', 3),\n",
       "  ('Chester_Bennington', 4),\n",
       "  ('Chester_Bennington', 10),\n",
       "  ('Chester_Bennington', 12),\n",
       "  ('Chester_Bennington', 13)],\n",
       " [('Inspectah_Deck', 0)],\n",
       " [('Tender_Mercies', 0)],\n",
       " [('Cristiano_Ronaldo', 0), ('Cristiano_Ronaldo', 4)],\n",
       " [('Paul_Simon', 0),\n",
       "  ('Paul_Simon', 5),\n",
       "  ('Paul_Simon', 7),\n",
       "  ('Paul_Simon', 12),\n",
       "  ('Paul_Simon', 14)],\n",
       " [('Baby_Driver', 0)],\n",
       " [('David_Dhawan', 0), ('David_Dhawan', 2), ('David_Dhawan', 4)],\n",
       " [('The_Challenge_-LRB-TV_series-RRB-', 0),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 1),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 6),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 8),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 11),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 12),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 13),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 14),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 17),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 0),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 1),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 6),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 8),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 11),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 12),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 13),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 14),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 17),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 0),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 1),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 6),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 8),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 11),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 12),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 13),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 14),\n",
       "  ('The_Challenge_-LRB-TV_series-RRB-', 17)],\n",
       " [('Rumi', 1), ('Rumi', 1), ('Rumi', 1), ('Rumi', 1), ('Rumi', 1)],\n",
       " [('Her_-LRB-film-RRB-', 16),\n",
       "  ('Her_-LRB-film-RRB-', 16),\n",
       "  ('Her_-LRB-film-RRB-', 16),\n",
       "  ('Her_-LRB-film-RRB-', 16),\n",
       "  ('Her_-LRB-film-RRB-', 16)],\n",
       " [('Las_Vegas', 14), ('Las_Vegas', 15)],\n",
       " [('Spread_-LRB-food-RRB-', 0)],\n",
       " [('Octavia_Spencer', 4), ('Octavia_Spencer', 4)],\n",
       " [('Ramayanam_-LRB-film-RRB-', 0),\n",
       "  ('Ramayanam_-LRB-film-RRB-', 0),\n",
       "  ('Ramayanam_-LRB-film-RRB-', 0)],\n",
       " [('Man_of_Steel_-LRB-film-RRB-', 0),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 5),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 6),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 19),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 0),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 5),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 6),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 0),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 6),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 0)],\n",
       " [('Leukemia', 3)],\n",
       " [('Until_Dawn', 0)],\n",
       " [('Timothy_Olyphant', 0), ('Timothy_Olyphant', 0)],\n",
       " [('Telangana', 3)],\n",
       " [('Cheaper_by_the_Dozen_-LRB-2003_film-RRB-', 1)],\n",
       " [('House_of_Lusignan', 0)],\n",
       " [('Ten_Commandments', 0)],\n",
       " [('Ariana_Grande', 11)],\n",
       " [('Ted_Bundy', 0), ('Ted_Bundy', 12)],\n",
       " [('Chinese_people', 0)],\n",
       " [(\"Schindler's_List\", 0),\n",
       "  (\"Schindler's_List\", 18),\n",
       "  (\"Schindler's_List\", 18),\n",
       "  (\"Schindler's_List\", 0),\n",
       "  (\"Schindler's_List\", 18),\n",
       "  (\"Schindler's_List\", 0),\n",
       "  (\"Schindler's_List\", 18),\n",
       "  (\"Schindler's_List\", 0),\n",
       "  (\"Schindler's_List\", 0),\n",
       "  (\"Schindler's_List\", 18)],\n",
       " [('Escape_from_Planet_Earth', 0), ('Escape_from_Planet_Earth', 0)],\n",
       " [('Venus_Williams', 16), ('Venus_Williams', 17)],\n",
       " [('Quentin_Tarantino', 5),\n",
       "  ('Quentin_Tarantino', 6),\n",
       "  ('Quentin_Tarantino', 7),\n",
       "  ('Quentin_Tarantino', 9),\n",
       "  ('Quentin_Tarantino', 13),\n",
       "  ('Quentin_Tarantino', 12),\n",
       "  ('Quentin_Tarantino', 14),\n",
       "  ('Quentin_Tarantino', 15),\n",
       "  ('Quentin_Tarantino', 17)],\n",
       " [('The_Beatles', 6)],\n",
       " [('50_First_Dates', 0)],\n",
       " [('A_Nightmare_on_Elm_Street', 0), ('A_Nightmare_on_Elm_Street', 6)],\n",
       " [('International_relations', 9)],\n",
       " [('Jemaine_Clement', 0)],\n",
       " [('Usain_Bolt', 4),\n",
       "  ('Usain_Bolt', 8),\n",
       "  ('Usain_Bolt', 9),\n",
       "  ('Usain_Bolt', 4),\n",
       "  ('Usain_Bolt', 8),\n",
       "  ('Usain_Bolt', 9)],\n",
       " [('Internet', 6)],\n",
       " [('Sarrainodu', 0)],\n",
       " [('Kurt_Sutter', 2),\n",
       "  ('Kurt_Sutter', 2),\n",
       "  ('Kurt_Sutter', 2),\n",
       "  ('Kurt_Sutter', 2),\n",
       "  ('Kurt_Sutter', 2)],\n",
       " [('La_La_Anthony', 0)],\n",
       " [('Lisa_Marie_Presley', 0)],\n",
       " [('Keanu_Reeves', 4),\n",
       "  ('Keanu_Reeves', 0),\n",
       "  ('Keanu_Reeves', 4),\n",
       "  ('Keanu_Reeves', 5),\n",
       "  ('Keanu_Reeves', 4),\n",
       "  ('Keanu_Reeves', 4),\n",
       "  ('Keanu_Reeves', 4),\n",
       "  ('Keanu_Reeves', 5),\n",
       "  ('Keanu_Reeves', 4),\n",
       "  ('Keanu_Reeves', 5)],\n",
       " [('Dwayne_Johnson', 12)],\n",
       " [('Heartland_-LRB-Canadian_TV_series-RRB-', 0)],\n",
       " [('The_Beatles', 18)],\n",
       " [('Mango', 7), ('Mango', 7), ('Mango', 7), ('Mango', 7)],\n",
       " [('Iris_Murdoch', 0),\n",
       "  ('Iris_Murdoch', 1),\n",
       "  ('Iris_Murdoch', 3),\n",
       "  ('Iris_Murdoch', 4)],\n",
       " [('David_Koepp', 0),\n",
       "  ('David_Koepp', 0),\n",
       "  ('David_Koepp', 0),\n",
       "  ('David_Koepp', 8),\n",
       "  ('David_Koepp', 0),\n",
       "  ('David_Koepp', 0),\n",
       "  ('David_Koepp', 0),\n",
       "  ('David_Koepp', 8)],\n",
       " [('Block_-LRB-basketball-RRB-', 10)],\n",
       " [('Paul_Pogba', 6)],\n",
       " [('Leroy_Jethro_Gibbs', 0)],\n",
       " [('Leslie_Howard_-LRB-actor-RRB-', 0), ('Leslie_Howard_-LRB-actor-RRB-', 3)],\n",
       " [('Catching_Fire', 0)],\n",
       " [('Avatar-COLON-_The_Last_Airbender', 0),\n",
       "  ('Avatar-COLON-_The_Last_Airbender', 0),\n",
       "  ('Avatar-COLON-_The_Last_Airbender', 17)],\n",
       " [('Hopman_Cup', 4)],\n",
       " [('Frank_Zappa', 10)],\n",
       " [('George_VI', 28)],\n",
       " [('The_Beach_Boys', 0), ('The_Beach_Boys', 23)],\n",
       " [('Billy_Bob_Thornton', 16),\n",
       "  ('Billy_Bob_Thornton', 3),\n",
       "  ('Billy_Bob_Thornton', 16)],\n",
       " [('Courteney_Cox', 1)],\n",
       " [('Ronin_-LRB-film-RRB-', 0)],\n",
       " [('Abraham_Lincoln', 10)],\n",
       " [('Fred_Savage', 0)],\n",
       " [('Interstellar_-LRB-film-RRB-', 3),\n",
       "  ('Interstellar_-LRB-film-RRB-', 4),\n",
       "  ('Interstellar_-LRB-film-RRB-', 9),\n",
       "  ('Interstellar_-LRB-film-RRB-', 10),\n",
       "  ('Interstellar_-LRB-film-RRB-', 12),\n",
       "  ('Interstellar_-LRB-film-RRB-', 13),\n",
       "  ('Interstellar_-LRB-film-RRB-', 15),\n",
       "  ('Interstellar_-LRB-film-RRB-', 20),\n",
       "  ('Interstellar_-LRB-film-RRB-', 21)],\n",
       " [('Britney_Spears', 6)],\n",
       " [('Mike_Tyson', 0),\n",
       "  ('Mike_Tyson', 1),\n",
       "  ('Mike_Tyson', 3),\n",
       "  ('Mike_Tyson', 4),\n",
       "  ('Mike_Tyson', 22),\n",
       "  ('Mike_Tyson', 27),\n",
       "  ('Mike_Tyson', 30)],\n",
       " [('Hermetic_Order_of_the_Golden_Dawn', 0)],\n",
       " [('Ellen_Burstyn', 11)],\n",
       " [('Princess_Agents', 0)],\n",
       " [('Gwen_Stefani', 0), ('Gwen_Stefani', 1)],\n",
       " [('Carrie_Fisher', 11)],\n",
       " [('Ice_hockey', 21), ('Ice_hockey', 24)],\n",
       " [('The_Fifth_Element', 15)],\n",
       " [('The_Hunger_Games_-LRB-film-RRB-', 1)],\n",
       " [('Audi', 14)],\n",
       " [('Indonesia', 4)],\n",
       " [('Wyatt_Earp', 7)],\n",
       " [('Brave_-LRB-2012_film-RRB-', 0), ('Brave_-LRB-2012_film-RRB-', 14)],\n",
       " [('Winter_Passing', 3)],\n",
       " [('Line_of_Duty', 0), ('Line_of_Duty', 1)],\n",
       " [('Dayton_Agreement', 0)],\n",
       " [('National_Security_Agency', 1), ('National_Security_Agency', 1)],\n",
       " [('Gold', 15)],\n",
       " [('Paleogene', 0)],\n",
       " [('Cher', 26)],\n",
       " [('Bernard_Madoff', 8)],\n",
       " [('Donkey_-LRB-Shrek-RRB-', 0)],\n",
       " [('O._J._Simpson', 0), ('O._J._Simpson', 17)],\n",
       " [('Ketogenic_diet', 0), ('Ketogenic_diet', 11), ('Ketogenic_diet', 13)],\n",
       " [('Dennis_Quaid', 0), ('Dennis_Quaid', 2), ('Dennis_Quaid', 3)],\n",
       " [('United_States', 29)],\n",
       " [('Thandie_Newton', 1)],\n",
       " [('Reese_Witherspoon', 6)],\n",
       " [('Curly_Top_-LRB-film-RRB-', 0)],\n",
       " [('Edgar_Allan_Poe', 0),\n",
       "  ('Edgar_Allan_Poe', 0),\n",
       "  ('Edgar_Allan_Poe', 0),\n",
       "  ('Edgar_Allan_Poe', 1),\n",
       "  ('Edgar_Allan_Poe', 2),\n",
       "  ('Edgar_Allan_Poe', 3),\n",
       "  ('Edgar_Allan_Poe', 4),\n",
       "  ('Edgar_Allan_Poe', 7),\n",
       "  ('Edgar_Allan_Poe', 10),\n",
       "  ('Edgar_Allan_Poe', 11),\n",
       "  ('Edgar_Allan_Poe', 12),\n",
       "  ('Edgar_Allan_Poe', 13),\n",
       "  ('Edgar_Allan_Poe', 14),\n",
       "  ('Edgar_Allan_Poe', 16),\n",
       "  ('Edgar_Allan_Poe', 19),\n",
       "  ('Edgar_Allan_Poe', 20),\n",
       "  ('Edgar_Allan_Poe', 21),\n",
       "  ('Edgar_Allan_Poe', 22),\n",
       "  ('Edgar_Allan_Poe', 25)],\n",
       " [('Danny_DeVito', 13)],\n",
       " [('PageRank', 1)],\n",
       " [('Morena_Baccarin', 1)],\n",
       " [('Jeffrey_Dahmer', 5)],\n",
       " [('Naomi_Scott', 2)],\n",
       " [('Minnesota_Vikings', 1),\n",
       "  ('Minnesota_Vikings', 2),\n",
       "  ('Minnesota_Vikings', 3),\n",
       "  ('Minnesota_Vikings', 4),\n",
       "  ('Minnesota_Vikings', 1),\n",
       "  ('Minnesota_Vikings', 2),\n",
       "  ('Minnesota_Vikings', 3),\n",
       "  ('Minnesota_Vikings', 4)],\n",
       " [('L.A._Law', 0)],\n",
       " [('Paradise_-LRB-Lana_Del_Rey_EP-RRB-', 0)],\n",
       " [('Dirt_-LRB-TV_series-RRB-', 9)],\n",
       " [('Arsenic_and_Old_Lace_-LRB-film-RRB-', 2)],\n",
       " [('The_Belko_Experiment', 0)],\n",
       " [('Psychology', 13), ('Psychology', 1)],\n",
       " [('Apple_Inc.', 6),\n",
       "  ('Apple_Inc.', 6),\n",
       "  ('Apple_Inc.', 6),\n",
       "  ('Apple_Inc.', 6),\n",
       "  ('Apple_Inc.', 6),\n",
       "  ('Apple_Inc.', 7)],\n",
       " [('Moscow', 0), ('Moscow', 0)],\n",
       " [('Cult_film', 4), ('Cult_film', 4)],\n",
       " [('September_11_attacks', 4)],\n",
       " [('Richard_III_-LRB-play-RRB-', 7)],\n",
       " [('Kathy_Bates', 0),\n",
       "  ('Kathy_Bates', 0),\n",
       "  ('Kathy_Bates', 0),\n",
       "  ('Kathy_Bates', 0),\n",
       "  ('Kathy_Bates', 0),\n",
       "  ('Kathy_Bates', 0)],\n",
       " [('Joseph_Barbera', 0)],\n",
       " [('Penny_Dreadful_-LRB-TV_series-RRB-', 2),\n",
       "  ('Penny_Dreadful_-LRB-TV_series-RRB-', 3),\n",
       "  ('Penny_Dreadful_-LRB-TV_series-RRB-', 4)],\n",
       " [('Overwatch_-LRB-video_game-RRB-', 0),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 1),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 22),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 23)],\n",
       " [('The_Lion_King', 0)],\n",
       " [('Before_We_Go', 0), ('Before_We_Go', 0)],\n",
       " [('Stevie_Nicks', 21)],\n",
       " [('Overwatch_-LRB-video_game-RRB-', 4),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 4),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 4),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 4),\n",
       "  ('Overwatch_-LRB-video_game-RRB-', 4)],\n",
       " [('Andrew_Wood_-LRB-singer-RRB-', 0),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 4),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 6),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 7),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 10),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 11),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 13),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 16),\n",
       "  ('Andrew_Wood_-LRB-singer-RRB-', 17)],\n",
       " [('James_McAvoy', 2)],\n",
       " [('John_Constantine', 0)],\n",
       " [('Portuguese_Empire', 0)],\n",
       " [('Eddie_Vedder', 0), ('Eddie_Vedder', 0)],\n",
       " [('Larry_Buttrose', 2)],\n",
       " [('Farrah_Fawcett', 23)],\n",
       " [('X-Men-COLON-_Days_of_Future_Past', 0)],\n",
       " [('San_Junipero', 0)],\n",
       " [('Alex_Rodriguez', 3), ('Alex_Rodriguez', 17)],\n",
       " [('John_F._Kennedy', 11)],\n",
       " [(\"Schindler's_List\", 20)],\n",
       " [('Juana_la_virgen', 0)],\n",
       " [('The_Champ_-LRB-1979_film-RRB-', 0)],\n",
       " [('Monarch_of_the_Glen_-LRB-TV_series-RRB-', 0)],\n",
       " [('Robert_Redford', 0)],\n",
       " [('Beasts_of_the_Southern_Wild', 5), ('Beasts_of_the_Southern_Wild', 5)],\n",
       " [('Los_Angeles_Lakers', 24)],\n",
       " [('Philippines', 32)],\n",
       " [('Joe_Manganiello', 5), ('Joe_Manganiello', 5)],\n",
       " [('Veronika_Decides_to_Die_-LRB-film-RRB-', 1)],\n",
       " [('Sully_-LRB-film-RRB-', 1), ('Sully_-LRB-film-RRB-', 0)],\n",
       " [('Walter_Matthau', 1)],\n",
       " [('Lisa_Kudrow', 6)],\n",
       " [('Myles_Kennedy', 10)],\n",
       " [('Jane_Fonda', 21)],\n",
       " [('Imagine_Dragons', 2)],\n",
       " [('Mike_Love', 0)],\n",
       " [('Apple_Inc.', 7)],\n",
       " [('Star_Search', 1)],\n",
       " [('Canada', 11),\n",
       "  ('Canada', 11),\n",
       "  ('Canada', 11),\n",
       "  ('Canada', 11),\n",
       "  ('Canada', 11),\n",
       "  ('Canada', 13)],\n",
       " [('System_of_a_Down', 7)],\n",
       " [('Led_Zeppelin', 5)],\n",
       " [('Peyton_Manning', 1), ('Peyton_Manning', 11)],\n",
       " [('Davis_Entertainment', 3)],\n",
       " [('Alex_Rodriguez', 1), ('Alex_Rodriguez', 10)],\n",
       " [('Charles_Marie_de_La_Condamine', 0)],\n",
       " [('Bohemian_Rhapsody', 18)],\n",
       " [('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 8),\n",
       "  ('Cate_Blanchett', 9),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7),\n",
       "  ('Cate_Blanchett', 1),\n",
       "  ('Cate_Blanchett', 2),\n",
       "  ('Cate_Blanchett', 3),\n",
       "  ('Cate_Blanchett', 4),\n",
       "  ('Cate_Blanchett', 7)],\n",
       " [('Slash_-LRB-musician-RRB-', 0),\n",
       "  ('Slash_-LRB-musician-RRB-', 0),\n",
       "  ('Slash_-LRB-musician-RRB-', 0),\n",
       "  ('Slash_-LRB-musician-RRB-', 0),\n",
       "  ('Slash_-LRB-musician-RRB-', 0),\n",
       "  ('Slash_-LRB-musician-RRB-', 0)],\n",
       " [('Drake_-LRB-musician-RRB-', 0)],\n",
       " [('R._Madhavan', 13)],\n",
       " [('James_and_the_Giant_Peach_-LRB-film-RRB-', 3)],\n",
       " [('Ayn_Rand', 11), ('Ayn_Rand', 11), ('Ayn_Rand', 11), ('Ayn_Rand', 11)],\n",
       " [('Anne_Hathaway', 3)],\n",
       " [('Oh_Yeon-seo', 0)],\n",
       " [('Los_Angeles_Lakers', 17)],\n",
       " [('Slovenia', 1)],\n",
       " [('Italy', 0)],\n",
       " [('Ron_Cobb', 0)],\n",
       " [('Just_Dance_-LRB-song-RRB-', 1),\n",
       "  ('Just_Dance_-LRB-song-RRB-', 3),\n",
       "  ('Just_Dance_-LRB-song-RRB-', 7),\n",
       "  ('Just_Dance_-LRB-song-RRB-', 8),\n",
       "  ('Just_Dance_-LRB-song-RRB-', 13),\n",
       "  ('Just_Dance_-LRB-song-RRB-', 16)],\n",
       " [('Journey_-LRB-band-RRB-', 0),\n",
       "  ('Journey_-LRB-band-RRB-', 1),\n",
       "  ('Journey_-LRB-band-RRB-', 2),\n",
       "  ('Journey_-LRB-band-RRB-', 0),\n",
       "  ('Journey_-LRB-band-RRB-', 1),\n",
       "  ('Journey_-LRB-band-RRB-', 2),\n",
       "  ('Journey_-LRB-band-RRB-', 3),\n",
       "  ('Journey_-LRB-band-RRB-', 11),\n",
       "  ('Journey_-LRB-band-RRB-', 16),\n",
       "  ('Journey_-LRB-band-RRB-', 21),\n",
       "  ('Journey_-LRB-band-RRB-', 0),\n",
       "  ('Journey_-LRB-band-RRB-', 0),\n",
       "  ('Journey_-LRB-band-RRB-', 1),\n",
       "  ('Journey_-LRB-band-RRB-', 2),\n",
       "  ('Journey_-LRB-band-RRB-', 3),\n",
       "  ('Journey_-LRB-band-RRB-', 11),\n",
       "  ('Journey_-LRB-band-RRB-', 0)],\n",
       " [('Life_Is_Peachy', 0)],\n",
       " [('Mole_-LRB-architecture-RRB-', 0)],\n",
       " [('Shahid_Kapoor', 20)],\n",
       " [('L.A._Law', 0)],\n",
       " [('Jerry_Seinfeld', 6)],\n",
       " [('Marie_Curie', 0)],\n",
       " [('Will_Ferrell', 6)],\n",
       " [('Donnie_Wahlberg', 5)],\n",
       " [('Rogue_-LRB-comics-RRB-', 0)],\n",
       " [('German_Shepherd', 6)],\n",
       " [('Diego_Costa', 0)],\n",
       " [(\"Ocean's_Eight\", 1)],\n",
       " [('United_Kingdom', 15)],\n",
       " [('John_Wick-COLON-_Chapter_2', 0)],\n",
       " [('Andorra', 0), ('Andorra', 5)],\n",
       " [('Axl_Rose', 3)],\n",
       " [('Steffi_Graf', 17)],\n",
       " [('Jennifer_Garner', 6)],\n",
       " [('Paul_Simon', 0)],\n",
       " [('Taiwan', 18)],\n",
       " [('Bill_Gates', 4)],\n",
       " [('Phalanx', 2)],\n",
       " [('Robbie_Collin', 3)],\n",
       " [('Christianity', 16)],\n",
       " [('Feels_So_Good_-LRB-Mel_B_song-RRB-', 1)],\n",
       " [('Deadpool_-LRB-film-RRB-', 1)],\n",
       " [('Algeria', 8)],\n",
       " [('Clint_Eastwood', 5)],\n",
       " [('Gorgeous_Ladies_of_Wrestling', 0)],\n",
       " [('Gustave_Eiffel', 3)],\n",
       " [('Idris_Elba', 2),\n",
       "  ('Idris_Elba', 2),\n",
       "  ('Idris_Elba', 2),\n",
       "  ('Idris_Elba', 2),\n",
       "  ('Idris_Elba', 2)],\n",
       " [('Role_Models', 3)],\n",
       " [('Twelfth_Doctor', 3)],\n",
       " [('Lisa_Kudrow', 11)],\n",
       " [('Grace_VanderWaal', 1)],\n",
       " [('Jennifer_Hudson', 12), ('Jennifer_Hudson', 15)],\n",
       " [('Marie_Antoinette', 0), ('Marie_Antoinette', 17)],\n",
       " [('Uruguay_national_football_team', 6)],\n",
       " [('Overwatch_-LRB-video_game-RRB-', 0)],\n",
       " [('The_Sopranos', 0), ('The_Sopranos', 19), ('The_Sopranos', 14)],\n",
       " [('Babe_Ruth', 0)],\n",
       " [('La_La_Anthony', 7)],\n",
       " [('United_States_Naval_Academy', 0)],\n",
       " [('Tom_Welling', 0)],\n",
       " [('In_the_Heart_of_the_Sea_-LRB-film-RRB-', 5)],\n",
       " [('AC/DC', 1)],\n",
       " [('Naruto', 1)],\n",
       " [('Marilyn_Monroe', 0)],\n",
       " [('Israel', 29)],\n",
       " [('American_Broadcasting_Company', 6)],\n",
       " [('Cindy_McCain', 5), ('Cindy_McCain', 0)],\n",
       " [('Leonardo_DiCaprio', 0),\n",
       "  ('Leonardo_DiCaprio', 1),\n",
       "  ('Leonardo_DiCaprio', 4),\n",
       "  ('Leonardo_DiCaprio', 0),\n",
       "  ('Leonardo_DiCaprio', 1),\n",
       "  ('Leonardo_DiCaprio', 4)],\n",
       " [('T-Pain', 8)],\n",
       " [('Beyoncé', 16), ('Beyoncé', 15)],\n",
       " [('Alan_Ladd', 0)],\n",
       " [('VHS', 1)],\n",
       " [('Chernobyl_disaster', 17)],\n",
       " [('Frédéric_Auguste_Bartholdi', 0)],\n",
       " [('Kumail_Nanjiani', 7)],\n",
       " [('Hacksaw_Ridge', 5)],\n",
       " [('Kick-Ass_-LRB-film-RRB-', 0), ('Kick-Ass_-LRB-film-RRB-', 2)],\n",
       " [('Harry_Potter_and_the_Deathly_Hallows', 2)],\n",
       " [('Sophie_Turner', 5)],\n",
       " [('Farrah_Fawcett', 26)],\n",
       " [('Tiger_Woods', 5),\n",
       "  ('Tiger_Woods', 6),\n",
       "  ('Tiger_Woods', 15),\n",
       "  ('Tiger_Woods', 16),\n",
       "  ('Tiger_Woods', 26),\n",
       "  ('Tiger_Woods', 27),\n",
       "  ('Tiger_Woods', 28),\n",
       "  ('Tiger_Woods', 29),\n",
       "  ('Tiger_Woods', 30),\n",
       "  ('Tiger_Woods', 31)],\n",
       " [('Portugal', 4), ('Portugal', 5), ('Portugal', 32)],\n",
       " [('Steven_Knight', 0)],\n",
       " [('Suits_-LRB-TV_series-RRB-', 3)],\n",
       " [('Tom_Hanks', 8), ('Tom_Hanks', 8), ('Tom_Hanks', 6)],\n",
       " [('Dwyane_Wade', 6)],\n",
       " [('The_Homesman', 1)],\n",
       " [('Star_Wars_-LRB-film-RRB-', 15)],\n",
       " [('The_Playboy_Club', 2)],\n",
       " [('Portuguese_Empire', 0)],\n",
       " [('Chris_Froome', 2)],\n",
       " [('Shueisha', 0)],\n",
       " [('Friendship', 2)],\n",
       " [('Belgium', 15)],\n",
       " [('Apple_Inc.', 2)],\n",
       " [('Kris_Wu', 1)],\n",
       " [('Super_Metroid', 0),\n",
       "  ('Super_Metroid', 1),\n",
       "  ('Super_Metroid', 2),\n",
       "  ('Super_Metroid', 7),\n",
       "  ('Super_Metroid', 8),\n",
       "  ('Super_Metroid', 9),\n",
       "  ('Super_Metroid', 13),\n",
       "  ('Super_Metroid', 12),\n",
       "  ('Super_Metroid', 15),\n",
       "  ('Super_Metroid', 16),\n",
       "  ('Super_Metroid', 14),\n",
       "  ('Super_Metroid', 17),\n",
       "  ('Super_Metroid', 18)],\n",
       " [('One_America_News_Network', 0)],\n",
       " [('Jennifer_Garner', 11)],\n",
       " [('Portuguese_Empire', 0)],\n",
       " [('Scott_Eastwood', 0)],\n",
       " [('Tall_Story', 0)],\n",
       " [('X-Men-COLON-_Days_of_Future_Past', 0)],\n",
       " [('Richard_Eyre', 0), ('Richard_Eyre', 0), ('Richard_Eyre', 0)],\n",
       " [('Furious_7', 16)],\n",
       " [('Prakash_Jha', 0)],\n",
       " [('Splatoon_2', 1)],\n",
       " [('Uranus', 6)],\n",
       " [('Sriti_Jha', 3),\n",
       "  ('Sriti_Jha', 3),\n",
       "  ('Sriti_Jha', 3),\n",
       "  ('Sriti_Jha', 3),\n",
       "  ('Sriti_Jha', 3),\n",
       "  ('Sriti_Jha', 3),\n",
       "  ('Sriti_Jha', 3)],\n",
       " [('Absolute_Beginners_-LRB-film-RRB-', 0)],\n",
       " [('The_Amanda_Show', 0)],\n",
       " [('Brock_Lesnar', 5),\n",
       "  ('Brock_Lesnar', 5),\n",
       "  ('Brock_Lesnar', 5),\n",
       "  ('Brock_Lesnar', 16)],\n",
       " [('The_Hangover_Part_III', 3)],\n",
       " [('In_&_Out_-LRB-film-RRB-', 0)],\n",
       " [('Tom_Felton', 0), ('Tom_Felton', 4)],\n",
       " [('Kalpana_Raghavendar', 0)],\n",
       " [('Emily_Ratajkowski', 1)],\n",
       " [('10_Cloverfield_Lane', 1),\n",
       "  ('10_Cloverfield_Lane', 1),\n",
       "  ('10_Cloverfield_Lane', 1)],\n",
       " [('Chris_Froome', 2), ('Chris_Froome', 0), ('Chris_Froome', 2)],\n",
       " [('Olivia_Munn', 1)],\n",
       " [('Batman_-LRB-1966_film-RRB-', 1)],\n",
       " [('Marlon_Brando', 0), ('Marlon_Brando', 16)],\n",
       " [('Lung_cancer', 4)],\n",
       " [('Iran', 36), ('Iran', 36), ('Iran', 36), ('Iran', 36)],\n",
       " [('Taj_Mahal', 12)],\n",
       " [('Lisbon', 0)],\n",
       " [('Arya_Stark', 4)],\n",
       " [('Pulp_Fiction', 0), ('Pulp_Fiction', 0)],\n",
       " [('Dan_Aykroyd', 4), ('Dan_Aykroyd', 4)],\n",
       " [('Dhool', 0)],\n",
       " [('Android_-LRB-operating_system-RRB-', 9),\n",
       "  ('Android_-LRB-operating_system-RRB-', 10)],\n",
       " [('Saddam_Hussein', 0),\n",
       "  ('Saddam_Hussein', 1),\n",
       "  ('Saddam_Hussein', 4),\n",
       "  ('Saddam_Hussein', 5),\n",
       "  ('Saddam_Hussein', 7),\n",
       "  ('Saddam_Hussein', 11),\n",
       "  ('Saddam_Hussein', 17),\n",
       "  ('Saddam_Hussein', 19),\n",
       "  ('Saddam_Hussein', 20),\n",
       "  ('Saddam_Hussein', 21)],\n",
       " [('Tim_Rice', 1), ('Tim_Rice', 1)],\n",
       " [('Hugo_Weaving', 0),\n",
       "  ('Hugo_Weaving', 1),\n",
       "  ('Hugo_Weaving', 5),\n",
       "  ('Hugo_Weaving', 6),\n",
       "  ('Hugo_Weaving', 7),\n",
       "  ('Hugo_Weaving', 10)],\n",
       " [('Time_Lord', 8)],\n",
       " [('Arsenic_and_Old_Lace_-LRB-film-RRB-', 0),\n",
       "  ('Arsenic_and_Old_Lace_-LRB-film-RRB-', 2)],\n",
       " [('The_Catalyst', 0), ('The_Catalyst', 1), ('The_Catalyst', 11)],\n",
       " [('Indiana_Jones', 7)],\n",
       " [('Umbrella_-LRB-song-RRB-', 6),\n",
       "  ('Umbrella_-LRB-song-RRB-', 16),\n",
       "  ('Umbrella_-LRB-song-RRB-', 17),\n",
       "  ('Umbrella_-LRB-song-RRB-', 20)],\n",
       " [('The_Proposal_-LRB-film-RRB-', 1), ('The_Proposal_-LRB-film-RRB-', 10)],\n",
       " [('Martin_Luther_King_Jr.', 11)],\n",
       " [('FC_Barcelona', 9),\n",
       "  ('FC_Barcelona', 22),\n",
       "  ('FC_Barcelona', 23),\n",
       "  ('FC_Barcelona', 24)],\n",
       " [('The_Great_Buck_Howard', 0)],\n",
       " [('John_McEnroe', 5)],\n",
       " [('John_Mayer', 5)],\n",
       " [('Dirty_Diana', 0), ('Dirty_Diana', 4)],\n",
       " [('American_Gods', 20),\n",
       "  ('American_Gods', 20),\n",
       "  ('American_Gods', 20),\n",
       "  ('American_Gods', 20)],\n",
       " [('Adrienne_Bailon', 0)],\n",
       " [('Marlon_Brando', 11)],\n",
       " [('Up_All_Night_-LRB-One_Direction_album-RRB-', 0)],\n",
       " [('Little_Miss_Sunshine', 3)],\n",
       " [('IO_Theater', 0)],\n",
       " [('Coke_Boys_Records', 0)],\n",
       " [('Gia', 0)],\n",
       " [('21_Jump_Street_-LRB-film-RRB-', 0)],\n",
       " [('Kuala_Lumpur', 14),\n",
       "  ('Kuala_Lumpur', 14),\n",
       "  ('Kuala_Lumpur', 14),\n",
       "  ('Kuala_Lumpur', 14),\n",
       "  ('Kuala_Lumpur', 14),\n",
       "  ('Kuala_Lumpur', 14)],\n",
       " [('Fargo_-LRB-TV_series-RRB-', 3),\n",
       "  ('Fargo_-LRB-TV_series-RRB-', 3),\n",
       "  ('Fargo_-LRB-TV_series-RRB-', 3),\n",
       "  ('Fargo_-LRB-TV_series-RRB-', 3),\n",
       "  ('Fargo_-LRB-TV_series-RRB-', 3)],\n",
       " [('Carolina_Panthers', 19)],\n",
       " [('Ronaldo_Maczinski', 0)],\n",
       " [('Born_This_Way_-LRB-song-RRB-', 0), ('Born_This_Way_-LRB-song-RRB-', 0)],\n",
       " [('Andre_Agassi', 13),\n",
       "  ('Andre_Agassi', 13),\n",
       "  ('Andre_Agassi', 13),\n",
       "  ('Andre_Agassi', 13),\n",
       "  ('Andre_Agassi', 13)],\n",
       " [('Charles,_Prince_of_Wales', 5)],\n",
       " [('Man_of_Steel_-LRB-film-RRB-', 0),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 16),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 0),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 16),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 0),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 16),\n",
       "  ('Man_of_Steel_-LRB-film-RRB-', 0)],\n",
       " [('Shakira', 0),\n",
       "  ('Shakira', 6),\n",
       "  ('Shakira', 2),\n",
       "  ('Shakira', 9),\n",
       "  ('Shakira', 13)],\n",
       " [('Michael_Schumacher', 0)],\n",
       " [('Bridget_Moynahan', 8)],\n",
       " [('Taj_Mahal', 2)],\n",
       " [('Cry_Freedom', 7)],\n",
       " [('Richard_Nixon', 21)],\n",
       " [('Museum_of_Science_and_Industry_-LRB-Chicago-RRB-', 8),\n",
       "  ('Museum_of_Science_and_Industry_-LRB-Chicago-RRB-', 8),\n",
       "  ('Museum_of_Science_and_Industry_-LRB-Chicago-RRB-', 0)],\n",
       " [('Carrie_Fisher', 4)],\n",
       " [('Anil_Kapoor', 0), ('Anil_Kapoor', 1)],\n",
       " [('Before_Night_Falls_-LRB-film-RRB-', 0),\n",
       "  ('Before_Night_Falls_-LRB-film-RRB-', 2),\n",
       "  ('Before_Night_Falls_-LRB-film-RRB-', 3)],\n",
       " [('Home_Alone', 0)],\n",
       " [('Steven_Price_-LRB-composer-RRB-', 0)],\n",
       " [('1991_NBA_Finals', 8), ('1991_NBA_Finals', 14), ('1991_NBA_Finals', 17)],\n",
       " [('Google', 22)],\n",
       " [('Instagram', 0)],\n",
       " [('Eagles_-LRB-band-RRB-', 25)],\n",
       " [('Black_Widow_-LRB-Natasha_Romanova-RRB-', 0)],\n",
       " [('Berlin', 3)],\n",
       " [('Twitter', 8)],\n",
       " [('Ramadan', 1)],\n",
       " [('Born_This_Way_-LRB-song-RRB-', 7)],\n",
       " [('Mao_Zedong', 0), ('Mao_Zedong', 16)],\n",
       " [('Steven_Tyler', 7)],\n",
       " [('Wikipedia', 9)],\n",
       " [('Ellyse_Perry', 0),\n",
       "  ('Ellyse_Perry', 0),\n",
       "  ('Ellyse_Perry', 1),\n",
       "  ('Ellyse_Perry', 5),\n",
       "  ('Ellyse_Perry', 6),\n",
       "  ('Ellyse_Perry', 7),\n",
       "  ('Ellyse_Perry', 11),\n",
       "  ('Ellyse_Perry', 12),\n",
       "  ('Ellyse_Perry', 16),\n",
       "  ('Ellyse_Perry', 17),\n",
       "  ('Ellyse_Perry', 18),\n",
       "  ('Ellyse_Perry', 21),\n",
       "  ('Ellyse_Perry', 22),\n",
       "  ('Ellyse_Perry', 23),\n",
       "  ('Ellyse_Perry', 15),\n",
       "  ('Ellyse_Perry', 10),\n",
       "  ('Ellyse_Perry', 0),\n",
       "  ('Ellyse_Perry', 1),\n",
       "  ('Ellyse_Perry', 5),\n",
       "  ('Ellyse_Perry', 6),\n",
       "  ('Ellyse_Perry', 7),\n",
       "  ('Ellyse_Perry', 11),\n",
       "  ('Ellyse_Perry', 12),\n",
       "  ('Ellyse_Perry', 16),\n",
       "  ('Ellyse_Perry', 22),\n",
       "  ('Ellyse_Perry', 23),\n",
       "  ('Ellyse_Perry', 0),\n",
       "  ('Ellyse_Perry', 1),\n",
       "  ('Ellyse_Perry', 5),\n",
       "  ('Ellyse_Perry', 0),\n",
       "  ('Ellyse_Perry', 1),\n",
       "  ('Ellyse_Perry', 5),\n",
       "  ('Ellyse_Perry', 6),\n",
       "  ('Ellyse_Perry', 7),\n",
       "  ('Ellyse_Perry', 11),\n",
       "  ('Ellyse_Perry', 12),\n",
       "  ('Ellyse_Perry', 16),\n",
       "  ('Ellyse_Perry', 17),\n",
       "  ('Ellyse_Perry', 18),\n",
       "  ('Ellyse_Perry', 15),\n",
       "  ('Ellyse_Perry', 21),\n",
       "  ('Ellyse_Perry', 22),\n",
       "  ('Ellyse_Perry', 23)],\n",
       " [('American_Idiot', 17), ('American_Idiot', 17)],\n",
       " [('Wyatt_Earp', 0)],\n",
       " [('Steve_Jobs', 0)],\n",
       " [('Destiny_-LRB-video_game-RRB-', 1)],\n",
       " [('Heather_Graham', 7)],\n",
       " [('Battle_Studies_World_Tour', 0)],\n",
       " [('Samuel_L._Jackson', 2)],\n",
       " [('Singapore', 1)],\n",
       " [('Jennifer_Hudson', 1)],\n",
       " [('Hong_Kong', 15)],\n",
       " [('Manchester_City_F.C.', 7)],\n",
       " [('Unapologetic', 0), ('Unapologetic', 4)],\n",
       " [('Andre_Agassi', 0), ('Andre_Agassi', 1)],\n",
       " [('Bangladesh', 40)],\n",
       " [('Jon_Pertwee', 5)],\n",
       " [('Lee_Child', 0)],\n",
       " [('Tequila', 0), ('Tequila', 11)],\n",
       " [('Michael_Phelps', 7)],\n",
       " [('Donald_Glover', 11),\n",
       "  ('Donald_Glover', 12),\n",
       "  ('Donald_Glover', 13),\n",
       "  ('Donald_Glover', 15)],\n",
       " [('FC_Barcelona', 11)],\n",
       " [('Ratatouille_-LRB-film-RRB-', 1)],\n",
       " [('Kurt_Angle', 3)],\n",
       " [('Iron_Fist_-LRB-TV_series-RRB-', 0)],\n",
       " [('Dangerous_Beauty', 0)],\n",
       " [('The_Avengers_-LRB-2012_film-RRB-', 2),\n",
       "  ('The_Avengers_-LRB-2012_film-RRB-', 3)],\n",
       " [('Jerry_Goldsmith', 4)],\n",
       " [('Charles,_Prince_of_Wales', 16)],\n",
       " [('30_Rock', 8), ('30_Rock', 8)],\n",
       " [('Nicki_Minaj', 0),\n",
       "  ('Nicki_Minaj', 2),\n",
       "  ('Nicki_Minaj', 5),\n",
       "  ('Nicki_Minaj', 6),\n",
       "  ('Nicki_Minaj', 9),\n",
       "  ('Nicki_Minaj', 12),\n",
       "  ('Nicki_Minaj', 16)],\n",
       " [('Margaret_of_Valois', 13)],\n",
       " [('Dayton_Agreement', 0), ('Dayton_Agreement', 0)],\n",
       " [('As_the_World_Turns', 23)],\n",
       " [('Princess_Agents', 0)],\n",
       " [('The_Vampire_Diaries', 1),\n",
       "  ('The_Vampire_Diaries', 1),\n",
       "  ('The_Vampire_Diaries', 14),\n",
       "  ('The_Vampire_Diaries', 13),\n",
       "  ('The_Vampire_Diaries', 16),\n",
       "  ('The_Vampire_Diaries', 15),\n",
       "  ('The_Vampire_Diaries', 1),\n",
       "  ('The_Vampire_Diaries', 13),\n",
       "  ('The_Vampire_Diaries', 14),\n",
       "  ('The_Vampire_Diaries', 15),\n",
       "  ('The_Vampire_Diaries', 16),\n",
       "  ('The_Vampire_Diaries', 1),\n",
       "  ('The_Vampire_Diaries', 1),\n",
       "  ('The_Vampire_Diaries', 13),\n",
       "  ('The_Vampire_Diaries', 14),\n",
       "  ('The_Vampire_Diaries', 15),\n",
       "  ('The_Vampire_Diaries', 16),\n",
       "  ('The_Vampire_Diaries', 13),\n",
       "  ('The_Vampire_Diaries', 14),\n",
       "  ('The_Vampire_Diaries', 15),\n",
       "  ('The_Vampire_Diaries', 16)],\n",
       " [('The_Ten_Commandments_-LRB-1956_film-RRB-', 0)],\n",
       " [('The_Bends', 0)],\n",
       " [('Alandi', 0)],\n",
       " [('Peggy_Sue_Got_Married', 0),\n",
       "  ('Peggy_Sue_Got_Married', 0),\n",
       "  ('Peggy_Sue_Got_Married', 0)],\n",
       " [('Role_Models', 0)],\n",
       " [('Gerald_Ford', 0), ('Gerald_Ford', 1)],\n",
       " [('Will_Ferrell', 1),\n",
       "  ('Will_Ferrell', 3),\n",
       "  ('Will_Ferrell', 7),\n",
       "  ('Will_Ferrell', 1),\n",
       "  ('Will_Ferrell', 3),\n",
       "  ('Will_Ferrell', 7)],\n",
       " [('Marshall_McLuhan', 5)],\n",
       " [('Walt_Disney', 26), ('Walt_Disney', 9)],\n",
       " [('Brock_Lesnar', 18)],\n",
       " [('Guam', 1), ('Guam', 0)],\n",
       " [('Sweden', 0)],\n",
       " [('David_Dhawan', 0)],\n",
       " [('International_relations', 10), ('International_relations', 9)],\n",
       " [('Underworld_-LRB-2003_film-RRB-', 3)],\n",
       " [('Avenged_Sevenfold', 6),\n",
       "  ('Avenged_Sevenfold', 8),\n",
       "  ('Avenged_Sevenfold', 10),\n",
       "  ('Avenged_Sevenfold', 11),\n",
       "  ('Avenged_Sevenfold', 13),\n",
       "  ('Avenged_Sevenfold', 14),\n",
       "  ('Avenged_Sevenfold', 17)],\n",
       " [('Brock_Lesnar', 18)],\n",
       " [('Romelu_Lukaku', 0)],\n",
       " [('Toy_Story', 0)],\n",
       " [('Al_Capone', 17)],\n",
       " [('Chet_Atkins', 8)],\n",
       " [('Trevor_Baxter', 1)],\n",
       " [('Timon_&_Pumbaa_-LRB-TV_series-RRB-', 9),\n",
       "  ('Timon_&_Pumbaa_-LRB-TV_series-RRB-', 9)],\n",
       " [('Zach_Galifianakis', 2)],\n",
       " [('Angela_Lansbury', 8), ('Angela_Lansbury', 8)],\n",
       " [('Hotel_Transylvania_2', 11)],\n",
       " [('Born_Naked', 0)],\n",
       " [('Reba_McEntire', 1), ('Reba_McEntire', 1)],\n",
       " [('Sleep', 13)],\n",
       " [('Frank_-LRB-Amy_Winehouse_album-RRB-', 0)],\n",
       " [('Robert_Redford', 4)],\n",
       " [('Katie_Stevens', 0)],\n",
       " [('Haiti', 27)],\n",
       " [('Anne_Hathaway', 2)],\n",
       " [('Twitter', 3)],\n",
       " [('Venus_Williams', 18), ('Venus_Williams', 33)],\n",
       " [('Red_-LRB-2010_film-RRB-', 0)],\n",
       " [('J._Cole', 0)],\n",
       " [('The_Playboy_Club', 2)],\n",
       " [('Randy_Orton', 5),\n",
       "  ('Randy_Orton', 4),\n",
       "  ('Randy_Orton', 1),\n",
       "  ('Randy_Orton', 0),\n",
       "  ('Randy_Orton', 25),\n",
       "  ('Randy_Orton', 24),\n",
       "  ('Randy_Orton', 21),\n",
       "  ('Randy_Orton', 20),\n",
       "  ('Randy_Orton', 19),\n",
       "  ('Randy_Orton', 18),\n",
       "  ('Randy_Orton', 17),\n",
       "  ('Randy_Orton', 16),\n",
       "  ('Randy_Orton', 15),\n",
       "  ('Randy_Orton', 14),\n",
       "  ('Randy_Orton', 13),\n",
       "  ('Randy_Orton', 12),\n",
       "  ('Randy_Orton', 11),\n",
       "  ('Randy_Orton', 8),\n",
       "  ('Randy_Orton', 7),\n",
       "  ('Randy_Orton', 6)],\n",
       " [('Rainn_Wilson', 4)],\n",
       " [('Zeus', 5)],\n",
       " [('Matt_Kuchar', 0)],\n",
       " [('Burundi', 6), ('Burundi', 29)],\n",
       " [('Foo_Fighters', 2)],\n",
       " [('Nickelodeon', 1), ('Nickelodeon', 1)],\n",
       " [('Arrested_Development_-LRB-TV_series-RRB-', 9)],\n",
       " [('Eminem', 16)],\n",
       " [('David_Thewlis', 2)],\n",
       " [('Led_Zeppelin', 0)],\n",
       " [('Dwyane_Wade', 0), ('Dwyane_Wade', 11)],\n",
       " [('Champion_-LRB-1949_film-RRB-', 0)],\n",
       " [('My_Bloody_Valentine_3D', 2)],\n",
       " [('Dwight_Schrute', 0)],\n",
       " [('Beasts_of_the_Southern_Wild', 5)],\n",
       " [('Transformers_-LRB-film-RRB-', 0)],\n",
       " [('Laura_Prepon', 1)],\n",
       " [('Charles_Dickens', 0), ('Charles_Dickens', 6)],\n",
       " [('Michael_Fassbender', 1)],\n",
       " [('Keeping_Up_with_the_Joneses_-LRB-film-RRB-', 2)],\n",
       " [('Pixar', 3)],\n",
       " [('The_Homesman', 1)],\n",
       " [('Peter_Cetera', 0), ('Peter_Cetera', 1)],\n",
       " [('Heartbreak_Ridge', 0)],\n",
       " [('Maggie_Smith', 0)],\n",
       " [('Jackie_Robinson', 0), ('Jackie_Robinson', 19)],\n",
       " [('Ethiopia', 0)],\n",
       " [('Gary_Ridgway', 0)],\n",
       " [('Eva_Longoria', 3)],\n",
       " [('Angela_Lansbury', 4)],\n",
       " [('World_Science_Festival', 0)],\n",
       " [('XXx-COLON-_Return_of_Xander_Cage', 0),\n",
       "  ('XXx-COLON-_Return_of_Xander_Cage', 0)],\n",
       " [('Kid_Rock', 0),\n",
       "  ('Kid_Rock', 7),\n",
       "  ('Kid_Rock', 0),\n",
       "  ('Kid_Rock', 0),\n",
       "  ('Kid_Rock', 0),\n",
       "  ('Kid_Rock', 7),\n",
       "  ('Kid_Rock', 8),\n",
       "  ('Kid_Rock', 9),\n",
       "  ('Kid_Rock', 12),\n",
       "  ('Kid_Rock', 13),\n",
       "  ('Kid_Rock', 14),\n",
       "  ('Kid_Rock', 15),\n",
       "  ('Kid_Rock', 16),\n",
       "  ('Kid_Rock', 19),\n",
       "  ('Kid_Rock', 21),\n",
       "  ('Kid_Rock', 1),\n",
       "  ('Kid_Rock', 2),\n",
       "  ('Kid_Rock', 3),\n",
       "  ('Kid_Rock', 4),\n",
       "  ('Kid_Rock', 0),\n",
       "  ('Kid_Rock', 0)],\n",
       " [('Hillary_Clinton', 17), ('Hillary_Clinton', 17)],\n",
       " [('The_Ten_Commandments_-LRB-1956_film-RRB-', 0)],\n",
       " [('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 1),\n",
       "  ('Kangana_Ranaut', 2),\n",
       "  ('Kangana_Ranaut', 3),\n",
       "  ('Kangana_Ranaut', 6),\n",
       "  ('Kangana_Ranaut', 7),\n",
       "  ('Kangana_Ranaut', 8),\n",
       "  ('Kangana_Ranaut', 9),\n",
       "  ('Kangana_Ranaut', 12),\n",
       "  ('Kangana_Ranaut', 15),\n",
       "  ('Kangana_Ranaut', 16),\n",
       "  ('Kangana_Ranaut', 17),\n",
       "  ('Kangana_Ranaut', 18),\n",
       "  ('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 1),\n",
       "  ('Kangana_Ranaut', 2),\n",
       "  ('Kangana_Ranaut', 3),\n",
       "  ('Kangana_Ranaut', 6),\n",
       "  ('Kangana_Ranaut', 7),\n",
       "  ('Kangana_Ranaut', 8),\n",
       "  ('Kangana_Ranaut', 9),\n",
       "  ('Kangana_Ranaut', 12),\n",
       "  ('Kangana_Ranaut', 15),\n",
       "  ('Kangana_Ranaut', 16),\n",
       "  ('Kangana_Ranaut', 17),\n",
       "  ('Kangana_Ranaut', 18),\n",
       "  ('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 1),\n",
       "  ('Kangana_Ranaut', 2),\n",
       "  ('Kangana_Ranaut', 3),\n",
       "  ('Kangana_Ranaut', 6),\n",
       "  ('Kangana_Ranaut', 7),\n",
       "  ('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 6),\n",
       "  ('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 1),\n",
       "  ('Kangana_Ranaut', 2),\n",
       "  ('Kangana_Ranaut', 3),\n",
       "  ('Kangana_Ranaut', 6),\n",
       "  ('Kangana_Ranaut', 7),\n",
       "  ('Kangana_Ranaut', 8),\n",
       "  ('Kangana_Ranaut', 9),\n",
       "  ('Kangana_Ranaut', 12),\n",
       "  ('Kangana_Ranaut', 15),\n",
       "  ('Kangana_Ranaut', 16),\n",
       "  ('Kangana_Ranaut', 17),\n",
       "  ('Kangana_Ranaut', 18),\n",
       "  ('Kangana_Ranaut', 0),\n",
       "  ('Kangana_Ranaut', 1),\n",
       "  ('Kangana_Ranaut', 2),\n",
       "  ('Kangana_Ranaut', 3),\n",
       "  ('Kangana_Ranaut', 6),\n",
       "  ('Kangana_Ranaut', 7),\n",
       "  ('Kangana_Ranaut', 8),\n",
       "  ('Kangana_Ranaut', 9),\n",
       "  ('Kangana_Ranaut', 11),\n",
       "  ('Kangana_Ranaut', 12),\n",
       "  ('Kangana_Ranaut', 15),\n",
       "  ('Kangana_Ranaut', 16),\n",
       "  ('Kangana_Ranaut', 17),\n",
       "  ('Kangana_Ranaut', 18)],\n",
       " [('Ringo_Starr', 12)],\n",
       " [('Oscar_Robertson', 0)],\n",
       " [('Wyoming', 1)],\n",
       " [('Heartland_-LRB-Canadian_TV_series-RRB-', 0)],\n",
       " [('Following', 0)],\n",
       " [('Jacinda_Barrett', 0), ('Jacinda_Barrett', 0), ('Jacinda_Barrett', 3)],\n",
       " [(\"Assassin's_Creed\", 0)],\n",
       " [('Edgar_Award', 1)],\n",
       " [('Lewis_Hamilton', 0),\n",
       "  ('Lewis_Hamilton', 1),\n",
       "  ('Lewis_Hamilton', 2),\n",
       "  ('Lewis_Hamilton', 23),\n",
       "  ('Lewis_Hamilton', 22),\n",
       "  ('Lewis_Hamilton', 21),\n",
       "  ('Lewis_Hamilton', 20),\n",
       "  ('Lewis_Hamilton', 18),\n",
       "  ('Lewis_Hamilton', 17),\n",
       "  ('Lewis_Hamilton', 16),\n",
       "  ('Lewis_Hamilton', 15),\n",
       "  ('Lewis_Hamilton', 11),\n",
       "  ('Lewis_Hamilton', 8),\n",
       "  ('Lewis_Hamilton', 7),\n",
       "  ('Lewis_Hamilton', 6)],\n",
       " [(\"Girls'_Generation\", 1)],\n",
       " [('Pirates_of_the_Caribbean_-LRB-film_series-RRB-', 8)],\n",
       " [('Ad-Rock', 2)],\n",
       " [('Gianluigi_Buffon', 24)],\n",
       " [('Rhona_Mitra', 0)],\n",
       " [('Lauren_Bacall', 11),\n",
       "  ('Lauren_Bacall', 11),\n",
       "  ('Lauren_Bacall', 11),\n",
       "  ('Lauren_Bacall', 11),\n",
       "  ('Lauren_Bacall', 11)],\n",
       " [('Led_Zeppelin_II', 0)],\n",
       " [('Brian_Helgeland', 1)],\n",
       " [('De_Profundis_-LRB-letter-RRB-', 0),\n",
       "  ('De_Profundis_-LRB-letter-RRB-', 0),\n",
       "  ('De_Profundis_-LRB-letter-RRB-', 0),\n",
       "  ('De_Profundis_-LRB-letter-RRB-', 9),\n",
       "  ('De_Profundis_-LRB-letter-RRB-', 0),\n",
       "  ('De_Profundis_-LRB-letter-RRB-', 0),\n",
       "  ('De_Profundis_-LRB-letter-RRB-', 9)],\n",
       " [('Planet_of_the_Apes_-LRB-1968_film-RRB-', 0)],\n",
       " [('Radioactive_-LRB-Imagine_Dragons_song-RRB-', 12)],\n",
       " [('Odyssey', 6)],\n",
       " [('Houston', 11)],\n",
       " [('Nice_&_Slow', 0)],\n",
       " [('Lewis_Hamilton', 0),\n",
       "  ('Lewis_Hamilton', 7),\n",
       "  ('Lewis_Hamilton', 8),\n",
       "  ('Lewis_Hamilton', 22)],\n",
       " [('Nicole_Scherzinger', 15)],\n",
       " [('Turkey', 30)],\n",
       " [('Emilio_Estevez', 0), ('Emilio_Estevez', 1)],\n",
       " [('Albanians', 17)],\n",
       " [('Mauritius', 17)],\n",
       " [('Star_vs._the_Forces_of_Evil', 0)],\n",
       " [('The_Ren_&_Stimpy_Show', 2),\n",
       "  ('The_Ren_&_Stimpy_Show', 5),\n",
       "  ('The_Ren_&_Stimpy_Show', 0)],\n",
       " [('Krysten_Ritter', 3)],\n",
       " [('The_Incredible_Hulk_-LRB-film-RRB-', 1)],\n",
       " [('A_Perfect_Circle', 0)],\n",
       " [('Farrah_Fawcett', 17)],\n",
       " [('Judy_Greer', 2), ('Judy_Greer', 0)],\n",
       " [('Eliza_Dushku', 0)],\n",
       " [('Little_Mix', 0)],\n",
       " [('Zindagi_Na_Milegi_Dobara', 2)],\n",
       " [('Baahubali-COLON-_The_Beginning', 0),\n",
       "  ('Baahubali-COLON-_The_Beginning', 13),\n",
       "  ('Baahubali-COLON-_The_Beginning', 17)],\n",
       " [('Christina_Aguilera', 2),\n",
       "  ('Christina_Aguilera', 5),\n",
       "  ('Christina_Aguilera', 7),\n",
       "  ('Christina_Aguilera', 6)],\n",
       " [('Bill_Gates', 4)],\n",
       " [('Keanu_Reeves', 16)],\n",
       " [('Gold', 1), ('Gold', 21), ('Gold', 21)],\n",
       " [('Lodging', 7)],\n",
       " [('K-pop_Star_2', 2)],\n",
       " [('Khmer_Empire', 0), ('Khmer_Empire', 1)],\n",
       " [('Gwen_Stefani', 15)],\n",
       " [('Buddy_Holly', 0)],\n",
       " [('Liev_Schreiber', 0)],\n",
       " [('Oakland,_California', 1)],\n",
       " [('Alex_Rodriguez', 0)],\n",
       " [('Nazi_Germany', 1)],\n",
       " [('Keturah', 0), ('Keturah', 1)],\n",
       " [('Patrick_Bateman', 0)],\n",
       " [('Jimmy_Carter', 0),\n",
       "  ('Jimmy_Carter', 0),\n",
       "  ('Jimmy_Carter', 2),\n",
       "  ('Jimmy_Carter', 10),\n",
       "  ('Jimmy_Carter', 11),\n",
       "  ('Jimmy_Carter', 15),\n",
       "  ('Jimmy_Carter', 17),\n",
       "  ('Jimmy_Carter', 19),\n",
       "  ('Jimmy_Carter', 22),\n",
       "  ('Jimmy_Carter', 23),\n",
       "  ('Jimmy_Carter', 26),\n",
       "  ('Jimmy_Carter', 27),\n",
       "  ('Jimmy_Carter', 29)],\n",
       " [('Extracorporeal_shockwave_therapy', 0)],\n",
       " [('Yusuke_Murata', 1)],\n",
       " [('Fred_Savage', 0)],\n",
       " [('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13)],\n",
       " [('Faith_Evans', 0),\n",
       "  ('Faith_Evans', 1),\n",
       "  ('Faith_Evans', 2),\n",
       "  ('Faith_Evans', 6),\n",
       "  ('Faith_Evans', 7),\n",
       "  ('Faith_Evans', 12),\n",
       "  ('Faith_Evans', 13),\n",
       "  ('Faith_Evans', 14)],\n",
       " [('Hot_-LRB-Mel_B_album-RRB-', 0)],\n",
       " [('Natalie_Portman', 2),\n",
       "  ('Natalie_Portman', 6),\n",
       "  ('Natalie_Portman', 8),\n",
       "  ('Natalie_Portman', 11)],\n",
       " [('Havoc_-LRB-2005_film-RRB-', 5)],\n",
       " [('Halle_Berry', 17)],\n",
       " [('Sharon_Tate', 0)],\n",
       " [('Nick_Kroll', 2)],\n",
       " [('Inhumans', 0), ('Inhumans', 0), ('Inhumans', 0)],\n",
       " [('Cloud_Atlas_-LRB-film-RRB-', 1)],\n",
       " [('Thirteen_-LRB-2003_film-RRB-', 0)],\n",
       " [('Caitlyn_Jenner', 12)],\n",
       " [('Slovakia', 21)],\n",
       " [('Little_Boy', 0)],\n",
       " [('Taylor_Schilling', 1)],\n",
       " [('Curse,_Inc.', 3)],\n",
       " [('Odin', 16)],\n",
       " [('Iain_Glen', 0)],\n",
       " [('Just_Dance_-LRB-song-RRB-', 9)],\n",
       " [('Louise_Simonson', 0),\n",
       "  ('Louise_Simonson', 0),\n",
       "  ('Louise_Simonson', 0),\n",
       "  ('Louise_Simonson', 0)],\n",
       " [('Nelson_Mandela', 0)],\n",
       " [('Janet_Varney', 0)],\n",
       " [('Lust_for_Life_-LRB-film-RRB-', 0), ('Lust_for_Life_-LRB-film-RRB-', 0)],\n",
       " [('Alex_Rodriguez', 3)],\n",
       " [('Lee_Min-ho_-LRB-actor,_born_1987-RRB-', 0)],\n",
       " [('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21),\n",
       "  ('Destiny_-LRB-video_game-RRB-', 21)],\n",
       " [('Amsterdam', 23)],\n",
       " [('Event_management', 4)],\n",
       " [('New_York_Giants', 0),\n",
       "  ('New_York_Giants', 1),\n",
       "  ('New_York_Giants', 0),\n",
       "  ('New_York_Giants', 1)],\n",
       " [('The_Secret_Life_of_Us', 1),\n",
       "  ('The_Secret_Life_of_Us', 2),\n",
       "  ('The_Secret_Life_of_Us', 6)],\n",
       " [('The_Dark_Knight_-LRB-film-RRB-', 2)],\n",
       " [('Jamie_Foxx', 1)],\n",
       " [('Anti-nuclear_antibody', 5)],\n",
       " [('Henry_VIII_of_England', 9)],\n",
       " [('How_to_Lose_Friends_&_Alienate_People_-LRB-film-RRB-', 6),\n",
       "  ('How_to_Lose_Friends_&_Alienate_People_-LRB-film-RRB-', 6)],\n",
       " [('Equidae', 0)],\n",
       " [('Ty_Cobb', 1)],\n",
       " [('Felicity_Jones', 10)],\n",
       " [('Jim_Rash', 0)],\n",
       " [('Paris', 26)],\n",
       " [('Toy_Story', 0),\n",
       "  ('Toy_Story', 1),\n",
       "  ('Toy_Story', 4),\n",
       "  ('Toy_Story', 6),\n",
       "  ('Toy_Story', 10),\n",
       "  ('Toy_Story', 12),\n",
       "  ('Toy_Story', 15),\n",
       "  ('Toy_Story', 16),\n",
       "  ('Toy_Story', 17),\n",
       "  ('Toy_Story', 18)],\n",
       " [('Kalpana_Raghavendar', 0)],\n",
       " [('Odia_language', 6)],\n",
       " [('Melisandre', 6)],\n",
       " [('Queen_Latifah', 19)],\n",
       " [('Laadla_-LRB-1994_film-RRB-', 0)],\n",
       " [('Stars_Dance', 0), ('Stars_Dance', 0)],\n",
       " [('Jet_Li', 11),\n",
       "  ('Jet_Li', 11),\n",
       "  ('Jet_Li', 11),\n",
       "  ('Jet_Li', 11),\n",
       "  ('Jet_Li', 11)],\n",
       " [('Suits_-LRB-TV_series-RRB-', 3)],\n",
       " [('Dr._Dre', 13)],\n",
       " [('Derrick_Rose', 5),\n",
       "  ('Derrick_Rose', 5),\n",
       "  ('Derrick_Rose', 5),\n",
       "  ('Derrick_Rose', 5),\n",
       "  ('Derrick_Rose', 5)],\n",
       " [('Paul_Newman', 0),\n",
       "  ('Paul_Newman', 3),\n",
       "  ('Paul_Newman', 0),\n",
       "  ('Paul_Newman', 1),\n",
       "  ('Paul_Newman', 2),\n",
       "  ('Paul_Newman', 3)],\n",
       " [('Charles_I_of_England', 0)],\n",
       " [('San_Francisco', 0)],\n",
       " [('Billy_Clanton', 1)],\n",
       " [('Stagira_-LRB-ancient_city-RRB-', 0)],\n",
       " [('Cara_Delevingne', 2)],\n",
       " [('David_Harbour', 0)],\n",
       " [('Burundi', 20)],\n",
       " [('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13),\n",
       "  ('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13),\n",
       "  ('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13),\n",
       "  ('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13),\n",
       "  ('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13),\n",
       "  ('Where_the_Wild_Things_Are_-LRB-film-RRB-', 13)],\n",
       " [('Furious_7', 0)],\n",
       " [('Sarah_Paulson', 3), ('Sarah_Paulson', 9)],\n",
       " [('Caroline,_Princess_of_Hanover', 5),\n",
       "  ('Caroline,_Princess_of_Hanover', 5),\n",
       "  ('Caroline,_Princess_of_Hanover', 5),\n",
       "  ('Caroline,_Princess_of_Hanover', 5)],\n",
       " [('Sean_Connery', 0)],\n",
       " [('Gujarat', 0)],\n",
       " [('Cancer', 0)],\n",
       " [('Marion_Cotillard', 13)],\n",
       " [('Chandni', 0)],\n",
       " [('Ronald_Reagan', 0)],\n",
       " [('Kill_Bill-COLON-_Volume_2', 1)],\n",
       " [('Music_journalism', 0)],\n",
       " [('Tom_Felton', 0),\n",
       "  ('Tom_Felton', 2),\n",
       "  ('Tom_Felton', 3),\n",
       "  ('Tom_Felton', 4),\n",
       "  ('Tom_Felton', 8),\n",
       "  ('Tom_Felton', 10)],\n",
       " [('Marvel_Cinematic_Universe', 6)],\n",
       " [('Hulk_-LRB-comics-RRB-', 13)],\n",
       " [('Roger_Federer', 0),\n",
       "  ('Roger_Federer', 6),\n",
       "  ('Roger_Federer', 7),\n",
       "  ('Roger_Federer', 8),\n",
       "  ('Roger_Federer', 9),\n",
       "  ('Roger_Federer', 12),\n",
       "  ('Roger_Federer', 0),\n",
       "  ('Roger_Federer', 1),\n",
       "  ('Roger_Federer', 6),\n",
       "  ('Roger_Federer', 9),\n",
       "  ('Roger_Federer', 13),\n",
       "  ('Roger_Federer', 0),\n",
       "  ('Roger_Federer', 1),\n",
       "  ('Roger_Federer', 3),\n",
       "  ('Roger_Federer', 6),\n",
       "  ('Roger_Federer', 7),\n",
       "  ('Roger_Federer', 8),\n",
       "  ('Roger_Federer', 9),\n",
       "  ('Roger_Federer', 12),\n",
       "  ('Roger_Federer', 13),\n",
       "  ('Roger_Federer', 14),\n",
       "  ('Roger_Federer', 15),\n",
       "  ('Roger_Federer', 0)],\n",
       " [('Andy_Kaufman', 0),\n",
       "  ('Andy_Kaufman', 0),\n",
       "  ('Andy_Kaufman', 7),\n",
       "  ('Andy_Kaufman', 8),\n",
       "  ('Andy_Kaufman', 9),\n",
       "  ('Andy_Kaufman', 11)],\n",
       " [('Bruce_Springsteen', 9), ('Bruce_Springsteen', 9)],\n",
       " [('Planet_of_the_Apes_-LRB-1968_film-RRB-', 0),\n",
       "  ('Planet_of_the_Apes_-LRB-1968_film-RRB-', 4)],\n",
       " [('Hayden_Panettiere', 0),\n",
       "  ('Hayden_Panettiere', 5),\n",
       "  ('Hayden_Panettiere', 1),\n",
       "  ('Hayden_Panettiere', 7)],\n",
       " [('Oh_Yeon-seo', 0)],\n",
       " [('Mango', 7)],\n",
       " [('George_Best', 2)],\n",
       " [('Bring_It_On_-LRB-film-RRB-', 1)],\n",
       " [('Roald_Dahl', 0),\n",
       "  ('Roald_Dahl', 0),\n",
       "  ('Roald_Dahl', 1),\n",
       "  ('Roald_Dahl', 0),\n",
       "  ('Roald_Dahl', 1),\n",
       "  ('Roald_Dahl', 11),\n",
       "  ('Roald_Dahl', 12),\n",
       "  ('Roald_Dahl', 7),\n",
       "  ('Roald_Dahl', 0),\n",
       "  ('Roald_Dahl', 5),\n",
       "  ('Roald_Dahl', 12),\n",
       "  ('Roald_Dahl', 13),\n",
       "  ('Roald_Dahl', 14),\n",
       "  ('Roald_Dahl', 0)],\n",
       " [('Hypothyroidism', 1)],\n",
       " [('Jon_Watts', 0)],\n",
       " [('Cosmopolitan_-LRB-magazine-RRB-', 2)],\n",
       " [('Nepal', 7)],\n",
       " [('Wentworth_-LRB-TV_series-RRB-', 1)],\n",
       " [('Fujitsu_iPAD', 0)],\n",
       " [('Cage_Warriors', 1)],\n",
       " [('American_Horror_Story-COLON-_Hotel', 0)],\n",
       " [('Lonesome_Dove_-LRB-miniseries-RRB-', 10)],\n",
       " [('Avatar-COLON-_The_Last_Airbender', 6)],\n",
       " [('Apple_Inc.', 6)],\n",
       " [('Bill_Clinton', 0),\n",
       "  ('Bill_Clinton', 1),\n",
       "  ('Bill_Clinton', 2),\n",
       "  ('Bill_Clinton', 9),\n",
       "  ('Bill_Clinton', 12),\n",
       "  ('Bill_Clinton', 13),\n",
       "  ('Bill_Clinton', 16),\n",
       "  ('Bill_Clinton', 21),\n",
       "  ('Bill_Clinton', 28),\n",
       "  ('Bill_Clinton', 31),\n",
       "  ('Bill_Clinton', 30)],\n",
       " [('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0),\n",
       "  ('Peter_Davison', 0)],\n",
       " [('Troy_Baker', 5)],\n",
       " [('Sons_of_Anarchy', 6)],\n",
       " [('Sarah_Palin', 1)],\n",
       " [('William_Hanna', 5)],\n",
       " [('Taylor_Kitsch', 1), ('Taylor_Kitsch', 4)],\n",
       " [('Rescue_Me_-LRB-U.S._TV_series-RRB-', 0)],\n",
       " [('Azerbaijan', 25)],\n",
       " [('Christopher_Lee', 2)],\n",
       " [('The_Muppet_Christmas_Carol', 0)],\n",
       " [('Mark_Zuckerberg', 16), ('Mark_Zuckerberg', 16), ('Mark_Zuckerberg', 16)],\n",
       " [('Lana_Del_Rey', 1), ('Lana_Del_Rey', 1)],\n",
       " [('Dirt_-LRB-TV_series-RRB-', 9)],\n",
       " [('Beauty_and_the_Beast_-LRB-2017_film-RRB-', 0)],\n",
       " [('Peyton_Manning', 3)],\n",
       " [('Song_Sung_Blue', 0),\n",
       "  ('Song_Sung_Blue', 1),\n",
       "  ('Song_Sung_Blue', 7),\n",
       "  ('Song_Sung_Blue', 6),\n",
       "  ('Song_Sung_Blue', 5),\n",
       "  ('Song_Sung_Blue', 11)],\n",
       " [('Purple_Haze', 0)],\n",
       " [('Land_Rover', 5), ('Land_Rover', 5)],\n",
       " [('Gilmore_Girls', 6)],\n",
       " [('2001-COLON-_A_Space_Odyssey_-LRB-film-RRB-', 9),\n",
       "  ('2001-COLON-_A_Space_Odyssey_-LRB-film-RRB-', 9)],\n",
       " [('Eva_Longoria', 0),\n",
       "  ('Eva_Longoria', 1),\n",
       "  ('Eva_Longoria', 2),\n",
       "  ('Eva_Longoria', 3),\n",
       "  ('Eva_Longoria', 4),\n",
       "  ('Eva_Longoria', 7)],\n",
       " [(\"Texas_Longhorns_men's_basketball\", 9)],\n",
       " [('Ajay_Devgn', 0)],\n",
       " [('Diana_-LRB-album-RRB-', 0)],\n",
       " [('The_Silence_of_the_Lambs_-LRB-film-RRB-', 0)],\n",
       " [('Megan_Fox', 2), ('Megan_Fox', 3), ('Megan_Fox', 4), ('Megan_Fox', 5)],\n",
       " [('Lee_Min-ho_-LRB-actor,_born_1987-RRB-', 2)],\n",
       " [('Leslie_Howard_-LRB-actor-RRB-', 0)],\n",
       " [('New_York_City', 33),\n",
       "  ('New_York_City', 33),\n",
       "  ('New_York_City', 33),\n",
       "  ('New_York_City', 33),\n",
       "  ('New_York_City', 33),\n",
       "  ('New_York_City', 33)],\n",
       " [('John_Wayne_Gacy', 0)],\n",
       " [('Black_Panther_-LRB-film-RRB-', 7),\n",
       "  ('Black_Panther_-LRB-film-RRB-', 7),\n",
       "  ('Black_Panther_-LRB-film-RRB-', 8),\n",
       "  ('Black_Panther_-LRB-film-RRB-', 7),\n",
       "  ('Black_Panther_-LRB-film-RRB-', 7),\n",
       "  ('Black_Panther_-LRB-film-RRB-', 7)],\n",
       " [('Joseph_Stalin', 0)],\n",
       " [('Heather_Watson', 7), ('Heather_Watson', 7)],\n",
       " [('The_Sopranos', 7)],\n",
       " [('England', 10)],\n",
       " [('Anna_Paquin', 2)],\n",
       " [('The_Matrix_Revolutions', 0)],\n",
       " [('Joker_-LRB-character-RRB-', 1)],\n",
       " [('Robert_Wagner', 7)],\n",
       " [('Angelina_Jolie', 8)],\n",
       " [('Dangerous_Liaisons', 0)],\n",
       " [('Michael_Jackson', 27)],\n",
       " [('Courtney_Love', 14)],\n",
       " [('Taiwan', 17)],\n",
       " [('The_Breakfast_Club', 0)],\n",
       " [('Stephen_Rider', 0)],\n",
       " [('Sade_-LRB-singer-RRB-', 0)],\n",
       " [('Edgar_Allan_Poe', 0)],\n",
       " [('Roger_Penrose', 5)],\n",
       " [('Muhammad', 21)],\n",
       " [('Anne_Hathaway', 1),\n",
       "  ('Anne_Hathaway', 2),\n",
       "  ('Anne_Hathaway', 3),\n",
       "  ('Anne_Hathaway', 7),\n",
       "  ('Anne_Hathaway', 8),\n",
       "  ('Anne_Hathaway', 9),\n",
       "  ('Anne_Hathaway', 13),\n",
       "  ('Anne_Hathaway', 14),\n",
       "  ('Anne_Hathaway', 15)],\n",
       " [('Margot_Kidder', 11)],\n",
       " [('Elephant', 33)],\n",
       " [('Three_Men_and_a_Baby', 0)],\n",
       " [('Tyra_Banks', 1), ('Tyra_Banks', 1)],\n",
       " [('Lydia_Hearst', 0), ('Lydia_Hearst', 0)],\n",
       " [('KJ_Apa', 2)],\n",
       " [('Canada', 2)],\n",
       " [('Napoleon', 4)],\n",
       " [('Loss_of_supply', 7)],\n",
       " [('Ringo_Starr', 17), ('Ringo_Starr', 17)],\n",
       " [('Ivan_Lendl', 17)],\n",
       " [('Sunburn_-LRB-film-RRB-', 2),\n",
       "  ('Sunburn_-LRB-film-RRB-', 2),\n",
       "  ('Sunburn_-LRB-film-RRB-', 2),\n",
       "  ('Sunburn_-LRB-film-RRB-', 2),\n",
       "  ('Sunburn_-LRB-film-RRB-', 2)],\n",
       " [('The_Belko_Experiment', 0)],\n",
       " [('84th_Academy_Awards', 13)],\n",
       " [('Michael_Keaton', 8),\n",
       "  ('Michael_Keaton', 8),\n",
       "  ('Michael_Keaton', 8),\n",
       "  ('Michael_Keaton', 8),\n",
       "  ('Michael_Keaton', 8)],\n",
       " [('Newcastle_United_F.C.', 18), ('Newcastle_United_F.C.', 19)],\n",
       " [('Randy_Orton', 1),\n",
       "  ('Randy_Orton', 5),\n",
       "  ('Randy_Orton', 4),\n",
       "  ('Randy_Orton', 7),\n",
       "  ('Randy_Orton', 6),\n",
       "  ('Randy_Orton', 0),\n",
       "  ('Randy_Orton', 8),\n",
       "  ('Randy_Orton', 11),\n",
       "  ('Randy_Orton', 12),\n",
       "  ('Randy_Orton', 13),\n",
       "  ('Randy_Orton', 15),\n",
       "  ('Randy_Orton', 17),\n",
       "  ('Randy_Orton', 19),\n",
       "  ('Randy_Orton', 21),\n",
       "  ('Randy_Orton', 24),\n",
       "  ('Randy_Orton', 25)],\n",
       " [('Adderall', 3)],\n",
       " [('Novak_Djokovic', 0)],\n",
       " [('Thirteen_-LRB-2003_film-RRB-', 0), ('Thirteen_-LRB-2003_film-RRB-', 1)],\n",
       " [('Gwen_Stefani', 1)],\n",
       " [('Spain', 0),\n",
       "  ('Spain', 2),\n",
       "  ('Spain', 5),\n",
       "  ('Spain', 6),\n",
       "  ('Spain', 0),\n",
       "  ('Spain', 2),\n",
       "  ('Spain', 5),\n",
       "  ('Spain', 6),\n",
       "  ('Spain', 0),\n",
       "  ('Spain', 5),\n",
       "  ('Spain', 0),\n",
       "  ('Spain', 0)],\n",
       " [('War_of_the_Worlds_-LRB-2005_film-RRB-', 0)],\n",
       " [('Mark_Hamill', 0)],\n",
       " [('Thor-COLON-_The_Dark_World', 16), ('Thor-COLON-_The_Dark_World', 16)],\n",
       " [('O._J._Simpson', 3),\n",
       "  ('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 4),\n",
       "  ('O._J._Simpson', 5),\n",
       "  ('O._J._Simpson', 6),\n",
       "  ('O._J._Simpson', 9),\n",
       "  ('O._J._Simpson', 10)],\n",
       " [('Ryan_Phillippe', 7)],\n",
       " [('Augusto_Pinochet', 0), ('Augusto_Pinochet', 27)],\n",
       " [('My_Sweet_Lord', 0)],\n",
       " [('Michael_Keaton', 5)],\n",
       " [('Moana_-LRB-2016_film-RRB-', 0),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 1),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 2),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 10),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 12),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 13),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 0),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 1),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 12),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 13),\n",
       "  ('Moana_-LRB-2016_film-RRB-', 0)],\n",
       " [('Mel_Gibson', 16)],\n",
       " [('Larry_Junstrom', 0), ('Larry_Junstrom', 0)],\n",
       " [('Emily_Blunt', 17)],\n",
       " [('The_Ten_Commandments_-LRB-1956_film-RRB-', 0),\n",
       "  ('The_Ten_Commandments_-LRB-1956_film-RRB-', 20)],\n",
       " [('Demi_Lovato', 28)],\n",
       " [('Ayutthaya_Kingdom', 1)],\n",
       " [('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 10),\n",
       "  ('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 0),\n",
       "  ('O._J._Simpson', 10)],\n",
       " [('Jennifer_Grey', 0)],\n",
       " [('Ben_Whishaw', 1)],\n",
       " [('Arrow_-LRB-TV_series-RRB-', 15)],\n",
       " [('The_Narrows_-LRB-film-RRB-', 0)],\n",
       " [('Academy_Honorary_Award', 0)],\n",
       " [('Caitlyn_Jenner', 11)],\n",
       " [('Travis_Barker', 1),\n",
       "  ('Travis_Barker', 2),\n",
       "  ('Travis_Barker', 6),\n",
       "  ('Travis_Barker', 9),\n",
       "  ('Travis_Barker', 10),\n",
       "  ('Travis_Barker', 11),\n",
       "  ('Travis_Barker', 14),\n",
       "  ('Travis_Barker', 0),\n",
       "  ('Travis_Barker', 5),\n",
       "  ('Travis_Barker', 8)],\n",
       " [('Judith_Quiney', 0)],\n",
       " [('Sam_Worthington', 0), ('Sam_Worthington', 0)],\n",
       " [('Ron_Perlman', 0)],\n",
       " [('United_States', 29)],\n",
       " [('Henry_V_-LRB-2012_film-RRB-', 0)],\n",
       " [('Neal_Schon', 0)],\n",
       " [('Chris_Evans_-LRB-actor-RRB-', 5)],\n",
       " [('Lily_James', 6)],\n",
       " [('Legally_Blonde', 1), ('Legally_Blonde', 1)],\n",
       " [('Major_League_Soccer', 4)],\n",
       " [('Ronald_Reagan', 22)],\n",
       " [('Joseph_Stalin', 0),\n",
       "  ('Joseph_Stalin', 1),\n",
       "  ('Joseph_Stalin', 2),\n",
       "  ('Joseph_Stalin', 10),\n",
       "  ('Joseph_Stalin', 23),\n",
       "  ('Joseph_Stalin', 29)],\n",
       " [('Facebook', 0)],\n",
       " [('Miley_Cyrus', 7)],\n",
       " [('Adrienne_Bailon', 0)],\n",
       " [('Cosmopolitan_-LRB-magazine-RRB-', 2)],\n",
       " [('Apocalypse_Now', 13)],\n",
       " [('Instagram', 11)],\n",
       " [('Rio_de_Janeiro', 1)],\n",
       " [('Reba_McEntire', 1), ('Reba_McEntire', 1)],\n",
       " [('David_Beckham', 0)],\n",
       " [('Sherlock_-LRB-TV_series-RRB-', 2),\n",
       "  ('Sherlock_-LRB-TV_series-RRB-', 2),\n",
       "  ('Sherlock_-LRB-TV_series-RRB-', 0)],\n",
       " [('William_Hanna', 0),\n",
       "  ('William_Hanna', 3),\n",
       "  ('William_Hanna', 4),\n",
       "  ('William_Hanna', 5),\n",
       "  ('William_Hanna', 6),\n",
       "  ('William_Hanna', 7),\n",
       "  ('William_Hanna', 8),\n",
       "  ('William_Hanna', 9),\n",
       "  ('William_Hanna', 14),\n",
       "  ('William_Hanna', 13)],\n",
       " [('Faith_Evans', 7)],\n",
       " [('The_Invention_of_Lying', 0)],\n",
       " [('Batman', 1)],\n",
       " [('Saving_Private_Ryan', 6)],\n",
       " [('Arrow_-LRB-TV_series-RRB-', 2)],\n",
       " [('The_Who', 20)],\n",
       " [('Life_After_Death', 0)],\n",
       " [('Las_Vegas', 0),\n",
       "  ('Las_Vegas', 0),\n",
       "  ('Las_Vegas', 1),\n",
       "  ('Las_Vegas', 2),\n",
       "  ('Las_Vegas', 6),\n",
       "  ('Las_Vegas', 9),\n",
       "  ('Las_Vegas', 13),\n",
       "  ('Las_Vegas', 0),\n",
       "  ('Las_Vegas', 2),\n",
       "  ('Las_Vegas', 0),\n",
       "  ('Las_Vegas', 1),\n",
       "  ('Las_Vegas', 2),\n",
       "  ('Las_Vegas', 7),\n",
       "  ('Las_Vegas', 13),\n",
       "  ('Las_Vegas', 0),\n",
       "  ('Las_Vegas', 0),\n",
       "  ('Las_Vegas', 2)],\n",
       " [('Michael_Fassbender', 9)],\n",
       " [('Noam_Chomsky', 0)],\n",
       " [('2017_Open_Championship', 0),\n",
       "  ('2017_Open_Championship', 0),\n",
       "  ('2017_Open_Championship', 0),\n",
       "  ('2017_Open_Championship', 0),\n",
       "  ('2017_Open_Championship', 0),\n",
       "  ('2017_Open_Championship', 0)],\n",
       " [('Kendrick_Lamar', 1)],\n",
       " [('Cirrhosis', 4)],\n",
       " [('Saved_-LRB-play-RRB-', 19)],\n",
       " [('Marc_Maron', 3),\n",
       "  ('Marc_Maron', 3),\n",
       "  ('Marc_Maron', 3),\n",
       "  ('Marc_Maron', 4),\n",
       "  ('Marc_Maron', 3),\n",
       "  ('Marc_Maron', 4)],\n",
       " [('Above_the_Law_-LRB-film-RRB-', 0)],\n",
       " [('Bruno_Mars', 20)],\n",
       " [('Stevie_Ray_Vaughan', 5), ('Stevie_Ray_Vaughan', 6)],\n",
       " [('George_Lucas', 1), ('George_Lucas', 1)],\n",
       " [('Purple_Haze', 0),\n",
       "  ('Purple_Haze', 1),\n",
       "  ('Purple_Haze', 4),\n",
       "  ('Purple_Haze', 8),\n",
       "  ('Purple_Haze', 9)],\n",
       " [('School_2013', 1)],\n",
       " [('Ajay_Devgn', 0)],\n",
       " [('Anti-nuclear_antibody', 5),\n",
       "  ('Anti-nuclear_antibody', 5),\n",
       "  ('Anti-nuclear_antibody', 5),\n",
       "  ('Anti-nuclear_antibody', 5)],\n",
       " [('Dirt_-LRB-TV_series-RRB-', 9)],\n",
       " [('United_Kingdom', 11)],\n",
       " [('Diego_Costa', 0)],\n",
       " [('Indiana_Jones', 7)],\n",
       " [('Arkham_Asylum', 5), ('Arkham_Asylum', 5)],\n",
       " [('Mao_Zedong', 20)],\n",
       " [('Man_of_Steel_-LRB-film-RRB-', 0)],\n",
       " [('2001-COLON-_A_Space_Odyssey_-LRB-film-RRB-', 9),\n",
       "  ('2001-COLON-_A_Space_Odyssey_-LRB-film-RRB-', 10)],\n",
       " [('Abigail_Breslin', 0),\n",
       "  ('Abigail_Breslin', 1),\n",
       "  ('Abigail_Breslin', 2),\n",
       "  ('Abigail_Breslin', 3)],\n",
       " [('Music_video_director', 8)],\n",
       " [('Victor_Hugo', 7)],\n",
       " [('Nicole_Kidman', 0)],\n",
       " [('Ryan_Gosling', 8)],\n",
       " [('Gone_with_the_Wind_-LRB-film-RRB-', 0),\n",
       "  ('Gone_with_the_Wind_-LRB-film-RRB-', 1),\n",
       "  ('Gone_with_the_Wind_-LRB-film-RRB-', 14),\n",
       "  ('Gone_with_the_Wind_-LRB-film-RRB-', 17)],\n",
       " [('Juana_la_virgen', 0),\n",
       "  ('Juana_la_virgen', 0),\n",
       "  ('Juana_la_virgen', 0),\n",
       "  ('Juana_la_virgen', 0)],\n",
       " [('Frederick_Trump', 0)],\n",
       " [('Maisie_Williams', 0),\n",
       "  ('Maisie_Williams', 1),\n",
       "  ('Maisie_Williams', 2),\n",
       "  ('Maisie_Williams', 6),\n",
       "  ('Maisie_Williams', 1),\n",
       "  ('Maisie_Williams', 2),\n",
       "  ('Maisie_Williams', 6)],\n",
       " [('Beastie_Boys', 0)],\n",
       " [('World_War_II', 8), ('World_War_II', 19), ('World_War_II', 24)],\n",
       " [('Ron_Perlman', 0)],\n",
       " [('Michael_Schumacher', 0)],\n",
       " [('Central_venous_catheter', 0)],\n",
       " [('Ellen_DeGeneres', 0)],\n",
       " [('Kaakha_Kaakha', 0),\n",
       "  ('Kaakha_Kaakha', 1),\n",
       "  ('Kaakha_Kaakha', 2),\n",
       "  ('Kaakha_Kaakha', 3)],\n",
       " [('Lymphoma', 0),\n",
       "  ('Lymphoma', 9),\n",
       "  ('Lymphoma', 11),\n",
       "  ('Lymphoma', 30),\n",
       "  ('Lymphoma', 23)],\n",
       " [('The_Godfather_Part_III', 9)],\n",
       " [('Michael_Winterbottom', 7)],\n",
       " [('David_Carradine', 0),\n",
       "  ('David_Carradine', 1),\n",
       "  ('David_Carradine', 2),\n",
       "  ('David_Carradine', 6),\n",
       "  ('David_Carradine', 7),\n",
       "  ('David_Carradine', 9),\n",
       "  ('David_Carradine', 14),\n",
       "  ('David_Carradine', 0),\n",
       "  ('David_Carradine', 7)],\n",
       " [('Dangerous_Beauty', 0)],\n",
       " [('The_Leftovers_-LRB-TV_series-RRB-', 0),\n",
       "  ('The_Leftovers_-LRB-TV_series-RRB-', 4)],\n",
       " [('Inhumans_-LRB-TV_series-RRB-', 0)],\n",
       " [('Sean_Combs', 2)],\n",
       " [(\"Girls'_Generation\", 0)],\n",
       " [('Varun_Dhawan', 6)],\n",
       " [('Lady_Gaga', 0)],\n",
       " [('100_Greatest_of_All_Time', 0)],\n",
       " [('Los_Angeles_Lakers', 24)],\n",
       " [('Rick_and_Morty', 8)],\n",
       " [('Gustave_Eiffel', 3)],\n",
       " [('Sean_Connery', 6)],\n",
       " [('Telangana', 3)],\n",
       " [('Karl_Urban', 1)],\n",
       " [('Steve_Mouzakis', 3)],\n",
       " [('Lithuania', 21), ('Lithuania', 24)],\n",
       " [('Mel_Gibson', 0)],\n",
       " [('Boston_Legal', 2)],\n",
       " [('Tom_Morello', 1), ('Tom_Morello', 10)],\n",
       " [('United_States', 30)],\n",
       " [('Lewis_Hamilton', 17)],\n",
       " [('Caspian_Sea', 2)],\n",
       " [('Peter_Cetera', 19)],\n",
       " [('Hayden_Panettiere', 5), ('Hayden_Panettiere', 5)],\n",
       " [('Molly_Bernard', 0)],\n",
       " [('Thirteen_Reasons_Why', 1)],\n",
       " [('The_Twilight_Saga_-LRB-film_series-RRB-', 1)],\n",
       " [('Iain_Glen', 0), ('Iain_Glen', 1), ('Iain_Glen', 2)],\n",
       " [('Shallow_Hal', 0)],\n",
       " [('Massachusetts', 1)],\n",
       " [('Kingdom_Hearts', 6)],\n",
       " [('CBS', 9)],\n",
       " [('Theodore_Roosevelt', 0),\n",
       "  ('Theodore_Roosevelt', 1),\n",
       "  ('Theodore_Roosevelt', 2),\n",
       "  ('Theodore_Roosevelt', 12),\n",
       "  ('Theodore_Roosevelt', 13),\n",
       "  ('Theodore_Roosevelt', 19),\n",
       "  ('Theodore_Roosevelt', 25),\n",
       "  ('Theodore_Roosevelt', 27),\n",
       "  ('Theodore_Roosevelt', 31),\n",
       "  ('Theodore_Roosevelt', 33)],\n",
       " [('Zambia', 0)],\n",
       " [('Horseshoe_Falls', 0)],\n",
       " [('Batman_Begins', 14)],\n",
       " [('Algeria', 8), ('Algeria', 8), ('Algeria', 8), ('Algeria', 8)],\n",
       " [('George_H._W._Bush', 9)],\n",
       " [('To_Kill_a_Mockingbird', 5), ('To_Kill_a_Mockingbird', 10)],\n",
       " [('Stars_Are_Blind', 2)],\n",
       " [('Estella_Warren', 0),\n",
       "  ('Estella_Warren', 1),\n",
       "  ('Estella_Warren', 2),\n",
       "  ('Estella_Warren', 5)],\n",
       " [('Myles_Kennedy', 6)],\n",
       " [('Andrew_Garfield', 0)],\n",
       " [('Amanda_Peet', 5)],\n",
       " [('L.A._Guns', 0)],\n",
       " [('Famous_in_Love', 1)],\n",
       " [('Hungary', 1)],\n",
       " [('Helena_Bonham_Carter', 2)],\n",
       " [('Karan_Johar', 4), ('Karan_Johar', 6)],\n",
       " [('All_the_Pretty_Horses_-LRB-film-RRB-', 0),\n",
       "  ('All_the_Pretty_Horses_-LRB-film-RRB-', 0)],\n",
       " [('Lauren_Graham', 1)],\n",
       " [('Kolyma', 9)],\n",
       " [('Christina_Aguilera', 1)],\n",
       " [('To_the_Bone_-LRB-film-RRB-', 1)],\n",
       " [('Master_of_None', 0)],\n",
       " [('Tom_Cruise', 12),\n",
       "  ('Tom_Cruise', 12),\n",
       "  ('Tom_Cruise', 12),\n",
       "  ('Tom_Cruise', 12),\n",
       "  ('Tom_Cruise', 12),\n",
       "  ('Tom_Cruise', 12)],\n",
       " [('Soundgarden', 6)],\n",
       " [('September_11_attacks', 3),\n",
       "  ('September_11_attacks', 8),\n",
       "  ('September_11_attacks', 10),\n",
       "  ('September_11_attacks', 11)],\n",
       " [('Jeff_Bezos', 11)],\n",
       " [('Brown_bear', 7), ('Brown_bear', 7)],\n",
       " [('Phylogenetic_tree', 0),\n",
       "  ('Phylogenetic_tree', 0),\n",
       "  ('Phylogenetic_tree', 0),\n",
       "  ('Phylogenetic_tree', 0)],\n",
       " [('Michael_Caine', 4)],\n",
       " [('Prison_Break', 0)],\n",
       " [('Central_America', 2)],\n",
       " [('John_D._Rockefeller', 7),\n",
       "  ('John_D._Rockefeller', 7),\n",
       "  ('John_D._Rockefeller', 7),\n",
       "  ('John_D._Rockefeller', 7),\n",
       "  ('John_D._Rockefeller', 7),\n",
       "  ('John_D._Rockefeller', 7)],\n",
       " [('Norway', 17), ('Norway', 17)],\n",
       " [('Daz_Dillinger', 0)],\n",
       " [('Matthew_Modine', 1)],\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making a clean evidence list\n",
    "\n",
    "clean_evidence = []\n",
    "\n",
    "for evidence in one_line_evidence:\n",
    "    \n",
    "    one_claim_list = []\n",
    "    for each in evidence:\n",
    "        one_evid = each[0]\n",
    "        \n",
    "        document = one_evid[2]\n",
    "        line = one_evid[3]\n",
    "        \n",
    "        one_claim_list.append((document, line))\n",
    "    \n",
    "    clean_evidence.append(one_claim_list)\n",
    "    \n",
    "clean_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a dicitonary of all required articles and lines\n",
    "all_required_dict = {}\n",
    "\n",
    "for evidence in clean_evidence:\n",
    "    \n",
    "    key_val = unicodedata.normalize('NFC', evidence[0][0])\n",
    "    \n",
    "    for each in evidence:\n",
    "        if key_val in all_required_dict:\n",
    "            all_required_dict[key_val].append(each[1])\n",
    "            all_required_dict[key_val] = list(set(all_required_dict[key_val]))\n",
    "        \n",
    "        else:\n",
    "            all_required_dict[key_val] = [each[1]] \n",
    "            \n",
    "all_required_dict\n",
    "\n",
    "#Retrieving just the article titles\n",
    "all_required_articles = list(all_required_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:53<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "veri_lines_dict = {}\n",
    "number_of_lines = {}  #number of lines storing later for negative sampling irrelevant sentences\n",
    "\n",
    "bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    iD = json.loads(line)['id']\n",
    "                    \n",
    "                    iD = unicodedata.normalize('NFC',iD)\n",
    "                        \n",
    "                    if iD in all_required_dict:\n",
    "           \n",
    "                        doc_lines = json.loads(line)['lines']\n",
    "                                                \n",
    "                        doc_line_list = doc_lines.split('\\n')\n",
    "                        \n",
    "                        number_of_lines[iD] = len(doc_line_list)\n",
    "                                                                \n",
    "                        for line_number in range(0, len(doc_line_list)):\n",
    "                            doc_line = doc_line_list[line_number].lower()\n",
    "                            \n",
    "                            #Isolating cleaned up version of line\n",
    "                            doc_line = doc_line.split('\\t')[1]\n",
    "                            doc_line = doc_line.split('.')[0]\n",
    "                            \n",
    "                            doc_line = doc_line.split(' ')\n",
    "                            \n",
    "                            #Sorting out the brackets\n",
    "                            doc_line_brac = []\n",
    "                            for word in doc_line:\n",
    "                                \n",
    "                                if word in set(list(bracket_dict.keys())):\n",
    "                                    word = bracket_dict[word]\n",
    "                                    \n",
    "                                doc_line_brac.append(word)\n",
    "                            \n",
    "                            veri_lines_dict[(iD, line_number)] = doc_line_brac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling to create irrelevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_list= []\n",
    "\n",
    "for evidence in clean_evidence:\n",
    "    evid_dict = {}\n",
    "    \n",
    "    for each_evid in evidence:\n",
    "        document = each_evid[0]\n",
    "        document = unicodedata.normalize('NFC',document)\n",
    "        \n",
    "        rel_line = each_evid[1]\n",
    "        \n",
    "        #Creating a dictionary with all\n",
    "        if document in evid_dict:\n",
    "            evid_dict[document].append(rel_line)\n",
    "            evid_dict[document] = list(set(evid_dict[document]))\n",
    "            \n",
    "        else:\n",
    "            evid_dict[document]  = [rel_line]\n",
    "     \n",
    "    try:\n",
    "        total_sentences = number_of_lines[document]\n",
    "        \n",
    "        sample_list = list(range(0,total_sentences))\n",
    "        sample_list = [number for number in sample_list if not number in evid_dict[document]]\n",
    "        \n",
    "    except:\n",
    "        sample_list = 'doc not found'\n",
    "        print(document)\n",
    "        \n",
    "    neg_sample_list.append((document, sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95338\n",
      "95338\n",
      "95338\n"
     ]
    }
   ],
   "source": [
    "#Everything is same length\n",
    "print(len(one_line_claims))\n",
    "print(len(clean_evidence))\n",
    "print(len(neg_sample_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Actual Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_lines_evidence = []\n",
    "\n",
    "for evidence in clean_evidence:\n",
    "    sub_list =[]\n",
    "    \n",
    "    for each in evidence:\n",
    "        each = (unicodedata.normalize('NFC',each[0]), each[1])\n",
    "        \n",
    "        actual_line = veri_lines_dict[each]\n",
    "        actual_line = actual_line[:-1]\n",
    "        sub_list.append(actual_line)\n",
    "        \n",
    "    actual_lines_evidence.append(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating equivalent negative sample for every claim\n",
    "import random\n",
    "random.seed(a=0)\n",
    "\n",
    "neg_lines_evidence = []\n",
    "\n",
    "for idx, evidence in enumerate(clean_evidence):\n",
    "    neg_sub_list =[]\n",
    "    \n",
    "    for each in evidence:\n",
    "        \n",
    "        neg_sample_numbers = neg_sample_list[idx][1]\n",
    "        neg_line_number = random.choice(neg_sample_numbers)\n",
    "        \n",
    "        each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "        neg_line = veri_lines_dict[each]\n",
    "        \n",
    "        #while loop to avoid empty list\n",
    "        while len(neg_line) == 0:\n",
    "            neg_line_number = random.choice(neg_sample_numbers)\n",
    "            each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "            neg_line = veri_lines_dict[each]\n",
    "        \n",
    "        neg_line = neg_line[:-1]\n",
    "        neg_sub_list.append(neg_line)\n",
    "    \n",
    "    neg_lines_evidence.append(neg_sub_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating claim evidence pairs and deriving vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise the claims\n",
    "\n",
    "one_line_tokenised = []\n",
    "\n",
    "for claim in one_line_claims:\n",
    "    no_stop = claim[:-1]\n",
    "    lower_claim = no_stop.lower()\n",
    "    tokenised = lower_claim.split(' ')\n",
    "    one_line_tokenised.append(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create verifiable tuples\n",
    "pos_claim_evidence = []\n",
    "neg_claim_evidence = []\n",
    "\n",
    "for idx,evidence in enumerate(actual_lines_evidence):\n",
    "    for each in evidence:\n",
    "        veri_tuples = (one_line_tokenised[idx], each)\n",
    "        pos_claim_evidence.append(veri_tuples)\n",
    "        \n",
    "for idx, evidence in enumerate(neg_lines_evidence):\n",
    "    for each in evidence:\n",
    "        neg_tuples = (one_line_tokenised[idx], each)\n",
    "        neg_claim_evidence.append(neg_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197514/197514 [00:09<00:00, 20213.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#Relevant arrays\n",
    "verif_arrays = []\n",
    "\n",
    "for pair in tqdm(pos_claim_evidence):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    evidence = pair[1]\n",
    "    evidence_array = np.zeros(50)\n",
    "    for word in evidence:\n",
    "        try:\n",
    "            evidence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, evidence_array])\n",
    "    \n",
    "    verif_arrays.append(array_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197514/197514 [00:06<00:00, 32165.65it/s]\n"
     ]
    }
   ],
   "source": [
    "#Irrelevant arrays\n",
    "neg_arrays = []\n",
    "\n",
    "for pair in tqdm(neg_claim_evidence):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    neg_sample = pair[1]\n",
    "    neg_array = np.zeros(50)\n",
    "    for word in neg_sample:\n",
    "        try:\n",
    "            neg_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, neg_array])\n",
    "    \n",
    "    neg_arrays.append(array_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing feature and target matrix \n",
    "\n",
    "veri_matrix = np.array(verif_arrays)\n",
    "neg_matrix = np.array(neg_arrays)\n",
    "rel_y = np.ones(len(verif_arrays))\n",
    "neg_y = np.zeros(len(neg_arrays))\n",
    "feature_matrix = np.concatenate((veri_matrix,neg_matrix), axis=0)\n",
    "target_matrix = np.concatenate((rel_y,neg_y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:12<00:00, 12.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def log_regression(x, y, epochs, learning_rate = 0.001):\n",
    "    \n",
    "    #Adding an intercept term\n",
    "    n = x.shape[0]\n",
    "    one_column = np.ones((x.shape[0],1))   #adding an intercept term\n",
    "    x = np.concatenate((one_column, x), axis = 1)\n",
    "    xT = x.T\n",
    "    \n",
    "    features = x.shape[1]\n",
    "    \n",
    "    #Initialising weights\n",
    "    w = np.zeros(features)\n",
    "    \n",
    "    avg_costs = []\n",
    "    \n",
    "    for epoch in trange(0,epochs):\n",
    "    \n",
    "        #Making prediction        \n",
    "        wx = w@xT\n",
    "        pred = 1/ (1 + np.exp(-wx))\n",
    "\n",
    "        #Calculating cost\n",
    "        class1_cost = -y*np.log(pred)\n",
    "        class0_cost = -(1-y)*np.log(1-pred)\n",
    "        \n",
    "        cost = class1_cost + class0_cost\n",
    "        avg_cost = np.mean(cost)\n",
    "\n",
    "        #Calculating gradient\n",
    "        predT = pred.T\n",
    "\n",
    "        gradient = xT@(predT - y)\n",
    "        gradient = gradient/n #average gradient\n",
    "\n",
    "        gradientT = gradient.T\n",
    "\n",
    "        gradientT *= learning_rate\n",
    "\n",
    "        w -= gradientT\n",
    "        \n",
    "        avg_costs.append(avg_cost)\n",
    "    \n",
    "    return(w, avg_costs)\n",
    "\n",
    "weights, loss_list = log_regression(feature_matrix, target_matrix.T, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b93ab1780>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking loss\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7204780420628412"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_func(feature_matrix, target_matrix, weights):\n",
    "    \n",
    "    one_column = np.ones((feature_matrix.shape[0],1))\n",
    "    feature_matrix = np.concatenate([one_column, feature_matrix ], axis = 1)\n",
    "    \n",
    "    lin_pred = feature_matrix@weights\n",
    "    class_pred = 1/ (1 + np.exp(-lin_pred))\n",
    "    class_pred = np.around(class_pred)\n",
    "\n",
    "    score = 0\n",
    "        \n",
    "    for idx in range(0,len(class_pred)):\n",
    "        val = target_matrix[idx]\n",
    "        pred = class_pred[idx]\n",
    "\n",
    "        if val == pred:\n",
    "            score += 1\n",
    "\n",
    "    accuracy = score/len(class_pred)\n",
    "    \n",
    "    return(class_pred, accuracy)\n",
    "\n",
    "pred, accuracy = accuracy_func(feature_matrix, target_matrix, weights)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part iii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving unique words in the dev set via cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_claim_dicts = []\n",
    "claims_4 = []\n",
    "claim_string_4 = []\n",
    "\n",
    "ten_claims = [137334, 111897, 89891, 181634, 219028, 108281, 204361, 54168, 105095, 18708]\n",
    "\n",
    "with open('data_files/shared_task_dev.jsonl') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "        \n",
    "            claim_dic = json.loads(line)\n",
    "            ID = claim_dic['id']\n",
    "            \n",
    "            if ID in ten_claims:\n",
    "                claim_string_4.append(claim_dic['claim'])\n",
    "                \n",
    "                dev_claim_dicts.append(claim_dic)\n",
    "                text = claim_dic['claim'].lower()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                \n",
    "                claims_4.append(tokens)\n",
    "\n",
    "                \n",
    "#All claim words\n",
    "all_words_4 = []\n",
    "\n",
    "for claim in claims_4:\n",
    "    for word in claim:\n",
    "        all_words_4.append(word)\n",
    "        \n",
    "#Removing stop words from claims\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_words_4 = [word for word in all_words_4 if not word in stop_words] \n",
    "all_words_4 = [stemmer.stem(word) for word in all_words_4]\n",
    "\n",
    "all_words_4 = list(set(all_words_4))\n",
    "all_words_4.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing tf-idf matrix to retrieve documents for 10 claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/109 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-dab3e117159a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0;31m#Stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;31m#Count the words in documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-dab3e117159a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0;31m#Stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;31m#Count the words in documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step2\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alli'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'al'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mbli_rule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'bli'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ble'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mabli_rule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'abli'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'able'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Constructing tf-matrix for the 10 claims that need to be verified\n",
    "\n",
    "tf_matrix_4 = np.zeros((total_doc, len(all_words_4)))\n",
    "current_doc = 0\n",
    "\n",
    "brackets = ['lrb', 'rrb', 'lsb', 'rsb', 'rcb', 'lcb']\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    \n",
    "                    text = json.loads(line)['text']\n",
    "                    text = text.lower()\n",
    "                    tokens = tokenizer.tokenize(text)\n",
    "                    \n",
    "                    #Removing stop words\n",
    "                    tokens = [word for word in tokens if not word in stop_words] \n",
    "                    \n",
    "                    #Removing brackets manually\n",
    "                    tokens = [word for word in tokens if not word in brackets] \n",
    "                    \n",
    "                    #Stemming\n",
    "                    tokens = [stemmer.stem(word) for word in tokens]\n",
    "                    \n",
    "                    #Count the words in documents\n",
    "                    for ind,word in enumerate(all_words_4):\n",
    "                        if word in set(tokens):\n",
    "                            word_tf = tokens.count(word)/len(tokens)\n",
    "                            tf_matrix_4[current_doc][ind] = word_tf\n",
    "                                \n",
    "                    current_doc += 1\n",
    "                    \n",
    "        print(tf_matrix_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_files/tf_matrix_4.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_out = pickle.dumps(tf_matrix_4)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, len(bytes_out), max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_files/tf_matrix_4.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_in = bytearray(0)\n",
    "input_size = os.path.getsize(file_path)\n",
    "with open(file_path, 'rb') as f_in:\n",
    "    for _ in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "tf_matrix_4 = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making IDF matrix for question 4\n",
    "\n",
    "#Counting number of documents with each term in it\n",
    "idf_matrix_4 = np.zeros(len(all_words_4))\n",
    "\n",
    "for index in range(0, len(all_words_4)):\n",
    "    doc_count = np.count_nonzero(tf_matrix_4[:,index])\n",
    "    idf_matrix_4[index] = math.log10(total_doc/doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_4 = tf_matrix_4*idf_matrix_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_claims_4 = []\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for claim in claims_4:\n",
    "    clean_claim = [word for word in claim if not word in stop_words] \n",
    "    clean_claim = [stemmer.stem(word) for word in clean_claim]\n",
    "    clean_claims_4.append(clean_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf for every claim\n",
    "claims_tf_idfs_4 = []\n",
    "\n",
    "for claim in clean_claims_4:\n",
    "    claim_tf_4 = np.zeros(len(all_words_4))\n",
    "    \n",
    "    for idx, word in enumerate(all_words_4):\n",
    "        claim_tf_4[idx] = claim.count(word)/len(claim)\n",
    "            \n",
    "    claim_tf_idf_4 = claim_tf_4*idf_matrix_4\n",
    "    claims_tf_idfs_4.append(claim_tf_idf_4)\n",
    "    \n",
    "#Norm adjusting the matrices\n",
    "norm_rows_docs_4 = []\n",
    "norm_rows_claims_4 = []\n",
    "\n",
    "for row in tf_idf_4:\n",
    "    norm_doc = np.linalg.norm(row)\n",
    "    norm_adj_row = row\n",
    "    \n",
    "    if norm_doc!= 0:\n",
    "        norm_adj_row = norm_adj_row/norm_doc\n",
    "    \n",
    "    norm_rows_docs_4.append(norm_adj_row)\n",
    "        \n",
    "for claim in claims_tf_idfs_4:\n",
    "    norm_claim = np.linalg.norm(claim)\n",
    "    norm_adj_claim = claim/norm_claim\n",
    "    norm_rows_claims_4.append(norm_adj_claim)\n",
    "    \n",
    "doc_norm_tf_idf_4 = np.array(norm_rows_docs_4)\n",
    "\n",
    "#Retrieving 5 highest for each claim\n",
    "top5_list_4 = []\n",
    "\n",
    "for claim in norm_rows_claims_4:\n",
    "    cosine_similarity = doc_norm_tf_idf_4@claim \n",
    "    highest_5_row = cosine_similarity.argsort()[-5:][::1]\n",
    "    \n",
    "    highest_5 = []\n",
    "    for ind in highest_5_row:\n",
    "        highest_5.append(ids[ind])\n",
    "        \n",
    "    highest_5.reverse()    \n",
    "    top5_list_4.append(highest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving list of sentences for every document\n",
    "rel_docs = merged = list(itertools.chain.from_iterable(top5_list_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all the sentences only in the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:58<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "all_lines_dict = {}   #Creating a dictionry of all lines\n",
    "number_of_lines = {}  #number of lines storing later for negative sampling irrelevant sentences\n",
    "\n",
    "bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    iD = json.loads(line)['id']\n",
    "                    \n",
    "                    iD = unicodedata.normalize('NFC',iD)\n",
    "                        \n",
    "                    if iD in rel_docs:\n",
    "           \n",
    "                        doc_lines = json.loads(line)['lines']\n",
    "                                                \n",
    "                        doc_line_list = doc_lines.split('\\n')\n",
    "                        \n",
    "                        number_of_lines[iD] = len(doc_line_list)\n",
    "                                                                \n",
    "                        for line_number in range(0, len(doc_line_list)):\n",
    "                            doc_line = doc_line_list[line_number].lower()\n",
    "                            \n",
    "                            #Isolating cleaned up version of line\n",
    "                            doc_line = doc_line.split('\\t')[1]\n",
    "                            doc_line = doc_line.split('.')[0]\n",
    "                            \n",
    "                            doc_line = doc_line.split(' ')\n",
    "                            \n",
    "                            #Sorting out the brackets\n",
    "                            if len(doc_line) != 0:\n",
    "                            \n",
    "                                doc_line_brac = []\n",
    "                                for word in doc_line:\n",
    "\n",
    "                                    if word in set(list(bracket_dict.keys())):\n",
    "                                        word = bracket_dict[word]\n",
    "\n",
    "                                    doc_line_brac.append(word)\n",
    "\n",
    "                                all_lines_dict[(iD, line_number)] = doc_line_brac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a cleaned up representation of claims and retrieving evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "evidences = []\n",
    "\n",
    "for dic in dev_claim_dicts:\n",
    "    evidences.append(dic['evidence'])\n",
    "    claim = dic['claim']\n",
    "    claim = claim.lower()\n",
    "    claim = claim[:-1]\n",
    "    claim = claim.split(' ')\n",
    "    \n",
    "    no_apos_claim = []\n",
    "    #Removing apostrophe for vector representation later\n",
    "    for word in claim:\n",
    "        if \"'\" in word:\n",
    "            no_apos_claim.append(word[:-2])\n",
    "            no_apos_claim.append(\"'s\")\n",
    "            \n",
    "        else:\n",
    "            no_apos_claim.append(word)\n",
    "    \n",
    "    claims.append(no_apos_claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary mapping document to claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_claim_mapper = {}\n",
    "\n",
    "for idx,docs in enumerate(top5_list_4):\n",
    "    for doc in docs:\n",
    "        doc_claim_mapper[doc] = claims[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the evidence and making a list of relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block to retrieve relevant sentences\n",
    "\n",
    "rel_sent = []\n",
    "for evidence in evidences:\n",
    "    for lst in evidence:\n",
    "        for one in lst:\n",
    "            rel_tuple = (one[2], one[3])\n",
    "            rel_sent.append(rel_tuple)\n",
    "\n",
    "rel_sent = list(set(rel_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nicholas_Brody', 4),\n",
       " ('Telemundo', 5),\n",
       " ('Hispanic_and_Latino_Americans', 0),\n",
       " ('Telemundo', 4),\n",
       " ('Cretaceous', 8),\n",
       " ('Cretaceous', 9),\n",
       " ('Carrie_Mathison', 0),\n",
       " ('Charles_Manson', 17),\n",
       " ('Murda_Beatz', 0),\n",
       " ('Mogadishu', 0),\n",
       " ('Andrew_Kevin_Walker', 0),\n",
       " ('Telemundo', 1),\n",
       " ('Soul_Food_-LRB-film-RRB-', 0),\n",
       " ('Carrie_Mathison', 2),\n",
       " ('Telemundo', 0),\n",
       " ('Savages_-LRB-2012_film-RRB-', 3),\n",
       " ('Nicholas_Brody', 1),\n",
       " ('Damon_Albarn', 17)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying which sentences cosine simlarity picked up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_search_sent = []\n",
    "\n",
    "for sent in rel_sent:\n",
    "    doc = sent[0]\n",
    "    \n",
    "    if doc in rel_docs:\n",
    "        rel_search_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carrie_Mathison', 0),\n",
       " ('Murda_Beatz', 0),\n",
       " ('Andrew_Kevin_Walker', 0),\n",
       " ('Soul_Food_-LRB-film-RRB-', 0),\n",
       " ('Carrie_Mathison', 2)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_search_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a list of sentences to loop and populate 3 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(all_lines_dict.items())\n",
    "\n",
    "claim_list = []\n",
    "sentence_list = []\n",
    "target_list = []\n",
    "\n",
    "for tup in all_sentences:\n",
    "    \n",
    "    #Find the claim\n",
    "    doc = tup[0][0]\n",
    "    claim = doc_claim_mapper[doc]\n",
    "    claim_list.append(claim)\n",
    "    \n",
    "    #Find the sentence\n",
    "    sentence = tup[1]\n",
    "    sentence_list.append(sentence)\n",
    "    \n",
    "    #Find whether relevant or not\n",
    "    check = tup[0]\n",
    "    \n",
    "    target = 0\n",
    "    if check in rel_sent:\n",
    "        target = 1\n",
    "    target_list.append(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vector Representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_arrays = []\n",
    "sentence_arrays = []\n",
    "\n",
    "for claim in claim_list:\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        claim_array += model[word]\n",
    "    claim_arrays.append(claim_array)\n",
    "    \n",
    "\n",
    "for sentence in sentence_list:\n",
    "    sentence_array = np.zeros(50)\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sentence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "    sentence_arrays.append(sentence_array)\n",
    "    \n",
    "claim_matrix = np.array(claim_arrays)\n",
    "sentence_matrix = np.array(sentence_arrays)\n",
    "one_column = np.ones((len(claim_arrays),1))\n",
    "\n",
    "#Making feature matrix\n",
    "feature_matrix_dev = np.concatenate([one_column,claim_matrix, sentence_matrix], axis = 1)\n",
    "target_matrix_dev = np.array(target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making prediction on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7814871016691958"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_pred = feature_matrix_dev@weights\n",
    "class_pred = 1/ (1 + np.exp(-lin_pred))\n",
    "class_pred = np.around(class_pred)\n",
    "\n",
    "score = 0\n",
    "\n",
    "for idx in range(0,len(class_pred)):\n",
    "    val = target_matrix_dev[idx]\n",
    "    pred = class_pred[idx]\n",
    "    \n",
    "    if val == pred:\n",
    "        score += 1\n",
    "        \n",
    "accuracy = score/len(class_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test set for logistic regression in same manner as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:55<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#Recalling same code chunk from before but with shared_task_dev file via function\n",
    "def pair_maker(filename):\n",
    "    \n",
    "    veri_claims = []\n",
    "    veri_evid = []\n",
    "    veri_label = []\n",
    "\n",
    "    binary_claim_dic = {'SUPPORTS':1, 'REFUTES':0}\n",
    "\n",
    "    with open(filename) as openfile:\n",
    "            for iline, line in enumerate(openfile.readlines()):\n",
    "\n",
    "                claim_dic = json.loads(line)\n",
    "                veri_check = claim_dic['verifiable']\n",
    "\n",
    "                if veri_check == 'VERIFIABLE':\n",
    "                    claim = claim_dic['claim']\n",
    "                    evidence = claim_dic['evidence']\n",
    "                    label = binary_claim_dic[claim_dic['label']]\n",
    "\n",
    "                    veri_claims.append(claim)\n",
    "                    veri_evid.append(evidence)\n",
    "                    veri_label.append(label)\n",
    "                    \n",
    "    one_line_claims = []\n",
    "    one_line_evidence = []\n",
    "    one_line_labels = []\n",
    "\n",
    "    for idx, evid in enumerate(veri_evid):\n",
    "        if len(evid[0]) == 1:\n",
    "            one_line_evidence.append(evid)\n",
    "            one_line_claims.append(veri_claims[idx])\n",
    "            one_line_labels.append(veri_label[idx])\n",
    "\n",
    "    clean_evidence = []\n",
    "    \n",
    "    for evidence in one_line_evidence:\n",
    "\n",
    "        one_claim_list = []\n",
    "        for each in evidence:\n",
    "            one_evid = each[0]\n",
    "\n",
    "            document = one_evid[2]\n",
    "            line = one_evid[3]\n",
    "\n",
    "            one_claim_list.append((document, line))\n",
    "\n",
    "        clean_evidence.append(one_claim_list)\n",
    "\n",
    "\n",
    "    #Making a dicitonary of all required articles and lines\n",
    "    all_required_dict = {}\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "\n",
    "        key_val = unicodedata.normalize('NFC', evidence[0][0])\n",
    "\n",
    "        for each in evidence:\n",
    "            if key_val in all_required_dict:\n",
    "                all_required_dict[key_val].append(each[1])\n",
    "                all_required_dict[key_val] = list(set(all_required_dict[key_val]))\n",
    "\n",
    "            else:\n",
    "                all_required_dict[key_val] = [each[1]] \n",
    "\n",
    "    all_required_dict\n",
    "\n",
    "    #Retrieving just the article titles\n",
    "    all_required_articles = list(all_required_dict.keys())\n",
    "\n",
    "    veri_lines_dict = {}\n",
    "    number_of_lines = {}  #number of lines storing later for negative sampling irrelevant sentences\n",
    "\n",
    "    bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "    for file in tqdm(list_of_wiki):\n",
    "            with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                    for iline,line in enumerate(openfile.readlines()):\n",
    "                        iD = json.loads(line)['id']\n",
    "\n",
    "                        iD = unicodedata.normalize('NFC',iD)\n",
    "\n",
    "                        if iD in all_required_dict:\n",
    "\n",
    "                            doc_lines = json.loads(line)['lines']\n",
    "\n",
    "                            doc_line_list = doc_lines.split('\\n')\n",
    "\n",
    "                            number_of_lines[iD] = len(doc_line_list)\n",
    "\n",
    "                            for line_number in range(0, len(doc_line_list)):\n",
    "                                doc_line = doc_line_list[line_number].lower()\n",
    "\n",
    "                                #Isolating cleaned up version of line\n",
    "                                doc_line = doc_line.split('\\t')[1]\n",
    "                                doc_line = doc_line.split('.')[0]\n",
    "\n",
    "                                doc_line = doc_line.split(' ')\n",
    "\n",
    "                                #Sorting out the brackets\n",
    "                                doc_line_brac = []\n",
    "                                for word in doc_line:\n",
    "\n",
    "                                    if word in set(list(bracket_dict.keys())):\n",
    "                                        word = bracket_dict[word]\n",
    "\n",
    "                                    doc_line_brac.append(word)\n",
    "\n",
    "                                veri_lines_dict[(iD, line_number)] = doc_line_brac\n",
    "\n",
    "    neg_sample_list= []\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "        evid_dict = {}\n",
    "\n",
    "        for each_evid in evidence:\n",
    "            document = each_evid[0]\n",
    "            document = unicodedata.normalize('NFC',document)\n",
    "\n",
    "            rel_line = each_evid[1]\n",
    "\n",
    "            #Creating a dictionary with all\n",
    "            if document in evid_dict:\n",
    "                evid_dict[document].append(rel_line)\n",
    "                evid_dict[document] = list(set(evid_dict[document]))\n",
    "\n",
    "            else:\n",
    "                evid_dict[document]  = [rel_line]\n",
    "\n",
    "        try:\n",
    "            total_sentences = number_of_lines[document]\n",
    "\n",
    "            sample_list = list(range(0,total_sentences))\n",
    "            sample_list = [number for number in sample_list if not number in evid_dict[document]]\n",
    "\n",
    "        except:\n",
    "            sample_list = 'doc not found'\n",
    "\n",
    "        neg_sample_list.append((document, sample_list))\n",
    "\n",
    "    actual_lines_evidence = []\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "        sub_list =[]\n",
    "\n",
    "        for each in evidence:\n",
    "            each = (unicodedata.normalize('NFC',each[0]), each[1])\n",
    "\n",
    "            actual_line = veri_lines_dict[each]\n",
    "            actual_line = actual_line[:-1]\n",
    "            sub_list.append(actual_line)\n",
    "\n",
    "        actual_lines_evidence.append(sub_list)\n",
    "\n",
    "    #Creating equivalent negative sample for every claim\n",
    "    neg_lines_evidence = []\n",
    "\n",
    "    for idx, evidence in enumerate(clean_evidence):\n",
    "        neg_sub_list =[]\n",
    "\n",
    "        for each in evidence:\n",
    "\n",
    "            neg_sample_numbers = neg_sample_list[idx][1]\n",
    "            neg_line_number = random.choice(neg_sample_numbers)\n",
    "\n",
    "            each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "            neg_line = veri_lines_dict[each]\n",
    "\n",
    "            #while loop to avoid empty list\n",
    "            while len(neg_line) == 0:\n",
    "                neg_line_number = random.choice(neg_sample_numbers)\n",
    "                each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "                neg_line = veri_lines_dict[each]\n",
    "\n",
    "            neg_line = neg_line[:-1]\n",
    "            neg_sub_list.append(neg_line)\n",
    "\n",
    "        neg_lines_evidence.append(neg_sub_list)\n",
    "\n",
    "    #tokenise the claims\n",
    "\n",
    "    one_line_tokenised = []\n",
    "\n",
    "    for claim in one_line_claims:\n",
    "        no_stop = claim[:-1]\n",
    "        lower_claim = no_stop.lower()\n",
    "        tokenised = lower_claim.split(' ')\n",
    "        one_line_tokenised.append(tokenised)\n",
    "\n",
    "    #Create verifiable tuples\n",
    "    pos_claim_evidence = []\n",
    "    neg_claim_evidence = []\n",
    "\n",
    "    for idx,evidence in enumerate(actual_lines_evidence):\n",
    "        for each in evidence:\n",
    "            veri_tuples = (one_line_tokenised[idx], each)\n",
    "            pos_claim_evidence.append(veri_tuples)\n",
    "\n",
    "    for idx, evidence in enumerate(neg_lines_evidence):\n",
    "        for each in evidence:\n",
    "            neg_tuples = (one_line_tokenised[idx], each)\n",
    "            neg_claim_evidence.append(neg_tuples)\n",
    "            \n",
    "    return(pos_claim_evidence, neg_claim_evidence)\n",
    "\n",
    "pos_claim_5, neg_claim_5 = pair_maker('data_files/shared_task_dev.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22556/22556 [00:01<00:00, 13822.91it/s]\n",
      "100%|██████████| 22556/22556 [00:00<00:00, 27929.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#Making representation arrays\n",
    "verif_arrays_5 = []\n",
    "\n",
    "for pair in tqdm(pos_claim_5):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    evidence = pair[1]\n",
    "    evidence_array = np.zeros(50)\n",
    "    for word in evidence:\n",
    "        try:\n",
    "            evidence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, evidence_array])\n",
    "    \n",
    "    verif_arrays_5.append(array_row)\n",
    "    \n",
    "#Making representation arrays\n",
    "neg_arrays_5 = []\n",
    "\n",
    "for pair in tqdm(neg_claim_5):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    evidence = pair[1]\n",
    "    evidence_array = np.zeros(50)\n",
    "    for word in evidence:\n",
    "        try:\n",
    "            evidence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, evidence_array])\n",
    "    \n",
    "    neg_arrays_5.append(array_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_matrix_5 = np.array(verif_arrays_5)\n",
    "neg_matrix_5 = np.array(neg_arrays_5)\n",
    "rel_y5 = np.ones(len(verif_arrays_5))\n",
    "neg_y5 = np.zeros(len(neg_arrays_5))\n",
    "feature_matrix_5 = np.concatenate((veri_matrix_5,neg_matrix_5), axis=0)\n",
    "target_matrix_5 = np.concatenate((rel_y5,neg_y5), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculting relevant metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_func(feature_matrix, target_matrix, weights):\n",
    "    \n",
    "    one_column = np.ones((feature_matrix.shape[0],1))\n",
    "    feature_matrix = np.concatenate([one_column, feature_matrix], axis = 1)\n",
    "    \n",
    "    lin_pred = feature_matrix@weights\n",
    "    class_pred = 1/ (1 + np.exp(-lin_pred))\n",
    "    class_pred = np.around(class_pred)\n",
    "\n",
    "    score = 0\n",
    "        \n",
    "    for idx in range(0,len(class_pred)):\n",
    "        val = target_matrix[idx]\n",
    "        pred = class_pred[idx]\n",
    "\n",
    "        if val == pred:\n",
    "            score += 1\n",
    "\n",
    "    accuracy = score/len(class_pred)\n",
    "    \n",
    "    return(class_pred, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, acc = accuracy_func(feature_matrix_5, target_matrix_5, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "all_data = len(preds)\n",
    "\n",
    "numer = 0\n",
    "denom = 0\n",
    "for i in range(0, all_data):\n",
    "    if preds[i] == 1:    #Checks if retrieved\n",
    "        denom += 1\n",
    "        \n",
    "    if (preds[i] == 1) and (target_matrix_5[i] == 1):  #Actual correct claims retrieved\n",
    "        numer += 1\n",
    "        \n",
    "precision = numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating recall\n",
    "\n",
    "numer = 0\n",
    "denom = 0\n",
    "for i in range(0, all_data):\n",
    "    if target_matrix_5[i] == 1:    #Total relevant\n",
    "        denom += 1\n",
    "        \n",
    "    if (preds[i] == 1) and (target_matrix_5[i] == 1):  #Correctly retrieved\n",
    "        numer += 1\n",
    "    \n",
    "recall = numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 Score\n",
    "F1 = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature and target matrices then actual model for 6 and 8 are built on separate files labelled Question 6 and Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper6(filename):\n",
    "    \n",
    "    veri_claims = []\n",
    "    veri_evid = []\n",
    "    veri_label = []\n",
    "\n",
    "\n",
    "    with open(filename) as openfile:\n",
    "                for iline, line in enumerate(openfile.readlines()):\n",
    "\n",
    "                    claim_dic = json.loads(line)\n",
    "                    veri_check = claim_dic['verifiable']\n",
    "\n",
    "                    if veri_check == 'VERIFIABLE':\n",
    "                        claim = claim_dic['claim']\n",
    "                        evidence = claim_dic['evidence']\n",
    "                        label = binary_claim_dic[claim_dic['label']]\n",
    "\n",
    "                        veri_claims.append(claim)\n",
    "                        veri_evid.append(evidence)\n",
    "                        veri_label.append(label)\n",
    "\n",
    "    one_line_claims = []\n",
    "    one_line_evidence = []\n",
    "    one_line_labels = []\n",
    "\n",
    "    for idx, evid in enumerate(veri_evid):\n",
    "        if len(evid[0]) == 1:\n",
    "            one_line_evidence.append(evid)\n",
    "            one_line_claims.append(veri_claims[idx])\n",
    "            one_line_labels.append(veri_label[idx])\n",
    "\n",
    "    clean_evidence = []\n",
    "\n",
    "    for evidence in one_line_evidence:\n",
    "\n",
    "        one_claim_list = []\n",
    "        for each in evidence:\n",
    "            one_evid = each[0]\n",
    "\n",
    "            document = one_evid[2]\n",
    "            line = one_evid[3]\n",
    "\n",
    "            one_claim_list.append((document, line))\n",
    "\n",
    "        clean_evidence.append(one_claim_list)\n",
    "\n",
    "    all_required_dict = {}\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "\n",
    "        key_val = unicodedata.normalize('NFC', evidence[0][0])\n",
    "\n",
    "        for each in evidence:\n",
    "            if key_val in all_required_dict:\n",
    "                all_required_dict[key_val].append(each[1])\n",
    "                all_required_dict[key_val] = list(set(all_required_dict[key_val]))\n",
    "\n",
    "            else:\n",
    "                all_required_dict[key_val] = [each[1]] \n",
    "\n",
    "\n",
    "    #Retrieving just the article titles\n",
    "    all_required_articles = list(all_required_dict.keys())\n",
    "\n",
    "    veri_lines_dict = {}\n",
    "\n",
    "    bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "    for file in tqdm(list_of_wiki):\n",
    "            with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                    for iline,line in enumerate(openfile.readlines()):\n",
    "                        iD = json.loads(line)['id']\n",
    "\n",
    "                        iD = unicodedata.normalize('NFC',iD)\n",
    "\n",
    "                        if iD in all_required_dict:\n",
    "\n",
    "                            doc_lines = json.loads(line)['lines']\n",
    "\n",
    "                            doc_line_list = doc_lines.split('\\n')\n",
    "\n",
    "                            number_of_lines[iD] = len(doc_line_list)\n",
    "\n",
    "                            for line_number in range(0, len(doc_line_list)):\n",
    "                                doc_line = doc_line_list[line_number].lower()\n",
    "\n",
    "                                #Isolating cleaned up version of line\n",
    "                                doc_line = doc_line.split('\\t')[1]\n",
    "                                doc_line = doc_line.split('.')[0]\n",
    "\n",
    "                                doc_line = doc_line.split(' ')\n",
    "\n",
    "                                #Sorting out the brackets\n",
    "                                doc_line_brac = []\n",
    "                                for word in doc_line:\n",
    "\n",
    "                                    if word in set(list(bracket_dict.keys())):\n",
    "                                        word = bracket_dict[word]\n",
    "\n",
    "                                    doc_line_brac.append(word)\n",
    "\n",
    "                                veri_lines_dict[(iD, line_number)] = doc_line_brac\n",
    "\n",
    "    actual_lines_evidence = []\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "        sub_list =[]\n",
    "\n",
    "        for each in evidence:\n",
    "            each = (unicodedata.normalize('NFC',each[0]), each[1])\n",
    "\n",
    "            actual_line = veri_lines_dict[each]\n",
    "            actual_line = actual_line[:-1]\n",
    "            sub_list.append(actual_line)\n",
    "\n",
    "        actual_lines_evidence.append(sub_list)\n",
    "\n",
    "    one_line_tokenised = []\n",
    "\n",
    "    #Claims have a lot of 's so need modification to claim\n",
    "    for claim in one_line_claims:\n",
    "        no_stop = claim[:-1]\n",
    "        lower_claim = no_stop.lower()\n",
    "        tokenised = lower_claim.split(' ')\n",
    "\n",
    "        one_line_tokenised.append(tokenised)\n",
    "\n",
    "    clean_claims = []\n",
    "    for claim in one_line_claims:   \n",
    "        claim = claim.lower()\n",
    "        no_stop = claim[:-1]\n",
    "        claim = claim.split(' ')\n",
    "\n",
    "        no_apos_claim = []\n",
    "        #Removing apostrophe for vector representation later\n",
    "        for word in claim:\n",
    "            if \"'\" in word:\n",
    "                no_apos_claim.append(word[:-2])\n",
    "                no_apos_claim.append(\"'s\")\n",
    "\n",
    "            else:\n",
    "                no_apos_claim.append(word)\n",
    "\n",
    "        clean_claims.append(no_apos_claim)\n",
    "\n",
    "    clean_claims = []\n",
    "    for claim in one_line_claims:   \n",
    "        claim = claim.lower()\n",
    "        no_stop = claim[:-1]\n",
    "        claim = claim.split(' ')\n",
    "\n",
    "        no_apos_claim = []\n",
    "        #Removing apostrophe for vector representation later\n",
    "        for word in claim:\n",
    "            if \"'\" in word:\n",
    "                no_apos_claim.append(word[:-2])\n",
    "                no_apos_claim.append(\"'s\")\n",
    "\n",
    "            else:\n",
    "                no_apos_claim.append(word)\n",
    "\n",
    "        clean_claims.append(no_apos_claim)\n",
    "\n",
    "    pair_claims = []\n",
    "    pair_evid = []\n",
    "    pair_label = []\n",
    "\n",
    "    for idx, sentences in enumerate(actual_lines_evidence):\n",
    "        for sentence in sentences:\n",
    "            pair_evid.append(sentence)\n",
    "            pair_claims.append(clean_claims[idx])\n",
    "            pair_label.append(one_line_labels[idx])\n",
    "            \n",
    "    return(pair_claims, pair_evid, pair_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:55<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "claims6, evid6, label6 = mapper6('data_files/train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(claims6, open(\"pkl_files/claims6.p\", \"wb\"))\n",
    "pickle.dump(evid6, open(\"pkl_files/evid6.p\", \"wb\"))\n",
    "pickle.dump(label6, open(\"pkl_files/label6.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:52<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "claimstest, evidtest, labeltest = mapper6('data_files/shared_task_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(claimstest, open(\"pkl_files/claimstest.p\", \"wb\"))\n",
    "pickle.dump(evidtest, open(\"pkl_files/evidtest.p\", \"wb\"))\n",
    "pickle.dump(labeltest, open(\"pkl_files/labeltest.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See files Question 6 and 8 for continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Code to make json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_extractor(filename):\n",
    "    \n",
    "    veri_id = []\n",
    "    veri_evid = []\n",
    "\n",
    "\n",
    "    with open(filename) as openfile:\n",
    "                for iline, line in enumerate(openfile.readlines()):\n",
    "\n",
    "                    claim_dic = json.loads(line)\n",
    "                    veri_check = claim_dic['verifiable']\n",
    "\n",
    "                    if veri_check == 'VERIFIABLE':\n",
    "                        veri_id.append(claim_dic['id'])\n",
    "                        veri_evid.append(claim_dic['evidence'])\n",
    "                        \n",
    "\n",
    "    \n",
    "    one_line_evidence = []\n",
    "    one_line_id = []\n",
    "\n",
    "    for idx, evid in enumerate(veri_evid):\n",
    "        if len(evid[0]) == 1:\n",
    "            one_line_evidence.append(evid)\n",
    "            one_line_id.append(veri_id[idx])\n",
    "            \n",
    "    clean_evidence = []\n",
    "\n",
    "    for evidence in one_line_evidence:\n",
    "\n",
    "        one_claim_list = []\n",
    "        for each in evidence:\n",
    "            one_evid = each[0]\n",
    "\n",
    "            document = one_evid[2]\n",
    "            line = one_evid[3]\n",
    "\n",
    "            one_claim_list.append((document, line))\n",
    "\n",
    "        clean_evidence.append(one_claim_list)\n",
    "        \n",
    "    pair_evidence = []\n",
    "    pair_iD = []\n",
    "\n",
    "    for idx, evidences in enumerate(clean_evidence):\n",
    "        for evid in evidences:\n",
    "            pair_evidence.append(evid)\n",
    "            pair_iD.append(one_line_id[idx])\n",
    "            \n",
    "    return(pair_evidence, pair_iD)\n",
    "                        \n",
    "json_evidence, json_iD= json_extractor('data_files/shared_task_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pickle.load( open( \"pkl_files/final_predictions.p\", \"rb\" ) )\n",
    "bo_final_pred = ['SUPPORTS' if x == True else \"REFUTES\" for x in final_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dics = []\n",
    "\n",
    "for idx,ID in enumerate(json_iD):\n",
    "    temp_dic = {}\n",
    "    temp_dic[\"id\"] = json_iD[idx]\n",
    "    temp_dic[\"predicted_label\"] = bo_final_pred[idx]\n",
    "    temp_dic[\"evidence\"] = json_evidence[idx]\n",
    "    \n",
    "    list_of_dics.append(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dics = []\n",
    "\n",
    "for i in range(0, len(list_of_dics)):\n",
    "    if list_of_dics[i] not in list_of_dics[i+1:]:\n",
    "        unique_dics.append(list_of_dics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"predictions.jsonl\"\n",
    "\n",
    "json_objects = []\n",
    "\n",
    "for dic in unique_dics:\n",
    "    json_objects.append(json.dumps(dic))\n",
    "line = \"\\n\".join(json_objects)\n",
    "\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
