{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import nltk\n",
    "import tqdm\n",
    "import itertools\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import os.path\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_wiki = os.listdir(\"data_files/wiki-pages/wiki-pages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [06:13<00:00,  3.38s/it]\n",
      "100%|██████████| 109/109 [06:13<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "#Counting frequency of every term\n",
    "word_count = {}\n",
    "\n",
    "brackets = ['lrd', 'rrb', 'lsb', 'rsb']\n",
    "\n",
    "for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "    with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "            for iline, line in enumerate(openfile.readlines()):\n",
    "                text = json.loads(line)['text']\n",
    "                text = text.lower()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "\n",
    "                for token in tokens:\n",
    "                    if token not in brackets:\n",
    "                        if token in word_count:\n",
    "                            word_count[token] += 1\n",
    "\n",
    "                        else:\n",
    "                            word_count[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_count, open(\"pkl_files/word_count.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pickle.load( open( \"pkl_files/word_count.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457271337"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell to calculate total number of words\n",
    "word_ranks = list(word_count.items())\n",
    "word_ranks.sort(key=lambda tup: tup[1], reverse = True) \n",
    "word_ranks\n",
    "\n",
    "total = sum(word_count.values())\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6.728126937026888),\n",
       " ('of', 3.5237113495263754),\n",
       " ('in', 3.1868513989102274),\n",
       " ('and', 2.7821435481752053),\n",
       " ('a', 2.385506179233797)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell to calculate proportion of most frequent words\n",
    "\n",
    "prop_list = []\n",
    "\n",
    "for word, count in word_ranks[:5]:\n",
    "    prop = (count/total) * 100\n",
    "    prop_list.append((word, prop))\n",
    "    \n",
    "prop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5xU1fnH8c+znbLUpbeVIkgTcEGKoKgoGgW7WLEFMZYYk1hSrL9EY9dgVFSwBrsGG4qVpsCCgFTpsLSl97bw/P6YIVmWWVhhZu/s7vf9es1rZ+69M/dLfeaec+455u6IiIgUlBB0ABERiU8qECIiEpEKhIiIRKQCISIiEalAiIhIRElBB4imjIwMz8zMDDqGiEiJMWnSpDXuXiPSvlJVIDIzM8nOzg46hohIiWFmiwvbpyYmERGJSAVCREQiilkTk5kNAc4Cct29dXjbW0Dz8CFVgA3u3i7CexcBm4E9QJ67Z8Uqp4iIRBbLPoiXgUHAq/s2uPvF+56b2WPAxoO8v6e7r4lZOhEROaiYFQh3H2VmmZH2mZkBFwEnx+r8IiJyZILqg+gOrHL3uYXsd+ALM5tkZgMO9kFmNsDMss0se/Xq1VEPKiJSVgVVIC4Bhh1kfzd37wCcAdxoZj0KO9DdB7t7lrtn1agRcSiviIgchmIvEGaWBJwHvFXYMe6+PPwzF/gA6BTLTE9/NZeJi9bF8hQiIiVOEFcQpwKz3T0n0k4zq2Bm6fueA6cB02MVZtOO3bwxfjEXPvc917w8kZnLN8XqVCIiJUrMCoSZDQO+B5qbWY6ZXRve1Y8CzUtmVtfMPg2/rAWMMbOpwATgE3cfEaucldKS+fYPPbmjdwuyF63jzKdHc8uwH1m0ZmusTikiUiJYaVpRLisry49kqo2N23czeNR8hoxZxO49e7m4YwNuOaUZtSqlRTGliEj8MLNJhd1rpgIRQe7mHQz6eh7DJiwhMcHo3zWTG05sQpXyKVFIKSISP1QgDtOStdt44suf+XDKMiqmJjHwxCZc3S2T8imlao5DESnDVCCO0OyVm3j085/5ctYqMiqmcsspTenXsSEpSZrKSkRKNhWIKJm0eD0Pj5jN+IXraFCtHL879WjOaltXhUJESiwViChyd0bNXcPDI2YzY/kmEgzqVS1HZvUKNKpePvyzApnVy9OgWnnSkhNjmkdE5EgcrECoMf0XMjNOPLoG3Ztm8PXsXKbmbGDR2m0sWbuV4VOWs2lHXr5joXFGBU5omkG3phl0aVKd9LTkANOLiBSdriCibMO2XSxau43Fa7eycM1WpizdwPgF69i+ew+JCUa7BlXo1jSD3q1q07JupUCzioioiSlgO/P28OOSDYyZu4Yx89YwLWcDex3a1q/MxR0b0OfYurqyEJFAqEDEmfVbd/GfKct4c+JSZq/cTLnkRPq2q8tfz2pJhVS1+olI8VEfRJypWiGFq7odRf+umUzL2cibE5fw1sSlLFyzlaFXd9R9FiISFzQ+M0BmxrENqvDgeW154uJ2TFy0jmtfzmb7rj1BRxMRUYGIF33b1ePxi9oxfuFarnt1Ijt2q0iISLBUIOLIOe3r8eiFxzJu/loufeEH5qzcHHQkESnDVCDizHkd6vPPS9qzYM1Wznx6NPd/NJNNO3YHHUtEyiD1hsahs9rWpVuTDB75Yg5Dxy3kwynL6N8lkyu7NKJqBc0oKyLFQ8Nc49y0nA089eVcvpqdS7nkRI6pk05igtGoegUePr8tCQkWdEQRKcEONsxVTUxxrm39Krx0VUc+v7UHfdvVpVxKItt37+HdSTlaR1tEYkpNTCVE89rpPHR+WwC27coj6/++5IMfl3F84+oBJxOR0kpXECVQ+ZQkzmhdh0+mrdBwWBGJGRWIEur8DvXYvDOPkTNXBR1FREqpmBUIMxtiZrlmNj3ftnvNbJmZTQk/zizkvb3NbI6ZzTOzO2OVsSTr3Lg6dSun8f7knKCjiEgpFcs+iJeBQcCrBbY/4e6PFvYmM0sEngF6ATnARDMb7u4zYxW0JEpIMM5pX49nv5vPmU+NpmalVNKSEmlSswL9u2ZSMz0t6IgiUsLFrEC4+ygzyzyMt3YC5rn7AgAzexPoC6hAFHBVt0y27sxj6frtrNmykx279/DFzJW8MHohd53Rgqu6ZmKmYbAicniCGMV0k5ldCWQDv3f39QX21wOW5nudAxxf2IeZ2QBgAEDDhg2jHDW+1UxP476+rffbtmjNVh74eCb3fTSTn5ZtpF/HhnTMrKpCISK/WHF3Uj8LNAHaASuAxyIcE+l/skLv5nP3we6e5e5ZNWrUiE7KEiwzowIvXJnFwBOb8NHU5Vz0/Pdc90o2a7bsDDqaiJQwxVog3H2Vu+9x973AC4SakwrKARrke10fWF4c+UqLhATjzjNaMPmvvfjLr45h9Nw19Hj4G/7v45lMXbqBjds1t5OIHFqxNjGZWR13XxF+eS4wPcJhE4FmZnYUsAzoB1xaTBFLlfS0ZK7r3piTmtfk6a/m8vK4Rbw4ZiFJCcYtpzTj5pObqulJRAoVswJhZsOAk4AMM8sB7gFOMrN2hJqMFgHXh4+tC7zo7me6e56Z3QR8DiQCQ9x9RqxylgVNa1bk6Uvac/fZLZmwcB2f/LSCx0f+TMfManRpojuxRSQyTdZXBm3dmUf7B0Zy2fENuefsVkHHEZEAabI+2U+F1CROaJrByJmrKE1fEEQkulQgyqjerWqTs347707SndgiEpkKRBl1bod6dG1SnT998BNL120LOo6IxCEViDIqOTGBxy9qB8BLYxYGnEZE4pHWgyjDaldOo2+7erz2w2IAbjmlGdW0pKmIhKlAlHF/PaslKUkJvPr9It6bnMNpLWvTq2UtereuHXQ0EQmYmpjKuMrlkvn7uW34/NYenHh0Db6dk8vA1yfxzezcoKOJSMBUIASAZrXSGXRpB0bf0ZMG1cpx9csTeWXcoqBjiUiAVCBkP+VTkvjgN904qXkN7hk+g5uH/ciyDduDjiUiAVCBkANkVEzlucuP45ZTmvHFjJWc/Oi3PP3VXN1UJ1LGqEBIRGnJidzW62i+/sNJnHJMTR4f+TO3vT2Vmcs3BR1NRIqJCoQcVL0q5Rh0SQfO61CPz6av4KqhExgzdw078/YEHU1EYkyT9UmRjV+wlstfGs/uPU5KYgKntqzJA31bU71iatDRROQwHWyyPhUI+UVWb97Jj0vW88OCdbw+PnSDXf8ujejdug5t6lUmJUkXpSIliQqExMTslZt49tv5DJ+6HHeokZ7KsfWrkJVZlV+1qUP9quW0IJFInFOBkJjK3byD0T+vYcSMlSxcs5V5uVsAuCirPg+c05rUpMSAE4pIYVQgpFjNXbWZx0f+zGfTV5JRMZWzj63Ddd0bU69KuaCjiUgBKhBS7Nydb+es5o3xS/ju51ySExO4/fTmnNmmDjUrpQUdT0TCVCAkUDnrt3HzsB/5cckGALo3y+C67o3p2qQ6yYnq1BYJ0sEKRMxmczWzIcBZQK67tw5vewQ4G9gFzAeudvcNEd67CNgM7AHyCgsvJUP9quV5b2BXpi/fyNvZS3lj/BJGz11D9QopXHBcffq0q8sxtSuRkKAObZF4ErMrCDPrAWwBXs1XIE4Dvnb3PDP7B4C73xHhvYuALHdf80vOqSuIkmHd1l18P38tb04MFQoIzSrbq2UtWtetRM8WNWlUvULAKUXKhsCamMwsE/h4X4EosO9c4AJ3vyzCvkWoQJQJqzbt4JvZuYydv5bv5uSyaUceAKceU5PrT2xCVqOqGiorEkOBNDEVwTXAW4Xsc+ALM3PgeXcfXHyxpDjVqpRGv04N6depIQBL123jnUk5vDR6AV/OyqVu5TTO7VCPyzs3ok5ljYISKU6BXEGY2Z+BLOA8jxDAzOq6+3IzqwmMBG5291GFnGMAMACgYcOGxy1evDi6vwgJxNadeYyYvpKPpy3nmzmrMYMrOjfi96c1p3K55KDjiZQacdXEZGb9gYHAKe6+rQifcS+wxd0fPdSxamIqneau2syQsYsYNmEJlcslc3HHBlzRuRENqpUPOppIiXewAlGsYwzNrDdwB9CnsOJgZhXMLH3fc+A0YHrxpZR406xWOg+e14aPbz6BLo2rM2TMQk557Dv+MWI281dvCTqeSKkVy1FMw4CTgAxgFXAPcBeQCqwNH/aDuw80s7rAi+5+ppk1Bj4I708C/u3ufyvKOXUFUTas2Lidf3w2mw+nLAegY2ZVbj31aLo1zQg4mUjJoxvlpFRasXE7w6csZ+jYRazctIOWdSpxUVZ9+rarR9UKKUHHEykRVCCkVNuxew+vfb+Y/0xdxvRlm0hONHq3rsMtJzelWa30oOOJxDUVCCkzZi7fxBvjFzNswhL2OpzTri4Pnd+WtGTNKCsSiQqElDmrN+/kue/m89KYhdSvWo6rumZy6fENKZ8S5K0/IvHniAtE+H6EbkBdYDuhUUXZ7r43mkGPlAqEFDR67mqe/HIukxavJ6NiChdmNeDaE44iQ8ukigBHUCDMrCdwJ1AN+BHIBdKAo4EmwLvAY+6+KdqhD4cKhBRm7Lw1DBmzkK/n5JKalEC/jg25/sTGujtbyrwjKRCPAP909yUR9iURmq010d3fi1bYI6ECIYcyf/UWnv12Ph/8uIwEg7Pb1qVfp4ZkNaqq2WSlTIpGE1Oiu++JerIoU4GQolq6bhuDRy3g/ck5bN21hzb1KnPXmS3o2kT3UkjZEo0CsZBQc9JQd58Z5XxRowIhv9TWnXl8PG05T381j2UbtnNJpwZc170xTWpUDDqaSLGIRoFIB/oBVxOanmMI8Ga89D3sowIhh2vH7j088vkcXv1+EXl7nXPb1+PWU46mYXXN9ySlW1SHuYYXAhoGVCF0VfGAu8874pRRoAIhRyp30w6e+24Bb4xfzF53ftWmDn868xitoy2l1hFP1mdmiWbWx8w+AJ4CHgMaAx8Bn0YtqUjAalZK4+6zWzLq9p5cdnwjPp+xit5PjebbOblBRxMpdkWdzXUu0Bd4xN3bu/vj7r7K3d8FRsQunkgwalVK494+rRh+UzdqVEzl6pcncu/wGazZsjPoaCLFpqh9EBXdPe7nVVYTk8TCtl15/P3TWQybsJTUpASu696Y35zURNN3SKkQjfUgnjGzKvk+sKqZDYlKOpE4Vz4lif87pw1f/K4HPZvX5Omv5nLaE6P47ufVlKapakQKKmqBaOvuG/a9cPf1QPvYRBKJT01qVOSZyzrw7+uOxwz6D5nA9a9NYtGarUFHE4mJohaIBDOruu+FmVUjtJiPSJnTtWkGn9/ag9t7N+fbn1dz2pOjePyLOWzdmRd0NJGoKmqBeAwYZ2YPmNkDwDjg4djFEolvacmJ/Oakpoz6Y09Ob1Wbp7+ex8mPfcu7k3LU7CSlRpHvgzCzVkBPwICv4vGOanVSS1AmLV7HAx/PYsrSDZzVtg4Pnd+Wiqm6yJb4F41OaoDZwPvAf4AtZtYwGuFESoPjGlXj/Ru68sfTm/PZ9JWc88xYRqkTW0q4ot4odzOwChgJfAx8Ev4pImEJCcaNPZvy8tUd2b5rD1cOmcBlL45n/uq4HyEuElFRryB+CzR391bu3tbd27h720O9ycyGmFmumU3Pt62amY00s7nhn1ULeW//8DFzzax/EXOKBK57sxp8/YcTua9PK35atpHeT47i/o9mslY32UkJU9QCsRTYeBif/zLQu8C2Own1YTQDvgq/3k94lNQ9wPFAJ+CewgqJSDxKTUqkf9dMvv79SZzbvh4vj1vIKY9/xzvZS9XsJCVGUQvEAuBbM7vLzG7b9zjUm9x9FLCuwOa+wCvh568A50R46+nASHdfF77nYiQHFhqRuFcjPZWHLziWz2/tQdMaFfnju9O47MXxLNS9E1ICFLVALCH0n3QKkJ7vcThqufsKgPDPmhGOqUfoqmWfnPC2A5jZADPLNrPs1atXH2YkkdhqViudt6/vwt/Obc1PORs5/clRPPPNPHbviatl3UX2U6RxeO5+H4CZVXD34vjqE2ntx4jX5e4+GBgMoWGusQwlciQSEozLjm/EqcfU4r6PZvDI53MYPmU5D57fhg4N1YIq8aeoo5i6mNlMYFb49bFm9q/DPOcqM6sT/pw6QKR5lHOABvle1weWH+b5ROJKrUpp/Ouy43jhyiw27djN+c+O457/TGfzjt1BRxPZT1GbmJ4k1C+wFsDdpwI9DvOcw4F9o5L6E7qvoqDPgdPCkwJWBU4LbxMpNXq1rMXI206kf5dMXv1hMb2fHM3kJeuDjiXyX0W+Uc7dlxbYtOdQ7zGzYcD3QHMzyzGza4GHgF5mNhfoFX6NmWWZ2Yvhc60DHgAmhh/3h7eJlCoVU5O4t08r3ruhKwAXPDuOv386i+27DvnPSyTmiroexLvA48AgoDNwC5Dl7v1iG++X0VQbUpJt2rGbBz+dzbAJS2hUvTxP9WtPuwZVDv1GkSMQjak2BgI3EhpJlAO0C78WkSiplJbMg+e1YdivO5O3x7no+e95e2LBC3eR4lPkyfpKAl1BSGmxbusubh42mbHz1nJeh3o80Lc1FTT5n8TAwa4givQ3zsyGEmGYqbtfc4TZRCSCahVSeOXqTvzz63n88+u5/LhkA/+8pD2t61UOOpqUIUVtYto3Qd8nhKbHqARoBjKRGEpKTOB3vY5m2K87s2P3Hs7911heHL2AvXtLz1W/xLfDamIyswTgS3c/OfqRDp+amKS0Wr91F3e8N40vZq7ipOY1ePTCY8momBp0LCkForUeRH7NAK0HIVJMqlZI4fkrjuOBvq0YN38tZzw1mh8WrA06lpRyRb2TerOZbdr3E/gIuCO20UQkPzPjii6ZDL+pG+lpSVz+4nhe/2Fx0LGkFCtSgXD3dHevlO/n0e7+XqzDiciBWtSuxIc3dqN7swz+8uF0/vTBT5r0T2KiqKOYOhxsv7tPjk4cESmKSmnJvNi/I498PofnvpvPojVbeeLidtSqlBZ0NClFijqw+l9AB2AaoZlW2wLjgd2Ehr/GVWe1SFmQmGDceUYLmtasyF8+/IlfPT2a56/I4rhGmhlWoqOondSLgOPcPcvdjwPaA/PcvWe8jWQSKWsuOK4+H998AuVTkug3+Hte+2GxVq2TqChqgWjh7j/te+Hu0wlNtyEicaBpzXQ+uukETmiawV/D/RK78tQvIUemqAVilpm9aGYnmdmJZvYC4bUhRCQ+VC6fzEv9O/Kbk5owbMJSLn3hB1Zv3hl0LCnBilogrgZmAL8FbgVmhreJSBxJSDBu792Cf17SnunLN9Jn0Bh+ytkYdCwpoYo6zHUH8Bxwp7uf6+5PhLeJSBw6+9i6vHdDVxLMuOC5cYyYviLoSFICFfVGuT7AFGBE+HU7Mxsey2AicmRa1a3Mhzd2o2XdStzwxmSe+26+Oq/lFylqE9M9QCdgA4C7TwEyY5RJRKKkRnoq/76uM2e2rsNDn83mljensGO3VquToilqgchzdzVkipRA5VISGXRpe/54enM+mrqcC54bR876bUHHkhKgqAViupldCiSaWTMz+ycwLoa5RCSKzIwbezblhSuzWLx2G+c8M5YpSzcEHUviXFELxM1AK2An8G9gI6HRTCJSgvRqWYsPftONcimJ9Bv8PSOmrww6ksSxQxYIM0sE7nP3P7t7x/DjL4c7isnMmpvZlHyPTWZ2a4FjTjKzjfmOuftwziUiB2pasyLv39CNFrUrMfD1STzzzTx1XktEh5yLyd33mNlx0Tqhu88hfBd2uPgsAz6IcOhodz8rWucVkf+pkZ7KmwM6c8d703jk8zlMX7aRRy88Vutey36K+rfhx/Cw1neArfs2uvv7R3j+U4D57q5J7UWKWVpyIk9e3I7WdSvz4GezmJu7hZf6Z9GoeoWgo0mcKGofRDVgLaFZW88OP6Lx7b4fMKyQfV3MbKqZfWZmrQr7ADMbYGbZZpa9evXqKEQSKTvMjF/3aMzr1x7Pmi07OeeZsYybvyboWBInDmtN6qic2CwFWA60cvdVBfZVAva6+xYzOxN4yt2bHeoztSa1yOFbuGYr170ykUVrt/FA39ZcerxWFS4LDntNajP7It/zu6Kc6wxgcsHiAODum9x9S/j5p0CymWVE+fwiks9RGRX48MZunNA0gz998BMPj5itzusy7lBNTDXyPb8wyue+hEKal8ystplZ+HknQjm1QrtIjKWnJfNS/ywu6dSAf307n1venMLOPN15XVYdqpM6Jl8fzKw80Au4Pt+2gQDu/hxwAXCDmeUB24F+rq8yIsUiKTGBv5/bhgbVyvPwiDms37qL5644jooa4VTmHLQPwsw2AKMILTPaPfz8v9y9T0zT/ULqgxCJrneyl3Ln+z9xdK10Xrm6IzW15nWpc7A+iEN9Jeib7/mj0YskIiXBhVkNqJGeym/emMy5/xrH0Ks7cnSt9KBjSTEJbBRTLOgKQiQ2fsrZyDWvTGT7rj08e3kHujerceg3SYlwJKOYPjKzs80sOcK+xmZ2v5ldE62gIhKf2tSvzH9u7Eb9quW4euhE3pywJOhIUgwONYrp14T6Hmab2UQz+9TMvjazBcDzwCR3HxLzlCISuLpVyvH2wC50aVKdO9//iXuHz2DP3tLTAiEHKnITk5llAnUIjSr62d3jbkJ5NTGJxN6evc7fP53FS2MWcuoxtXj6knaUT9EIp5LqsJuY8nP3Re7+vbtPicfiICLFIzHB+OtZLbn37JZ8PXsV/Qb/wOrNO4OOJTFQ1DWpN4en5c7/WGpmH5hZ41iHFJH4c1W3o3j+iizmrtrCec+O5edVm4OOJFFW1CuIx4E/AvWA+sAfgBeANwH1QYiUUb1a1mLYgM7s2L2X8/81jm/n5AYdSaKoqAWit7s/7+6bw/MkDQbOdPe3gKoxzCcica5dgyp8eGM36lcrz7WvZPP2xKVBR5IoKWqB2GtmF5lZQvhxUb59GsYgUsbVq1KOdwZ2oWuT6tz+3jQe/XwOezXCqcQraoG4DLgCyA0/rgAuN7NywE0xyiYiJUjF1CSGXNWRi7MaMOibefz2rSns2K2J/kqyIo1Nc/cFhBYJimRM9OKISEmWnJjAQ+e3ITOjAv8YMZs1m3fy/JXHUSntgHttpQQo6iim+uERS7lmtsrM3jOz+rEOJyIlj5lxw0lNeOLiY5m4aB39nv+BlRt3BB1LDkNRm5iGAsOBuoRGMn0U3iYiEtG57evzYv8sFq/dyrn/0jDYkqioBaKGuw9197zw42X2X0xIROQAJzWvydsDu7B7z14ueHYc4xdo3a+SpKgFYo2ZXW5mieHH5WiFNxEpglZ1K/PBb7qRkZ7KFUMmMGL6iqAjSREVtUBcA1wErARWEFrx7epYhRKR0qVBtfK8N7ArLWqnc8Mbk3lpzMKgI0kRFKlAuPsSd+/j7jXcvaa7nwOcF+NsIlKKVK2QwpsDOtOzeU0e+Hgm9300Q/dKxLkiT9YXwW1RSyEiZUL5lCReuDKLq7pmMnTsIm5580d25uleiXh1JHP02pGc2MwWAZuBPUBewelmzcyAp4AzgW3AVe4++UjOKSLBS0ww7u3TitqV03jos9ls2Lab5644joqpmjI83hzJFUQ0rg17unu7QuYiPwNoFn4MAJ6NwvlEJE4MPLEJj1zQlnHz13DBs+NYvmF70JGkgEMtORppmu9NZraZ0D0RsdQXeNVDfgCqmFmdGJ9TRIrRhVkNGHxFFovXbqPvM2OZuXxT0JEkn4MWCHdPd/dKER7p7n6k14MOfGFmk8xsQIT99YD800LmhLeJSClyastavH19F9ydC58bx+i5q4OOJGFH0sR0pLq5ewdCTUk3mlmPAvsj9XEc0KxlZgPMLNvMslev1l8skZKoTf3KvDuwKzXSU7lq6ERNGR4nAisQ7r48/DMX+ADoVOCQHKBBvtf1geURPmewu2e5e1aNGrq5W6SkysyowHs3dKV13Urc/t40Hh/5M+4aBhukQAqEmVUws/R9z4HTgOkFDhsOXGkhnYGN7q5bMEVKseoVU3lzQBfObFObp7+ay21vT9Uw2AAFNa6sFvBBaCQrScC/3X2EmQ0EcPfngE8JDXGdR2iYq+7cFikDyqUk8sylHRj09TweG/kzi9du5cX+HalWISXoaGWOlaZLuKysLM/Ozg46hohEyfuTc/j9O1OpUymNV67pRLNa6UFHKnXMbFIhtxoE2kktInJQ53Woz2vXHM/6bbs5e9AYvvtZA1GKkwqEiMS1E5plMPymblRKS6b/kAkMHauJ/oqLCoSIxL1mtdL59LfdaV4rnfs+mskd707TCKdioAIhIiVCRsVUPrr5BHo2r8Fb2Us555mxbNmZF3SsUk0FQkRKjJSkBIZc1ZFfdz+KqTkb6fHwNyxeuzXoWKWWCoSIlChmxp9/1ZJHLmjLuq27OPGRb3knW3dex4IKhIiUSBdmNeDNAZ0B+OO707h3+IyAE5U+KhAiUmJ1blydT245AYCXxy3igmfHaZW6KFKBEJESrVXdyky5uxcA2YvX0/KeEWzctjvgVKWDCoSIlHhVyqcw729ncHStiuzYvZdj7/+CSYvXBx2rxFOBEJFSISkxgS9+dyJXdc0E4Pxnx/Ha94uCjFTiqUCISKlyb59WPH7RsQD89T8zuOnfk9UvcZhUIESk1DmvQ32++F1oDbKPp62g+8PfsGO3pg3/pVQgRKRUOrpWOjPuO50KKYks27CdFn8dwfzVW4KOVaKoQIhIqVUhNYnp953OyS1qAnDKY98xfOoBC1NKIVQgRKRUMzOGXNWRO89oAcAtw37kvo9mqF+iCFQgRKRMGHhiE16/9ngAho5dxKmPf8f2XeqXOBgVCBEpM05olsGEP5+CGSxYs5Vj7h7BkrXbgo4Vt1QgRKRMqZmexqz7e9OzeQ0AejzyDe9Oygk4VXxSgRCRMictOZGhV3firnC/xB/emarJ/iIo9gJhZg3M7Bszm2VmM8zstxGOOcnMNprZlPDj7uLOKSKl3/X5+iVeHreI3k+OUr9EPkFcQeQBv3f3Y4DOwI1m1jLCcaPdvV34cX/xRhSRsuKEZhl8f9fJVKuQwuyVmznm7hFMXZqkX/IAAAvZSURBVLoh6FhxodgLhLuvcPfJ4eebgVlAveLOISKyT53K5Rh1e09OCd8v0feZsQweNb/M330daB+EmWUC7YHxEXZ3MbOpZvaZmbU6yGcMMLNsM8tevXp1jJKKSGlXMTWJwVdm8eB5bQD4+6ezuf61SeSsL7ujnMw9mJtFzKwi8B3wN3d/v8C+SsBed99iZmcCT7l7s0N9ZlZWlmdnZ8cmsIiUGUvXbaP7w98AcGyDKvzj/Da0qF0p4FSxYWaT3D0r0r5AriDMLBl4D3ijYHEAcPdN7r4l/PxTINnMMoo5poiUUQ2qlWfSX07lzDa1mbp0A72fHM30ZRsJ6gt1UIIYxWTAS8Asd3+8kGNqh4/DzDoRyrm2+FKKSFlXvWIqD57X9r9NTmf9cwxXDpkQcKrilRTAObsBVwA/mdmU8LY/AQ0B3P054ALgBjPLA7YD/byslW4RCVzlcslcnNWAvL3Ohz8uY/zCddzw+iTO61CfXi1rBR0v5oq9QLj7GMAOccwgYFDxJBIRKVxCgnFF50bUr1KOf4yYzTdzclm7ZRdNa1akTuU00pITg44YM7qTWkSkCHq2qMmIW3twQtMaTFi0jp6Pfsv1r00KOlZMqUCIiPwC9/ZpyZMXt6NDwyrMWrGJd7KX8uXMVUHHigkVCBGRX6B+1fKc074enRtXJ3fzTv747jSuezWbhWu2Bh0t6lQgREQOwx9Oa86YO3ry6IXHAvD17Fy+mZPL3FWbA04WPUGMYhIRKfESEoz6VcvTsk4eAA98PBOACimJTL/vdMIj9Us0FQgRkSPQsm4lRv6uB1t25vHe5Bxe/2EJO/P2lorRTSoQIiJHqFmtdACmhGeBvXroRJISjZTEBP56VksyMyoEGe+wqQ9CRCRKujSpzvFHVWNn3h42bNvNV7Nz+WFByZ0EQlcQIiJR0qJ2Jd66vgsAa7fs5Lj/+7JETxmuAiEiEgP7+iDGzl9L/nmCWtWtTKejqgUT6hdSgRARiYFyyYnUqpTKyJmrGJnvRrp6Vcox9s6TA0xWdCoQIiIxkJBgjLq9535rXP/tk1l8PTs3wFS/jAqEiEiMpCYlkpr0v+GuFVKT2LVnb4CJfhmNYhIRKSYpSQnsLkEFQlcQIiLFJDnR2LF7L+c8Mzbi/suOb8iFWQ2KOVXhdAUhIlJMTj2mFj2b16BSueQDHvNyt/D5jJVBR9yPriBERIpJ+4ZVGXp1p4j7+gwaw5698bVwpq4gRETiQGKCkacCISIiBSWasddVIEREpIDEBCNvjwoEZtbbzOaY2TwzuzPC/lQzeyu8f7yZZRZ/ShGR4pOUaOqDMLNE4BngDKAlcImZtSxw2LXAendvCjwB/KN4U4qIFK8EM/bEWRNTEKOYOgHz3H0BgJm9CfQFZuY7pi9wb/j5u8AgMzP3OPvdExGJkqQEY+byTfR6/Ltf/N6q5VN4e2CX6GeK+iceWj1gab7XOcDxhR3j7nlmthGoDqwp+GFmNgAYANCwYcNY5BURiblLj29EuZTDW4WuUlpylNOEBFEgIi3UWvDKoCjHhDa6DwYGA2RlZekKQ0RKpF4ta9GrZa2gY+wniE7qHCD/veT1geWFHWNmSUBlYF2xpBMRESCYAjERaGZmR5lZCtAPGF7gmOFA//DzC4Cv1f8gIlK8ir2JKdyncBPwOZAIDHH3GWZ2P5Dt7sOBl4DXzGweoSuHfsWdU0SkrAtkLiZ3/xT4tMC2u/M93wFcWNy5RETkf3QntYiIRKQCISIiEalAiIhIRCoQIiISkZWm0aNmthpYfJhvzyDCndpxJN7zQfxnjPd8EP8Z4z0fxH/GeMvXyN1rRNpRqgrEkTCzbHfPCjpHYeI9H8R/xnjPB/GfMd7zQfxnjPd8+amJSUREIlKBEBGRiFQg/mdw0AEOId7zQfxnjPd8EP8Z4z0fxH/GeM/3X+qDEBGRiHQFISIiEalAiIhIRGW+QJhZbzObY2bzzOzOoPMUZGYNzOwbM5tlZjPM7LdBZ4rEzBLN7Ecz+zjoLJGYWRUze9fMZod/L6O/PuMRMLPfhf98p5vZMDNLi4NMQ8ws18ym59tWzcxGmtnc8M+qcZbvkfCf8TQz+8DMqgSVr7CM+fb9wczczDKCyFYUZbpAmFki8AxwBtASuMTMWgab6gB5wO/d/RigM3BjHGYE+C0wK+gQB/EUMMLdWwDHEkdZzawecAuQ5e6tCU2DHw9T3L8M9C6w7U7gK3dvBnwVfh2Ulzkw30igtbu3BX4G7iruUAW8zIEZMbMGQC9gSXEH+iXKdIEAOgHz3H2Bu+8C3gT6BpxpP+6+wt0nh59vJvQfW71gU+3PzOoDvwJeDDpLJGZWCehBaJ0R3H2Xu28INtUBkoBy4RUUy3PgKovFzt1HceBKjn2BV8LPXwHOKdZQ+UTK5+5fuHte+OUPhFasDEwhv4cATwC3U8hSyvGirBeIesDSfK9ziLP/fPMzs0ygPTA+2CQHeJLQX/a9QQcpRGNgNTA03Az2oplVCDrUPu6+DHiU0LfJFcBGd/8i2FSFquXuKyD05QWoGXCeg7kG+CzoEAWZWR9gmbtPDTrLoZT1AmERtsVlRTezisB7wK3uvinoPPuY2VlArrtPCjrLQSQBHYBn3b09sJVgm0b2E27H7wscBdQFKpjZ5cGmKtnM7M+EmmffCDpLfmZWHvgzcPehjo0HZb1A5AAN8r2uTxxc2hdkZsmEisMb7v5+0HkK6Ab0MbNFhJroTjaz14ONdIAcIMfd9115vUuoYMSLU4GF7r7a3XcD7wNdA85UmFVmVgcg/DM34DwHMLP+wFnAZXG4ln0TQl8Epob/zdQHJptZ7UBTFaKsF4iJQDMzO8rMUgh1DA4PONN+zMwItZ3PcvfHg85TkLvf5e713T2T0O/f1+4eV99+3X0lsNTMmoc3nQLMDDBSQUuAzmZWPvznfQpx1IlewHCgf/h5f+A/AWY5gJn1Bu4A+rj7tqDzFOTuP7l7TXfPDP+byQE6hP+Oxp0yXSDCnVk3AZ8T+gf5trvPCDbVAboBVxD6Zj4l/Dgz6FAl0M3AG2Y2DWgH/D3gPP8VvrJ5F5gM/ETo32Xg0zGY2TDge6C5meWY2bXAQ0AvM5tLaBTOQ3GWbxCQDowM/1t5Lqh8B8lYYmiqDRERiahMX0GIiEjhVCBERCQiFQgREYlIBUJERCJSgRARkYhUIEQAM9sSpc+ps29GWzM7ycw2hqf3mG1mj0Y7o5nVMLMRR/K5IoVRgRCJrtuAF/K9Hh2e3qM9cJaZdYvmydx9NbAi2p8rAioQIoUys0Zm9lV4bYGvzKxheHsTM/vBzCaa2f0FvtmfDxzwjd7dtwNTCE8GaWadzGxc+Opi3L67vM3sKjN738xGhNdceDhCrgwz+97MfhXe9CFwWXR/9SIqECIHMwh4Nby2wBvA0+HtTwFPuXtH8s3dZWZHAevdfWfBDwpPyNcMGBXeNBvoEb66uJv97+xuB1wMtAEuDq8dsO9zagGfAHe7+yfhzdlA9yP8tYocQAVCpHBdgH+Hn78GnJBv+zvh5//Od3wdQtOK59c9PL3HSuDjfHPuVAbeCa809gTQKt97vnL3je6+g9CcUY3C25MJLdJzu7uPzHd8LqFZYEWiSgVCpOgONS/NdqDgUqGjw1cgbYAbzKxdePsDwDfhFeTOLvC+/FcgewhNVw6h6asnAacXOEda+NwiUaUCIVK4cfxv6c/LgDHh5z8Q6muA/ZcG/RnIjPRB7v4z8CChmUYhdAWxLPz8qiLmcUKL4LSw/ddPPxo4YM1jkSOlAiESUj482+a+x22E1om+OtxEdAWhdbcBbgVuM7MJhJqVNgK4+1Zgvpk1LeQczwE9wn0VDwMPmtlYQmtQF4m77yFUlHqa2W/Cm3sS6pcQiSrN5iryC4VXBdvu7m5m/YBL3L1veN+5wHHu/pdizDMK6Ovu64vrnFI2JB36EBEp4DhgUHhxnw2Emn0AcPcPzKx6cQUxsxrA4yoOEgu6ghARkYjUByEiIhGpQIiISEQqECIiEpEKhIiIRKQCISIiEf0/6ES+kKDSPH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting curve for zipfs law\n",
    "counts = list(word_count.values())\n",
    "counts.sort(reverse = True)\n",
    "\n",
    "#Making a log version of lists\n",
    "log_freq = []\n",
    "log_ranks = []\n",
    "\n",
    "for count in counts:\n",
    "    log_freq.append( math.log(count))\n",
    "    \n",
    "for rank in range(1,len(log_freq) + 1):\n",
    "    log_ranks.append(math.log(rank))\n",
    " \n",
    "    \n",
    "f = plt.figure()\n",
    "plt.plot(log_ranks, log_freq)\n",
    "plt.xlabel('Log(Rank)')\n",
    "plt.ylabel('Log(Frequency)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.977259523130932"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating value of constant K\n",
    "\n",
    "log_sums = []\n",
    "\n",
    "for idx, val in enumerate(log_freq):\n",
    "\n",
    "    item_sum = log_freq[idx] + log_ranks[idx]\n",
    "    log_sums.append(item_sum)\n",
    "    \n",
    "avg_k = sum(log_sums)/len(log_sums)\n",
    "avg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3195517.239955811"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(avg_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3300000, 0, 30]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting K across range of values\n",
    "plt.plot(log_sums)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('K')\n",
    "plt.tick_params(\n",
    "    axis='x',          \n",
    "    which='both',      \n",
    "    bottom=False,      \n",
    "    top=False,         \n",
    "    labelbottom=False) \n",
    "plt.axis([0, 3300000, 0, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Making a list of all words in the claims and stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "\n",
    "ten_claims =  [75397,150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253]\n",
    "claim_string = []\n",
    "\n",
    "with open('data_files/train.jsonl') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "        \n",
    "            claim_dic = json.loads(line)\n",
    "            ID = claim_dic['id']\n",
    "            \n",
    "            \n",
    "            if ID in ten_claims:\n",
    "                claim_string.append(claim_dic['claim'])\n",
    "                text = claim_dic['claim'].lower()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                \n",
    "                claims.append(tokens)\n",
    "                \n",
    "#All claim words\n",
    "all_words = []\n",
    "\n",
    "for claim in claims:\n",
    "    for word in claim:\n",
    "        all_words.append(word)\n",
    "        \n",
    "#Removing stop words from claims\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_words = [word for word in all_words if not word in stop_words] \n",
    "all_words = [stemmer.stem(word) for word in all_words]\n",
    "\n",
    "all_words = list(set(all_words))\n",
    "all_words.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing TF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:10<00:00, 10.36it/s]\n"
     ]
    }
   ],
   "source": [
    "total_docs = 0\n",
    "for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    total_docs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [1:42:47<00:00, 56.58s/it]\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = np.zeros((total_docs, len(all_words)))\n",
    "doc_lengths = []\n",
    "ids = []\n",
    "current_doc = 0\n",
    "\n",
    "brackets = ['lrb', 'rrb', 'lsb', 'rsb', 'rcb', 'lcb']\n",
    "\n",
    "for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    \n",
    "                    text = json.loads(line)['text']\n",
    "                    text = text.lower()\n",
    "                    tokens = tokenizer.tokenize(text)\n",
    "                    \n",
    "                    #Removing stop words\n",
    "                    tokens = [word for word in tokens if not word in stop_words] \n",
    "                    \n",
    "                    #Removing brackets manually\n",
    "                    tokens = [word for word in tokens if not word in brackets] \n",
    "                    \n",
    "                    #Stemming\n",
    "                    tokens = [stemmer.stem(word) for word in tokens]\n",
    "                    \n",
    "                    #Appending doc_lengths\n",
    "                    doc_lengths.append(len(tokens))\n",
    "                    \n",
    "                    #Count the words in documents\n",
    "                    for ind,word in enumerate(all_words):\n",
    "                        if word in set(tokens):\n",
    "                            word_tf = tokens.count(word)/len(tokens)\n",
    "                            tf_matrix[current_doc][ind] = word_tf\n",
    "                                \n",
    "                    ids.append(json.loads(line)['id'])\n",
    "                    current_doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading in files to save time in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ids, open(\"pkl_files/ids.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pickle.load( open( \"pkl_files/ids.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(doc_lengths, open(\"pkl_files/doc_lengths.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = pickle.load( open( \"pkl_files/doc_lengths.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pkl_files/tf_matrix.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_out = pickle.dumps(tf_matrix)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, len(bytes_out), max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pkl_files/tf_matrix.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_in = bytearray(0)\n",
    "input_size = os.path.getsize(file_path)\n",
    "with open(file_path, 'rb') as f_in:\n",
    "    for _ in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "tf_matrix = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_doc = tf_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making IDF matrix\n",
    "\n",
    "#Counting number of documents with each term in it\n",
    "idf_matrix = np.zeros(len(all_words))\n",
    "\n",
    "for index in range(0, len(all_words)):\n",
    "    doc_count = np.count_nonzero(tf_matrix[:,index])\n",
    "    idf_matrix[index] = math.log10(total_doc/doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_matrix*idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing TF-IDF representation of claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_claims = []\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for claim in claims:\n",
    "    clean_claim = [word for word in claim if not word in stop_words] \n",
    "    clean_claim = [stemmer.stem(word) for word in clean_claim]\n",
    "    clean_claims.append(clean_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf for every claim/norm adjusted\n",
    "claims_tf_idfs = []\n",
    "\n",
    "for claim in clean_claims:\n",
    "    claim_tf = np.zeros(len(all_words))\n",
    "    \n",
    "    for idx, word in enumerate(all_words):\n",
    "        claim_tf[idx] = claim.count(word)/len(claim)\n",
    "            \n",
    "    claim_tf_idf = claim_tf*idf_matrix\n",
    "    claims_tf_idfs.append(claim_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Norm adjusting the matrices\n",
    "norm_rows_docs = []\n",
    "norm_rows_claims = []\n",
    "\n",
    "for row in tf_idf:\n",
    "    norm_doc = np.linalg.norm(row)\n",
    "    norm_adj_row = row\n",
    "    \n",
    "    if norm_doc!= 0:\n",
    "        norm_adj_row = norm_adj_row/norm_doc\n",
    "    \n",
    "    norm_rows_docs.append(norm_adj_row)\n",
    "        \n",
    "for claim in claims_tf_idfs:\n",
    "    norm_claim = np.linalg.norm(claim)\n",
    "    norm_adj_claim = claim/norm_claim\n",
    "    norm_rows_claims.append(norm_adj_claim)\n",
    "    \n",
    "doc_norm_tf_idf = np.array(norm_rows_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving 5 highest for each claim\n",
    "top5_list = []\n",
    "\n",
    "for claim in norm_rows_claims:\n",
    "    cosine_similarity = doc_norm_tf_idf@claim \n",
    "    highest_5_row = cosine_similarity.argsort()[-5:][::1]\n",
    "    \n",
    "    highest_5 = []\n",
    "    for ind in highest_5_row:\n",
    "        highest_5.append(ids[ind])\n",
    "        \n",
    "    highest_5.reverse()    \n",
    "    top5_list.append(highest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_disp_list = list(zip(claim_string, top5_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Likelihood Model\n",
    "unigram_similar = []\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        vec = tf_matrix[:, position]\n",
    "        probabs = probabs*vec\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    non_zero_unigram = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        if probabs[ind]!=0:\n",
    "            non_zero_unigram.append(ids[ind])\n",
    "    \n",
    "    non_zero_unigram.reverse()\n",
    "    unigram_similar.append((claim, non_zero_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 5, 5, 1, 1, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating returned unigrams\n",
    "returned_unigram = []\n",
    "\n",
    "for tup in unigram_similar:\n",
    "    returned_unigram.append(len(tup[1]))\n",
    "    \n",
    "returned_unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_similar = []\n",
    "#Creating array of doc lengths to adjust tf matrix back to just count\n",
    "doc_length_array = np.array(doc_lengths)\n",
    "\n",
    "#Creating new denominator to divide by\n",
    "adj_denom = doc_length_array + len(counts)\n",
    "adj_denom = np.reciprocal(adj_denom.astype(np.float32))\n",
    "\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        \n",
    "        #Required modification to tf matrix\n",
    "        vec = tf_matrix[:, position]\n",
    "        term_freq = vec*doc_length_array\n",
    "        \n",
    "        term_freq_plus1 = term_freq + 1\n",
    "        term_freq_div = term_freq_plus1*adj_denom\n",
    "        \n",
    "        #Calculating probabilities like with mle\n",
    "        probabs = probabs*term_freq_div\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    #Retrieving top5 artilces\n",
    "    top5_laplace = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        top5_laplace.append(ids[ind])\n",
    "    \n",
    "    top5_laplace.reverse()\n",
    "    laplace_similar.append((claim, top5_laplace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building necessary vector to represent entire corpus\n",
    "\n",
    "#Total words\n",
    "total_words = sum(counts)\n",
    "\n",
    "#Making a stemmed word dictionary\n",
    "word_count_list = list(word_count.items())\n",
    "\n",
    "stem_word_count = {}\n",
    "\n",
    "for word_tuple in word_count_list:\n",
    "    word = word_tuple[0]\n",
    "    count = word_tuple[1]\n",
    "    \n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    \n",
    "    if stemmed_word in stem_word_count:\n",
    "        stem_word_count[stemmed_word] += count\n",
    "        \n",
    "    else:\n",
    "        stem_word_count[stemmed_word] = count\n",
    "        \n",
    "claim_word_counts = []\n",
    "\n",
    "for word in all_words:\n",
    "    claim_word_counts.append(stem_word_count[word])\n",
    "    \n",
    "claim_words_array = np.array(claim_word_counts)\n",
    "claim_words_array_div = claim_words_array/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting lambda value\n",
    "lambda_val = 0.5\n",
    "\n",
    "#Calculating jelinek mercer similarity\n",
    "jelinek_similar = []\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        vec = tf_matrix[:, position]\n",
    "        \n",
    "        #jelinek mercer smoothing\n",
    "        lam_vec = lambda_val*vec\n",
    "        corpus_vec = lam_vec + ((1-lambda_val)*claim_words_array_div[all_words.index(word)])\n",
    "        \n",
    "        probabs = probabs*corpus_vec\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    jelinek_similar_claim = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        jelinek_similar_claim.append(ids[ind])\n",
    "    \n",
    "    jelinek_similar_claim.reverse()\n",
    "    jelinek_similar.append((claim, jelinek_similar_claim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building modified lambda value for dirichlet\n",
    "avg_length = total_words/ tf_matrix.shape[0]\n",
    "\n",
    "dirich_denom = doc_length_array + avg_length\n",
    "dirich_denom = np.reciprocal(dirich_denom.astype(np.float32))\n",
    "\n",
    "dirich_constant = doc_length_array*dirich_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_similar = []\n",
    "\n",
    "for claim in clean_claims:\n",
    "\n",
    "    probabs = np.ones(tf_matrix.shape[0])\n",
    "    \n",
    "    for word in claim:\n",
    "        position = all_words.index(word)\n",
    "        vec = tf_matrix[:, position]\n",
    "        \n",
    "        \n",
    "        dirich_vec = dirich_constant*vec\n",
    "        corpus_dirich_vec = dirich_vec + ((1-dirich_constant)*claim_words_array_div[all_words.index(word)])\n",
    "        \n",
    "        probabs = probabs*corpus_dirich_vec\n",
    "\n",
    "    highest_prob_indices = probabs.argsort()[-5:][::1]\n",
    "    \n",
    "    dirich_similar_claim = []   \n",
    "    for ind in highest_prob_indices:\n",
    "        dirich_similar_claim.append(ids[ind])\n",
    "    \n",
    "    dirich_similar_claim.reverse()\n",
    "    dirichlet_similar.append((claim, dirich_similar_claim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending output to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_claims =  [75397,150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253]\n",
    "\n",
    "def csv_sender(results, ten_claims):\n",
    "    \n",
    "    list_of_lists =[]\n",
    "    \n",
    "    for result in results:\n",
    "        list_of_lists.append(result[1])\n",
    "        \n",
    "    df = pd.DataFrame(list_of_lists)\n",
    "    \n",
    "    df.columns = ['doc id_1', 'doc id_2', 'doc id_3', 'doc id_4','doc id_5' ]\n",
    "    df.index = ten_claims\n",
    "    df.index.name = 'claim id'\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "csv_sender(claim_disp_list, ten_claims).to_csv('csv_files/cosine.csv')\n",
    "csv_sender(unigram_similar, ten_claims).to_csv('csv_files/unigram.csv')\n",
    "csv_sender(laplace_similar, ten_claims).to_csv('csv_files/laplace.csv')\n",
    "csv_sender(jelinek_similar, ten_claims).to_csv('csv_files/jelinek.csv')\n",
    "csv_sender(dirichlet_similar, ten_claims).to_csv('csv_files/dirichlet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "#Loading in the glove model\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel('glove.6B.50D.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in document lines for specific claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['nikolaj', 'coster', 'waldau', 'work', 'fox', 'broadcast', 'compani'],\n",
       " ['New_Amsterdam_-LRB-TV_series-RRB-',\n",
       "  'Nikolaj_Coster-Waldau',\n",
       "  'The_Other_Woman_-LRB-2014_film-RRB-',\n",
       "  'Ved_verdens_ende',\n",
       "  'Nukaaka_Coster-Waldau'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting the claim to check\n",
    "claim_to_check = dirichlet_similar[0]\n",
    "claim_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:54<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "#Loading in lines from relevant documents\n",
    "\n",
    "lines_from_doc = []\n",
    "\n",
    "bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    iD = json.loads(line)['id']\n",
    "                    \n",
    "                    if iD in claim_to_check[1]:\n",
    "                        doc_lines = json.loads(line)['lines']\n",
    "                        doc_line_list = doc_lines.split('\\n')\n",
    "                        \n",
    "            \n",
    "                        for doc_line in doc_line_list:\n",
    "                            doc_line = doc_line.lower()\n",
    "                            #Isolating cleaned up version of line\n",
    "                            doc_line = doc_line.split('\\t')[1]\n",
    "                            doc_line = doc_line.split('.')[0]\n",
    "                            \n",
    "                            doc_line = doc_line.split(' ')\n",
    "                            \n",
    "                            doc_line = doc_line[:-1]\n",
    "                            \n",
    "                            #Sorting out the brackets\n",
    "                            doc_line_brac = []\n",
    "                            for word in doc_line:\n",
    "                                \n",
    "                                if word in set(list(bracket_dict.keys())):\n",
    "                                    word = bracket_dict[word]\n",
    "                                \n",
    "                                doc_line_brac.append(word)\n",
    "                                \n",
    "                        #doc_tokens = nltk.word_tokenize(doc_lines)\n",
    "                            lines_from_doc.append(doc_line_brac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing empty lines\n",
    "clean_doc_lines = []\n",
    "\n",
    "for line in lines_from_doc:\n",
    "    \n",
    "    if len(line) != 0:\n",
    "        clean_doc_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector representation of all sentences and claim (1 claim and 51 sentences)\n",
    "vector_representations = []\n",
    "\n",
    "the_claim = claim_to_check[0]\n",
    "\n",
    "sum_count_vec = np.zeros(50)\n",
    "for word in the_claim:\n",
    "    try:\n",
    "        vec_rep = model[word]\n",
    "\n",
    "    except:\n",
    "        vec_rep = 0\n",
    "    \n",
    "    sum_count_vec += vec_rep\n",
    "\n",
    "vector_representations.append(sum_count_vec)\n",
    "\n",
    "for sentence in clean_doc_lines:\n",
    "    \n",
    "    sent_count_vec = np.zeros(50)\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            vec_rep = model[word]\n",
    "\n",
    "        except:\n",
    "            vec_rep = 0\n",
    "\n",
    "        sent_count_vec += vec_rep\n",
    "\n",
    "    vector_representations.append(sent_count_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in training claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_claims = []\n",
    "veri_evid = []\n",
    "veri_label = []\n",
    "\n",
    "binary_claim_dic = {'SUPPORTS':1, 'REFUTES':0}\n",
    "\n",
    "with open('data_files/train.jsonl') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "            \n",
    "            claim_dic = json.loads(line)\n",
    "            veri_check = claim_dic['verifiable']\n",
    "            \n",
    "            if veri_check == 'VERIFIABLE':\n",
    "                claim = claim_dic['claim']\n",
    "                evidence = claim_dic['evidence']\n",
    "                label = binary_claim_dic[claim_dic['label']]\n",
    "                \n",
    "                veri_claims.append(claim)\n",
    "                veri_evid.append(evidence)\n",
    "                veri_label.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolating subset which can be verified by just one line\n",
    "one_line_claims = []\n",
    "one_line_evidence = []\n",
    "one_line_labels = []\n",
    "\n",
    "for idx, evid in enumerate(veri_evid):\n",
    "    if len(evid[0]) == 1:\n",
    "        one_line_evidence.append(evid)\n",
    "        one_line_claims.append(veri_claims[idx])\n",
    "        one_line_labels.append(veri_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a clean evidence list\n",
    "\n",
    "clean_evidence = []\n",
    "\n",
    "for evidence in one_line_evidence:\n",
    "    \n",
    "    one_claim_list = []\n",
    "    for each in evidence:\n",
    "        one_evid = each[0]\n",
    "        \n",
    "        document = one_evid[2]\n",
    "        line = one_evid[3]\n",
    "        \n",
    "        one_claim_list.append((document, line))\n",
    "    \n",
    "    clean_evidence.append(one_claim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a dicitonary of all required articles and lines\n",
    "all_required_dict = {}\n",
    "\n",
    "for evidence in clean_evidence:\n",
    "    \n",
    "    key_val = unicodedata.normalize('NFC', evidence[0][0])\n",
    "    \n",
    "    for each in evidence:\n",
    "        if key_val in all_required_dict:\n",
    "            all_required_dict[key_val].append(each[1])\n",
    "            all_required_dict[key_val] = list(set(all_required_dict[key_val]))\n",
    "        \n",
    "        else:\n",
    "            all_required_dict[key_val] = [each[1]] \n",
    "            \n",
    "all_required_dict\n",
    "\n",
    "#Retrieving just the article titles\n",
    "all_required_articles = list(all_required_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:52<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "veri_lines_dict = {}\n",
    "number_of_lines = {}  #number of lines storing later for negative sampling irrelevant sentences\n",
    "\n",
    "bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "for file in tqdm(list_of_wiki):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    iD = json.loads(line)['id']\n",
    "                    \n",
    "                    iD = unicodedata.normalize('NFC',iD)\n",
    "                        \n",
    "                    if iD in all_required_dict:\n",
    "           \n",
    "                        doc_lines = json.loads(line)['lines']\n",
    "                                                \n",
    "                        doc_line_list = doc_lines.split('\\n')\n",
    "                        \n",
    "                        number_of_lines[iD] = len(doc_line_list)\n",
    "                                                                \n",
    "                        for line_number in range(0, len(doc_line_list)):\n",
    "                            doc_line = doc_line_list[line_number].lower()\n",
    "                            \n",
    "                            #Isolating cleaned up version of line\n",
    "                            doc_line = doc_line.split('\\t')[1]\n",
    "                            doc_line = doc_line.split('.')[0]\n",
    "                            \n",
    "                            doc_line = doc_line.split(' ')\n",
    "                            \n",
    "                            #Sorting out the brackets\n",
    "                            doc_line_brac = []\n",
    "                            for word in doc_line:\n",
    "                                \n",
    "                                if word in set(list(bracket_dict.keys())):\n",
    "                                    word = bracket_dict[word]\n",
    "                                    \n",
    "                                doc_line_brac.append(word)\n",
    "                            \n",
    "                            veri_lines_dict[(iD, line_number)] = doc_line_brac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling to create irrelevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_list= []\n",
    "\n",
    "for evidence in clean_evidence:\n",
    "    evid_dict = {}\n",
    "    \n",
    "    for each_evid in evidence:\n",
    "        document = each_evid[0]\n",
    "        document = unicodedata.normalize('NFC',document)\n",
    "        \n",
    "        rel_line = each_evid[1]\n",
    "        \n",
    "        #Creating a dictionary with all\n",
    "        if document in evid_dict:\n",
    "            evid_dict[document].append(rel_line)\n",
    "            evid_dict[document] = list(set(evid_dict[document]))\n",
    "            \n",
    "        else:\n",
    "            evid_dict[document]  = [rel_line]\n",
    "     \n",
    "    try:\n",
    "        total_sentences = number_of_lines[document]\n",
    "        \n",
    "        sample_list = list(range(0,total_sentences))\n",
    "        sample_list = [number for number in sample_list if not number in evid_dict[document]]\n",
    "        \n",
    "    except:\n",
    "        sample_list = 'doc not found'\n",
    "        print(document)\n",
    "        \n",
    "    neg_sample_list.append((document, sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95338\n",
      "95338\n",
      "95338\n"
     ]
    }
   ],
   "source": [
    "#Everything is same length\n",
    "print(len(one_line_claims))\n",
    "print(len(clean_evidence))\n",
    "print(len(neg_sample_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Actual Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_lines_evidence = []\n",
    "\n",
    "for evidence in clean_evidence:\n",
    "    sub_list =[]\n",
    "    \n",
    "    for each in evidence:\n",
    "        each = (unicodedata.normalize('NFC',each[0]), each[1])\n",
    "        \n",
    "        actual_line = veri_lines_dict[each]\n",
    "        actual_line = actual_line[:-1]\n",
    "        sub_list.append(actual_line)\n",
    "        \n",
    "    actual_lines_evidence.append(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating equivalent negative sample for every claim\n",
    "import random\n",
    "random.seed(a=0)\n",
    "\n",
    "neg_lines_evidence = []\n",
    "\n",
    "for idx, evidence in enumerate(clean_evidence):\n",
    "    neg_sub_list =[]\n",
    "    \n",
    "    for each in evidence:\n",
    "        \n",
    "        neg_sample_numbers = neg_sample_list[idx][1]\n",
    "        neg_line_number = random.choice(neg_sample_numbers)\n",
    "        \n",
    "        each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "        neg_line = veri_lines_dict[each]\n",
    "        \n",
    "        #while loop to avoid empty list\n",
    "        while len(neg_line) == 0:\n",
    "            neg_line_number = random.choice(neg_sample_numbers)\n",
    "            each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "            neg_line = veri_lines_dict[each]\n",
    "        \n",
    "        neg_line = neg_line[:-1]\n",
    "        neg_sub_list.append(neg_line)\n",
    "    \n",
    "    neg_lines_evidence.append(neg_sub_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating claim evidence pairs and deriving vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise the claims\n",
    "\n",
    "one_line_tokenised = []\n",
    "\n",
    "for claim in one_line_claims:\n",
    "    no_stop = claim[:-1]\n",
    "    lower_claim = no_stop.lower()\n",
    "    tokenised = lower_claim.split(' ')\n",
    "    one_line_tokenised.append(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create verifiable tuples\n",
    "pos_claim_evidence = []\n",
    "neg_claim_evidence = []\n",
    "\n",
    "for idx,evidence in enumerate(actual_lines_evidence):\n",
    "    for each in evidence:\n",
    "        veri_tuples = (one_line_tokenised[idx], each)\n",
    "        pos_claim_evidence.append(veri_tuples)\n",
    "        \n",
    "for idx, evidence in enumerate(neg_lines_evidence):\n",
    "    for each in evidence:\n",
    "        neg_tuples = (one_line_tokenised[idx], each)\n",
    "        neg_claim_evidence.append(neg_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197514/197514 [00:10<00:00, 18138.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Relevant arrays\n",
    "verif_arrays = []\n",
    "\n",
    "for pair in tqdm(pos_claim_evidence):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    evidence = pair[1]\n",
    "    evidence_array = np.zeros(50)\n",
    "    for word in evidence:\n",
    "        try:\n",
    "            evidence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, evidence_array])\n",
    "    \n",
    "    verif_arrays.append(array_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197514/197514 [00:06<00:00, 31407.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#Irrelevant arrays\n",
    "neg_arrays = []\n",
    "\n",
    "for pair in tqdm(neg_claim_evidence):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    neg_sample = pair[1]\n",
    "    neg_array = np.zeros(50)\n",
    "    for word in neg_sample:\n",
    "        try:\n",
    "            neg_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, neg_array])\n",
    "    \n",
    "    neg_arrays.append(array_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing feature and target matrix \n",
    "\n",
    "veri_matrix = np.array(verif_arrays)\n",
    "neg_matrix = np.array(neg_arrays)\n",
    "rel_y = np.ones(len(verif_arrays))\n",
    "neg_y = np.zeros(len(neg_arrays))\n",
    "feature_matrix = np.concatenate((veri_matrix,neg_matrix), axis=0)\n",
    "target_matrix = np.concatenate((rel_y,neg_y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def log_regression(x, y, epochs, learning_rate = 0.001):\n",
    "    \n",
    "    #Adding an intercept term\n",
    "    n = x.shape[0]\n",
    "    one_column = np.ones((x.shape[0],1))   #adding an intercept term\n",
    "    x = np.concatenate((one_column, x), axis = 1)\n",
    "    xT = x.T\n",
    "    \n",
    "    features = x.shape[1]\n",
    "    \n",
    "    #Initialising weights\n",
    "    w = np.zeros(features)\n",
    "    \n",
    "    avg_costs = []\n",
    "    \n",
    "    for epoch in trange(0,epochs):\n",
    "    \n",
    "        #Making prediction        \n",
    "        wx = w@xT\n",
    "        pred = 1/ (1 + np.exp(-wx))\n",
    "\n",
    "        #Calculating cost\n",
    "        class1_cost = -y*np.log(pred)\n",
    "        class0_cost = -(1-y)*np.log(1-pred)\n",
    "        \n",
    "        cost = class1_cost + class0_cost\n",
    "        avg_cost = np.mean(cost)\n",
    "\n",
    "        #Calculating gradient\n",
    "        predT = pred.T\n",
    "\n",
    "        gradient = xT@(predT - y)\n",
    "        gradient = gradient/n #average gradient\n",
    "\n",
    "        gradientT = gradient.T\n",
    "\n",
    "        gradientT *= learning_rate\n",
    "\n",
    "        w -= gradientT\n",
    "        \n",
    "        avg_costs.append(avg_cost)\n",
    "    \n",
    "    return(w, avg_costs)\n",
    "\n",
    "weights, loss_list = log_regression(feature_matrix, target_matrix.T, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking loss\n",
    "plt.plot(loss_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7204780420628412"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_func(feature_matrix, target_matrix, weights):\n",
    "    \n",
    "    one_column = np.ones((feature_matrix.shape[0],1))\n",
    "    feature_matrix = np.concatenate([one_column, feature_matrix ], axis = 1)\n",
    "    \n",
    "    lin_pred = feature_matrix@weights\n",
    "    class_pred = 1/ (1 + np.exp(-lin_pred))\n",
    "    class_pred = np.around(class_pred)\n",
    "\n",
    "    score = 0\n",
    "        \n",
    "    for idx in range(0,len(class_pred)):\n",
    "        val = target_matrix[idx]\n",
    "        pred = class_pred[idx]\n",
    "\n",
    "        if val == pred:\n",
    "            score += 1\n",
    "\n",
    "    accuracy = score/len(class_pred)\n",
    "    \n",
    "    return(class_pred, accuracy)\n",
    "\n",
    "pred, accuracy = accuracy_func(feature_matrix, target_matrix, weights)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part iii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving unique words in the dev set via cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_claim_dicts = []\n",
    "claims_4 = []\n",
    "claim_string_4 = []\n",
    "\n",
    "ten_claims = [137334, 111897, 89891, 181634, 219028, 108281, 204361, 54168, 105095, 18708]\n",
    "\n",
    "with open('data_files/shared_task_dev.jsonl') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "        \n",
    "            claim_dic = json.loads(line)\n",
    "            ID = claim_dic['id']\n",
    "            \n",
    "            if ID in ten_claims:\n",
    "                claim_string_4.append(claim_dic['claim'])\n",
    "                \n",
    "                dev_claim_dicts.append(claim_dic)\n",
    "                text = claim_dic['claim'].lower()\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                \n",
    "                claims_4.append(tokens)\n",
    "\n",
    "                \n",
    "#All claim words\n",
    "all_words_4 = []\n",
    "\n",
    "for claim in claims_4:\n",
    "    for word in claim:\n",
    "        all_words_4.append(word)\n",
    "        \n",
    "#Removing stop words from claims\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_words_4 = [word for word in all_words_4 if not word in stop_words] \n",
    "all_words_4 = [stemmer.stem(word) for word in all_words_4]\n",
    "\n",
    "all_words_4 = list(set(all_words_4))\n",
    "all_words_4.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing tf-idf matrix to retrieve documents for 10 claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [1:32:18<00:00, 50.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#Constructing tf-matrix for the 10 claims that need to be verified\n",
    "\n",
    "tf_matrix_4 = np.zeros((total_doc, len(all_words_4)))\n",
    "current_doc = 0\n",
    "\n",
    "brackets = ['lrb', 'rrb', 'lsb', 'rsb', 'rcb', 'lcb']\n",
    "\n",
    "for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    \n",
    "                    text = json.loads(line)['text']\n",
    "                    text = text.lower()\n",
    "                    tokens = tokenizer.tokenize(text)\n",
    "                    \n",
    "                    #Removing stop words\n",
    "                    tokens = [word for word in tokens if not word in stop_words] \n",
    "                    \n",
    "                    #Removing brackets manually\n",
    "                    tokens = [word for word in tokens if not word in brackets] \n",
    "                    \n",
    "                    #Stemming\n",
    "                    tokens = [stemmer.stem(word) for word in tokens]\n",
    "                    \n",
    "                    #Count the words in documents\n",
    "                    for ind,word in enumerate(all_words_4):\n",
    "                        if word in set(tokens):\n",
    "                            word_tf = tokens.count(word)/len(tokens)\n",
    "                            tf_matrix_4[current_doc][ind] = word_tf\n",
    "                                \n",
    "                    current_doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_files/tf_matrix_4.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_out = pickle.dumps(tf_matrix_4)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, len(bytes_out), max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_files/tf_matrix_4.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "data = bytearray(n_bytes)\n",
    "\n",
    "bytes_in = bytearray(0)\n",
    "input_size = os.path.getsize(file_path)\n",
    "with open(file_path, 'rb') as f_in:\n",
    "    for _ in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "tf_matrix_4 = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making IDF matrix for question 4\n",
    "\n",
    "#Counting number of documents with each term in it\n",
    "idf_matrix_4 = np.zeros(len(all_words_4))\n",
    "\n",
    "for index in range(0, len(all_words_4)):\n",
    "    doc_count = np.count_nonzero(tf_matrix_4[:,index])\n",
    "    idf_matrix_4[index] = math.log10(total_doc/doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_4 = tf_matrix_4*idf_matrix_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_claims_4 = []\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for claim in claims_4:\n",
    "    clean_claim = [word for word in claim if not word in stop_words] \n",
    "    clean_claim = [stemmer.stem(word) for word in clean_claim]\n",
    "    clean_claims_4.append(clean_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf for every claim\n",
    "claims_tf_idfs_4 = []\n",
    "\n",
    "for claim in clean_claims_4:\n",
    "    claim_tf_4 = np.zeros(len(all_words_4))\n",
    "    \n",
    "    for idx, word in enumerate(all_words_4):\n",
    "        claim_tf_4[idx] = claim.count(word)/len(claim)\n",
    "            \n",
    "    claim_tf_idf_4 = claim_tf_4*idf_matrix_4\n",
    "    claims_tf_idfs_4.append(claim_tf_idf_4)\n",
    "    \n",
    "#Norm adjusting the matrices\n",
    "norm_rows_docs_4 = []\n",
    "norm_rows_claims_4 = []\n",
    "\n",
    "for row in tf_idf_4:\n",
    "    norm_doc = np.linalg.norm(row)\n",
    "    norm_adj_row = row\n",
    "    \n",
    "    if norm_doc!= 0:\n",
    "        norm_adj_row = norm_adj_row/norm_doc\n",
    "    \n",
    "    norm_rows_docs_4.append(norm_adj_row)\n",
    "        \n",
    "for claim in claims_tf_idfs_4:\n",
    "    norm_claim = np.linalg.norm(claim)\n",
    "    norm_adj_claim = claim/norm_claim\n",
    "    norm_rows_claims_4.append(norm_adj_claim)\n",
    "    \n",
    "doc_norm_tf_idf_4 = np.array(norm_rows_docs_4)\n",
    "\n",
    "#Retrieving 5 highest for each claim\n",
    "top5_list_4 = []\n",
    "\n",
    "for claim in norm_rows_claims_4:\n",
    "    cosine_similarity = doc_norm_tf_idf_4@claim \n",
    "    highest_5_row = cosine_similarity.argsort()[-5:][::1]\n",
    "    \n",
    "    highest_5 = []\n",
    "    for ind in highest_5_row:\n",
    "        highest_5.append(ids[ind])\n",
    "        \n",
    "    highest_5.reverse()    \n",
    "    top5_list_4.append(highest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving list of sentences for every document\n",
    "rel_docs = merged = list(itertools.chain.from_iterable(top5_list_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all the sentences only in the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:55<00:00,  1.97it/s]\n",
      "100%|██████████| 109/109 [00:55<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "all_lines_dict = {}   #Creating a dictionry of all lines\n",
    "number_of_lines = {}  #number of lines storing later for negative sampling irrelevant sentences\n",
    "\n",
    "bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "        with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                for iline,line in enumerate(openfile.readlines()):\n",
    "                    iD = json.loads(line)['id']\n",
    "                    \n",
    "                    iD = unicodedata.normalize('NFC',iD)\n",
    "                        \n",
    "                    if iD in rel_docs:\n",
    "           \n",
    "                        doc_lines = json.loads(line)['lines']\n",
    "                                                \n",
    "                        doc_line_list = doc_lines.split('\\n')\n",
    "                        \n",
    "                        number_of_lines[iD] = len(doc_line_list)\n",
    "                                                                \n",
    "                        for line_number in range(0, len(doc_line_list)):\n",
    "                            doc_line = doc_line_list[line_number].lower()\n",
    "                            \n",
    "                            #Isolating cleaned up version of line\n",
    "                            doc_line = doc_line.split('\\t')[1]\n",
    "                            doc_line = doc_line.split('.')[0]\n",
    "                            \n",
    "                            doc_line = doc_line.split(' ')\n",
    "                            \n",
    "                            #Sorting out the brackets\n",
    "                            if len(doc_line) != 0:\n",
    "                            \n",
    "                                doc_line_brac = []\n",
    "                                for word in doc_line:\n",
    "\n",
    "                                    if word in set(list(bracket_dict.keys())):\n",
    "                                        word = bracket_dict[word]\n",
    "\n",
    "                                    doc_line_brac.append(word)\n",
    "\n",
    "                                all_lines_dict[(iD, line_number)] = doc_line_brac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a cleaned up representation of claims and retrieving evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "evidences = []\n",
    "\n",
    "for dic in dev_claim_dicts:\n",
    "    evidences.append(dic['evidence'])\n",
    "    claim = dic['claim']\n",
    "    claim = claim.lower()\n",
    "    claim = claim[:-1]\n",
    "    claim = claim.split(' ')\n",
    "    \n",
    "    no_apos_claim = []\n",
    "    #Removing apostrophe for vector representation later\n",
    "    for word in claim:\n",
    "        if \"'\" in word:\n",
    "            no_apos_claim.append(word[:-2])\n",
    "            no_apos_claim.append(\"'s\")\n",
    "            \n",
    "        else:\n",
    "            no_apos_claim.append(word)\n",
    "    \n",
    "    claims.append(no_apos_claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary mapping document to claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_claim_mapper = {}\n",
    "\n",
    "for idx,docs in enumerate(top5_list_4):\n",
    "    for doc in docs:\n",
    "        doc_claim_mapper[doc] = claims[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the evidence and making a list of relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block to retrieve relevant sentences\n",
    "\n",
    "rel_sent = []\n",
    "for evidence in evidences:\n",
    "    for lst in evidence:\n",
    "        for one in lst:\n",
    "            rel_tuple = (one[2], one[3])\n",
    "            rel_sent.append(rel_tuple)\n",
    "\n",
    "rel_sent = list(set(rel_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying which sentences cosine simlarity picked up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_search_sent = []\n",
    "\n",
    "for sent in rel_sent:\n",
    "    doc = sent[0]\n",
    "    \n",
    "    if doc in rel_docs:\n",
    "        rel_search_sent.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a list of sentences to loop and populate 3 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(all_lines_dict.items())\n",
    "\n",
    "claim_list = []\n",
    "sentence_list = []\n",
    "target_list = []\n",
    "\n",
    "for tup in all_sentences:\n",
    "    \n",
    "    #Find the claim\n",
    "    doc = tup[0][0]\n",
    "    claim = doc_claim_mapper[doc]\n",
    "    claim_list.append(claim)\n",
    "    \n",
    "    #Find the sentence\n",
    "    sentence = tup[1]\n",
    "    sentence_list.append(sentence)\n",
    "    \n",
    "    #Find whether relevant or not\n",
    "    check = tup[0]\n",
    "    \n",
    "    target = 0\n",
    "    if check in rel_sent:\n",
    "        target = 1\n",
    "    target_list.append(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vector Representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_arrays = []\n",
    "sentence_arrays = []\n",
    "\n",
    "for claim in claim_list:\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        claim_array += model[word]\n",
    "    claim_arrays.append(claim_array)\n",
    "    \n",
    "\n",
    "for sentence in sentence_list:\n",
    "    sentence_array = np.zeros(50)\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sentence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "    sentence_arrays.append(sentence_array)\n",
    "    \n",
    "claim_matrix = np.array(claim_arrays)\n",
    "sentence_matrix = np.array(sentence_arrays)\n",
    "one_column = np.ones((len(claim_arrays),1))\n",
    "\n",
    "#Making feature matrix\n",
    "feature_matrix_dev = np.concatenate([one_column,claim_matrix, sentence_matrix], axis = 1)\n",
    "target_matrix_dev = np.array(target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making prediction on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_pred = feature_matrix_dev@weights\n",
    "class_pred = 1/ (1 + np.exp(-lin_pred))\n",
    "class_pred = np.around(class_pred)\n",
    "\n",
    "score = 0\n",
    "\n",
    "for idx in range(0,len(class_pred)):\n",
    "    val = target_matrix_dev[idx]\n",
    "    pred = class_pred[idx]\n",
    "    \n",
    "    if val == pred:\n",
    "        score += 1\n",
    "        \n",
    "accuracy = score/len(class_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test set for logistic regression in same manner as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:51<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#Recalling same code chunk from before but with shared_task_dev file via function\n",
    "def pair_maker(filename):\n",
    "    \n",
    "    veri_claims = []\n",
    "    veri_evid = []\n",
    "    veri_label = []\n",
    "\n",
    "    binary_claim_dic = {'SUPPORTS':1, 'REFUTES':0}\n",
    "\n",
    "    with open(filename) as openfile:\n",
    "            for iline, line in enumerate(openfile.readlines()):\n",
    "\n",
    "                claim_dic = json.loads(line)\n",
    "                veri_check = claim_dic['verifiable']\n",
    "\n",
    "                if veri_check == 'VERIFIABLE':\n",
    "                    claim = claim_dic['claim']\n",
    "                    evidence = claim_dic['evidence']\n",
    "                    label = binary_claim_dic[claim_dic['label']]\n",
    "\n",
    "                    veri_claims.append(claim)\n",
    "                    veri_evid.append(evidence)\n",
    "                    veri_label.append(label)\n",
    "                    \n",
    "    one_line_claims = []\n",
    "    one_line_evidence = []\n",
    "    one_line_labels = []\n",
    "\n",
    "    for idx, evid in enumerate(veri_evid):\n",
    "        if len(evid[0]) == 1:\n",
    "            one_line_evidence.append(evid)\n",
    "            one_line_claims.append(veri_claims[idx])\n",
    "            one_line_labels.append(veri_label[idx])\n",
    "\n",
    "    clean_evidence = []\n",
    "    \n",
    "    for evidence in one_line_evidence:\n",
    "\n",
    "        one_claim_list = []\n",
    "        for each in evidence:\n",
    "            one_evid = each[0]\n",
    "\n",
    "            document = one_evid[2]\n",
    "            line = one_evid[3]\n",
    "\n",
    "            one_claim_list.append((document, line))\n",
    "\n",
    "        clean_evidence.append(one_claim_list)\n",
    "\n",
    "\n",
    "    #Making a dicitonary of all required articles and lines\n",
    "    all_required_dict = {}\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "\n",
    "        key_val = unicodedata.normalize('NFC', evidence[0][0])\n",
    "\n",
    "        for each in evidence:\n",
    "            if key_val in all_required_dict:\n",
    "                all_required_dict[key_val].append(each[1])\n",
    "                all_required_dict[key_val] = list(set(all_required_dict[key_val]))\n",
    "\n",
    "            else:\n",
    "                all_required_dict[key_val] = [each[1]] \n",
    "\n",
    "    all_required_dict\n",
    "\n",
    "    #Retrieving just the article titles\n",
    "    all_required_articles = list(all_required_dict.keys())\n",
    "\n",
    "    veri_lines_dict = {}\n",
    "    number_of_lines = {}  #number of lines storing later for negative sampling irrelevant sentences\n",
    "\n",
    "    bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "    for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "            with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                    for iline,line in enumerate(openfile.readlines()):\n",
    "                        iD = json.loads(line)['id']\n",
    "\n",
    "                        iD = unicodedata.normalize('NFC',iD)\n",
    "\n",
    "                        if iD in all_required_dict:\n",
    "\n",
    "                            doc_lines = json.loads(line)['lines']\n",
    "\n",
    "                            doc_line_list = doc_lines.split('\\n')\n",
    "\n",
    "                            number_of_lines[iD] = len(doc_line_list)\n",
    "\n",
    "                            for line_number in range(0, len(doc_line_list)):\n",
    "                                doc_line = doc_line_list[line_number].lower()\n",
    "\n",
    "                                #Isolating cleaned up version of line\n",
    "                                doc_line = doc_line.split('\\t')[1]\n",
    "                                doc_line = doc_line.split('.')[0]\n",
    "\n",
    "                                doc_line = doc_line.split(' ')\n",
    "\n",
    "                                #Sorting out the brackets\n",
    "                                doc_line_brac = []\n",
    "                                for word in doc_line:\n",
    "\n",
    "                                    if word in set(list(bracket_dict.keys())):\n",
    "                                        word = bracket_dict[word]\n",
    "\n",
    "                                    doc_line_brac.append(word)\n",
    "\n",
    "                                veri_lines_dict[(iD, line_number)] = doc_line_brac\n",
    "\n",
    "    neg_sample_list= []\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "        evid_dict = {}\n",
    "\n",
    "        for each_evid in evidence:\n",
    "            document = each_evid[0]\n",
    "            document = unicodedata.normalize('NFC',document)\n",
    "\n",
    "            rel_line = each_evid[1]\n",
    "\n",
    "            #Creating a dictionary with all\n",
    "            if document in evid_dict:\n",
    "                evid_dict[document].append(rel_line)\n",
    "                evid_dict[document] = list(set(evid_dict[document]))\n",
    "\n",
    "            else:\n",
    "                evid_dict[document]  = [rel_line]\n",
    "\n",
    "        try:\n",
    "            total_sentences = number_of_lines[document]\n",
    "\n",
    "            sample_list = list(range(0,total_sentences))\n",
    "            sample_list = [number for number in sample_list if not number in evid_dict[document]]\n",
    "\n",
    "        except:\n",
    "            sample_list = 'doc not found'\n",
    "\n",
    "        neg_sample_list.append((document, sample_list))\n",
    "\n",
    "    actual_lines_evidence = []\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "        sub_list =[]\n",
    "\n",
    "        for each in evidence:\n",
    "            each = (unicodedata.normalize('NFC',each[0]), each[1])\n",
    "\n",
    "            actual_line = veri_lines_dict[each]\n",
    "            actual_line = actual_line[:-1]\n",
    "            sub_list.append(actual_line)\n",
    "\n",
    "        actual_lines_evidence.append(sub_list)\n",
    "\n",
    "    #Creating equivalent negative sample for every claim\n",
    "    neg_lines_evidence = []\n",
    "\n",
    "    for idx, evidence in enumerate(clean_evidence):\n",
    "        neg_sub_list =[]\n",
    "\n",
    "        for each in evidence:\n",
    "\n",
    "            neg_sample_numbers = neg_sample_list[idx][1]\n",
    "            neg_line_number = random.choice(neg_sample_numbers)\n",
    "\n",
    "            each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "            neg_line = veri_lines_dict[each]\n",
    "\n",
    "            #while loop to avoid empty list\n",
    "            while len(neg_line) == 0:\n",
    "                neg_line_number = random.choice(neg_sample_numbers)\n",
    "                each = (unicodedata.normalize('NFC',each[0]), neg_line_number)\n",
    "                neg_line = veri_lines_dict[each]\n",
    "\n",
    "            neg_line = neg_line[:-1]\n",
    "            neg_sub_list.append(neg_line)\n",
    "\n",
    "        neg_lines_evidence.append(neg_sub_list)\n",
    "\n",
    "    #tokenise the claims\n",
    "\n",
    "    one_line_tokenised = []\n",
    "\n",
    "    for claim in one_line_claims:\n",
    "        no_stop = claim[:-1]\n",
    "        lower_claim = no_stop.lower()\n",
    "        tokenised = lower_claim.split(' ')\n",
    "        one_line_tokenised.append(tokenised)\n",
    "\n",
    "    #Create verifiable tuples\n",
    "    pos_claim_evidence = []\n",
    "    neg_claim_evidence = []\n",
    "\n",
    "    for idx,evidence in enumerate(actual_lines_evidence):\n",
    "        for each in evidence:\n",
    "            veri_tuples = (one_line_tokenised[idx], each)\n",
    "            pos_claim_evidence.append(veri_tuples)\n",
    "\n",
    "    for idx, evidence in enumerate(neg_lines_evidence):\n",
    "        for each in evidence:\n",
    "            neg_tuples = (one_line_tokenised[idx], each)\n",
    "            neg_claim_evidence.append(neg_tuples)\n",
    "            \n",
    "    return(pos_claim_evidence, neg_claim_evidence)\n",
    "\n",
    "pos_claim_5, neg_claim_5 = pair_maker('data_files/shared_task_dev.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22556/22556 [00:01<00:00, 20585.27it/s]\n",
      "100%|██████████| 22556/22556 [00:00<00:00, 32592.65it/s]\n"
     ]
    }
   ],
   "source": [
    "#Making representation arrays\n",
    "verif_arrays_5 = []\n",
    "\n",
    "for pair in tqdm(pos_claim_5, position = 0, leave = True):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    evidence = pair[1]\n",
    "    evidence_array = np.zeros(50)\n",
    "    for word in evidence:\n",
    "        try:\n",
    "            evidence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, evidence_array])\n",
    "    \n",
    "    verif_arrays_5.append(array_row)\n",
    "    \n",
    "#Making representation arrays\n",
    "neg_arrays_5 = []\n",
    "\n",
    "for pair in tqdm(neg_claim_5, position = 0, leave = True):\n",
    "    claim = pair[0]\n",
    "    claim_array = np.zeros(50)\n",
    "    for word in claim:\n",
    "        try:\n",
    "            claim_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    evidence = pair[1]\n",
    "    evidence_array = np.zeros(50)\n",
    "    for word in evidence:\n",
    "        try:\n",
    "            evidence_array += model[word]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    array_row = np.concatenate([claim_array, evidence_array])\n",
    "    \n",
    "    neg_arrays_5.append(array_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_matrix_5 = np.array(verif_arrays_5)\n",
    "neg_matrix_5 = np.array(neg_arrays_5)\n",
    "rel_y5 = np.ones(len(verif_arrays_5))\n",
    "neg_y5 = np.zeros(len(neg_arrays_5))\n",
    "feature_matrix_5 = np.concatenate((veri_matrix_5,neg_matrix_5), axis=0)\n",
    "target_matrix_5 = np.concatenate((rel_y5,neg_y5), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculting relevant metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_func(feature_matrix, target_matrix, weights):\n",
    "    \n",
    "    one_column = np.ones((feature_matrix.shape[0],1))\n",
    "    feature_matrix = np.concatenate([one_column, feature_matrix], axis = 1)\n",
    "    \n",
    "    lin_pred = feature_matrix@weights\n",
    "    class_pred = 1/ (1 + np.exp(-lin_pred))\n",
    "    class_pred = np.around(class_pred)\n",
    "\n",
    "    score = 0\n",
    "        \n",
    "    for idx in range(0,len(class_pred)):\n",
    "        val = target_matrix[idx]\n",
    "        pred = class_pred[idx]\n",
    "\n",
    "        if val == pred:\n",
    "            score += 1\n",
    "\n",
    "    accuracy = score/len(class_pred)\n",
    "    \n",
    "    return(class_pred, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, acc = accuracy_func(feature_matrix_5, target_matrix_5, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "all_data = len(preds)\n",
    "\n",
    "numer = 0\n",
    "denom = 0\n",
    "for i in range(0, all_data):\n",
    "    if preds[i] == 1:    #Checks if retrieved\n",
    "        denom += 1\n",
    "        \n",
    "    if (preds[i] == 1) and (target_matrix_5[i] == 1):  #Actual correct claims retrieved\n",
    "        numer += 1\n",
    "        \n",
    "precision = numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating recall\n",
    "\n",
    "numer = 0\n",
    "denom = 0\n",
    "for i in range(0, all_data):\n",
    "    if target_matrix_5[i] == 1:    #Total relevant\n",
    "        denom += 1\n",
    "        \n",
    "    if (preds[i] == 1) and (target_matrix_5[i] == 1):  #Correctly retrieved\n",
    "        numer += 1\n",
    "    \n",
    "recall = numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 Score\n",
    "F1 = 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature and target matrices then actual model for 6 and 8 are built on separate files labelled Question 6 and Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper6(filename):\n",
    "    \n",
    "    veri_claims = []\n",
    "    veri_evid = []\n",
    "    veri_label = []\n",
    "\n",
    "\n",
    "    with open(filename) as openfile:\n",
    "                for iline, line in enumerate(openfile.readlines()):\n",
    "\n",
    "                    claim_dic = json.loads(line)\n",
    "                    veri_check = claim_dic['verifiable']\n",
    "\n",
    "                    if veri_check == 'VERIFIABLE':\n",
    "                        claim = claim_dic['claim']\n",
    "                        evidence = claim_dic['evidence']\n",
    "                        label = binary_claim_dic[claim_dic['label']]\n",
    "\n",
    "                        veri_claims.append(claim)\n",
    "                        veri_evid.append(evidence)\n",
    "                        veri_label.append(label)\n",
    "\n",
    "    one_line_claims = []\n",
    "    one_line_evidence = []\n",
    "    one_line_labels = []\n",
    "\n",
    "    for idx, evid in enumerate(veri_evid):\n",
    "        if len(evid[0]) == 1:\n",
    "            one_line_evidence.append(evid)\n",
    "            one_line_claims.append(veri_claims[idx])\n",
    "            one_line_labels.append(veri_label[idx])\n",
    "\n",
    "    clean_evidence = []\n",
    "\n",
    "    for evidence in one_line_evidence:\n",
    "\n",
    "        one_claim_list = []\n",
    "        for each in evidence:\n",
    "            one_evid = each[0]\n",
    "\n",
    "            document = one_evid[2]\n",
    "            line = one_evid[3]\n",
    "\n",
    "            one_claim_list.append((document, line))\n",
    "\n",
    "        clean_evidence.append(one_claim_list)\n",
    "\n",
    "    all_required_dict = {}\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "\n",
    "        key_val = unicodedata.normalize('NFC', evidence[0][0])\n",
    "\n",
    "        for each in evidence:\n",
    "            if key_val in all_required_dict:\n",
    "                all_required_dict[key_val].append(each[1])\n",
    "                all_required_dict[key_val] = list(set(all_required_dict[key_val]))\n",
    "\n",
    "            else:\n",
    "                all_required_dict[key_val] = [each[1]] \n",
    "\n",
    "\n",
    "    #Retrieving just the article titles\n",
    "    all_required_articles = list(all_required_dict.keys())\n",
    "\n",
    "    veri_lines_dict = {}\n",
    "\n",
    "    bracket_dict = {'-lrb-': '(', '-rrb-': ')', '-lsb-': '[', '-rsb-': ']', '-lcb': '{', 'rcb': '}'}\n",
    "\n",
    "    for file in tqdm(list_of_wiki, position = 0, leave = True):\n",
    "            with open('data_files/wiki-pages/wiki-pages/' + file, 'r') as openfile:\n",
    "                    for iline,line in enumerate(openfile.readlines()):\n",
    "                        iD = json.loads(line)['id']\n",
    "\n",
    "                        iD = unicodedata.normalize('NFC',iD)\n",
    "\n",
    "                        if iD in all_required_dict:\n",
    "\n",
    "                            doc_lines = json.loads(line)['lines']\n",
    "\n",
    "                            doc_line_list = doc_lines.split('\\n')\n",
    "\n",
    "                            number_of_lines[iD] = len(doc_line_list)\n",
    "\n",
    "                            for line_number in range(0, len(doc_line_list)):\n",
    "                                doc_line = doc_line_list[line_number].lower()\n",
    "\n",
    "                                #Isolating cleaned up version of line\n",
    "                                doc_line = doc_line.split('\\t')[1]\n",
    "                                doc_line = doc_line.split('.')[0]\n",
    "\n",
    "                                doc_line = doc_line.split(' ')\n",
    "\n",
    "                                #Sorting out the brackets\n",
    "                                doc_line_brac = []\n",
    "                                for word in doc_line:\n",
    "\n",
    "                                    if word in set(list(bracket_dict.keys())):\n",
    "                                        word = bracket_dict[word]\n",
    "\n",
    "                                    doc_line_brac.append(word)\n",
    "\n",
    "                                veri_lines_dict[(iD, line_number)] = doc_line_brac\n",
    "\n",
    "    actual_lines_evidence = []\n",
    "\n",
    "    for evidence in clean_evidence:\n",
    "        sub_list =[]\n",
    "\n",
    "        for each in evidence:\n",
    "            each = (unicodedata.normalize('NFC',each[0]), each[1])\n",
    "\n",
    "            actual_line = veri_lines_dict[each]\n",
    "            actual_line = actual_line[:-1]\n",
    "            sub_list.append(actual_line)\n",
    "\n",
    "        actual_lines_evidence.append(sub_list)\n",
    "\n",
    "    one_line_tokenised = []\n",
    "\n",
    "    #Claims have a lot of 's so need modification to claim\n",
    "    for claim in one_line_claims:\n",
    "        no_stop = claim[:-1]\n",
    "        lower_claim = no_stop.lower()\n",
    "        tokenised = lower_claim.split(' ')\n",
    "\n",
    "        one_line_tokenised.append(tokenised)\n",
    "\n",
    "    clean_claims = []\n",
    "    for claim in one_line_claims:   \n",
    "        claim = claim.lower()\n",
    "        no_stop = claim[:-1]\n",
    "        claim = claim.split(' ')\n",
    "\n",
    "        no_apos_claim = []\n",
    "        #Removing apostrophe for vector representation later\n",
    "        for word in claim:\n",
    "            if \"'\" in word:\n",
    "                no_apos_claim.append(word[:-2])\n",
    "                no_apos_claim.append(\"'s\")\n",
    "\n",
    "            else:\n",
    "                no_apos_claim.append(word)\n",
    "\n",
    "        clean_claims.append(no_apos_claim)\n",
    "\n",
    "    clean_claims = []\n",
    "    for claim in one_line_claims:   \n",
    "        claim = claim.lower()\n",
    "        no_stop = claim[:-1]\n",
    "        claim = claim.split(' ')\n",
    "\n",
    "        no_apos_claim = []\n",
    "        #Removing apostrophe for vector representation later\n",
    "        for word in claim:\n",
    "            if \"'\" in word:\n",
    "                no_apos_claim.append(word[:-2])\n",
    "                no_apos_claim.append(\"'s\")\n",
    "\n",
    "            else:\n",
    "                no_apos_claim.append(word)\n",
    "\n",
    "        clean_claims.append(no_apos_claim)\n",
    "\n",
    "    pair_claims = []\n",
    "    pair_evid = []\n",
    "    pair_label = []\n",
    "\n",
    "    for idx, sentences in enumerate(actual_lines_evidence):\n",
    "        for sentence in sentences:\n",
    "            pair_evid.append(sentence)\n",
    "            pair_claims.append(clean_claims[idx])\n",
    "            pair_label.append(one_line_labels[idx])\n",
    "            \n",
    "    return(pair_claims, pair_evid, pair_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:53<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "claims6, evid6, label6 = mapper6('data_files/train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(claims6, open(\"pkl_files/claims6.p\", \"wb\"))\n",
    "pickle.dump(evid6, open(\"pkl_files/evid6.p\", \"wb\"))\n",
    "pickle.dump(label6, open(\"pkl_files/label6.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:53<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "claimstest, evidtest, labeltest = mapper6('data_files/shared_task_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(claimstest, open(\"pkl_files/claimstest.p\", \"wb\"))\n",
    "pickle.dump(evidtest, open(\"pkl_files/evidtest.p\", \"wb\"))\n",
    "pickle.dump(labeltest, open(\"pkl_files/labeltest.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See files Question 6 and 8 for continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Code to make json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_extractor(filename):\n",
    "    \n",
    "    veri_id = []\n",
    "    veri_evid = []\n",
    "\n",
    "\n",
    "    with open(filename) as openfile:\n",
    "                for iline, line in enumerate(openfile.readlines()):\n",
    "\n",
    "                    claim_dic = json.loads(line)\n",
    "                    veri_check = claim_dic['verifiable']\n",
    "\n",
    "                    if veri_check == 'VERIFIABLE':\n",
    "                        veri_id.append(claim_dic['id'])\n",
    "                        veri_evid.append(claim_dic['evidence'])\n",
    "                        \n",
    "\n",
    "    \n",
    "    one_line_evidence = []\n",
    "    one_line_id = []\n",
    "\n",
    "    for idx, evid in enumerate(veri_evid):\n",
    "        if len(evid[0]) == 1:\n",
    "            one_line_evidence.append(evid)\n",
    "            one_line_id.append(veri_id[idx])\n",
    "            \n",
    "    clean_evidence = []\n",
    "\n",
    "    for evidence in one_line_evidence:\n",
    "\n",
    "        one_claim_list = []\n",
    "        for each in evidence:\n",
    "            one_evid = each[0]\n",
    "\n",
    "            document = one_evid[2]\n",
    "            line = one_evid[3]\n",
    "\n",
    "            one_claim_list.append((document, line))\n",
    "\n",
    "        clean_evidence.append(one_claim_list)\n",
    "        \n",
    "    pair_evidence = []\n",
    "    pair_iD = []\n",
    "\n",
    "    for idx, evidences in enumerate(clean_evidence):\n",
    "        for evid in evidences:\n",
    "            pair_evidence.append(evid)\n",
    "            pair_iD.append(one_line_id[idx])\n",
    "            \n",
    "    return(pair_evidence, pair_iD)\n",
    "                        \n",
    "json_evidence, json_iD= json_extractor('data_files/shared_task_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pickle.load( open( \"pkl_files/final_predictions.p\", \"rb\" ) )\n",
    "bo_final_pred = ['SUPPORTS' if x == True else \"REFUTES\" for x in final_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dics = []\n",
    "\n",
    "for idx,ID in enumerate(json_iD):\n",
    "    temp_dic = {}\n",
    "    temp_dic[\"id\"] = json_iD[idx]\n",
    "    temp_dic[\"predicted_label\"] = bo_final_pred[idx]\n",
    "    temp_dic[\"evidence\"] = json_evidence[idx]\n",
    "    \n",
    "    list_of_dics.append(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dics = []\n",
    "\n",
    "for i in range(0, len(list_of_dics)):\n",
    "    if list_of_dics[i] not in list_of_dics[i+1:]:\n",
    "        unique_dics.append(list_of_dics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"predictions.jsonl\"\n",
    "\n",
    "json_objects = []\n",
    "\n",
    "for dic in unique_dics:\n",
    "    json_objects.append(json.dumps(dic))\n",
    "line = \"\\n\".join(json_objects)\n",
    "\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
