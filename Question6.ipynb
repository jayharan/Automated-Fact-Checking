{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import nltk\n",
    "import tqdm\n",
    "import itertools\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import os.path\n",
    "import unicodedata\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, LSTM, Multiply\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Add, Subtract\n",
    "import keras_metrics\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "modelw = loadGloveModel('glove.6B.50D.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code cell to remove full stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims6 = pickle.load(open( \"pkl_files/claims6.p\", \"rb\" ))\n",
    "evid6 = pickle.load(open( \"pkl_files/evid6.p\", \"rb\" ))\n",
    "label6 = pickle.load(open( \"pkl_files/label6.p\", \"rb\" ))\n",
    "\n",
    "claimstest = pickle.load(open( \"pkl_files/claimstest.p\", \"rb\" ))\n",
    "evidtest = pickle.load(open( \"pkl_files/evidtest.p\", \"rb\" ))\n",
    "labeltest = pickle.load(open( \"pkl_files/labeltest.p\", \"rb\" ))\n",
    "\n",
    "claims_temp = []\n",
    "\n",
    "for claim in claims6:\n",
    "    last_word = claim[-1]\n",
    "    temp_claim = claim[:-1]\n",
    "    temp_claim.append(last_word[:-1])\n",
    "    claims_temp.append(temp_claim)\n",
    "    \n",
    "claims6 = claims_temp\n",
    "\n",
    "claims_temp = []\n",
    "\n",
    "for claim in claimstest:\n",
    "    last_word = claim[-1]\n",
    "    temp_claim = claim[:-1]\n",
    "    temp_claim.append(last_word[:-1])\n",
    "    claims_temp.append(temp_claim)\n",
    "    \n",
    "claimstest = claims_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_maker(claims, evids):\n",
    "    \n",
    "    claim_arrays = []\n",
    "    evid_arrays = []\n",
    "\n",
    "    for claim in claims:\n",
    "        claim_array = np.zeros(50)\n",
    "        for word in claim:\n",
    "            try:\n",
    "                claim_array += modelw[word] \n",
    "            except:\n",
    "                pass\n",
    "        claim_arrays.append(claim_array)\n",
    "\n",
    "    for evid in evids:\n",
    "        evid_array = np.zeros(50)\n",
    "        for word in evid:\n",
    "            try:\n",
    "                evid_array += modelw[word]     \n",
    "            except:\n",
    "                pass\n",
    "        evid_arrays.append(evid_array)\n",
    "        \n",
    "    claim_array = np.array(claim_arrays)\n",
    "    evid_array = np.array(evid_arrays)\n",
    "    bias = np.ones((len(claim_arrays), 1))\n",
    "    \n",
    "    input_matrix = np.concatenate([bias,claim_array, evid_array], axis = 1)\n",
    "    \n",
    "    return(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_train = rep_maker(claims6, evid6)\n",
    "claim_test = rep_maker(claimstest, evidtest)\n",
    "y_train = label6\n",
    "y_test = labeltest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 158011 samples, validate on 39503 samples\n",
      "Epoch 1/10\n",
      "158011/158011 [==============================] - 33s 207us/step - loss: 0.5497 - acc: 0.7406 - precision: 0.7513 - recall: 0.9559 - val_loss: 0.5207 - val_acc: 0.7523 - val_precision: 0.7676 - val_recall: 0.9442\n",
      "Epoch 2/10\n",
      "158011/158011 [==============================] - 33s 207us/step - loss: 0.5034 - acc: 0.7645 - precision: 0.7716 - recall: 0.9557 - val_loss: 0.5062 - val_acc: 0.7661 - val_precision: 0.7863 - val_recall: 0.9300\n",
      "Epoch 3/10\n",
      "158011/158011 [==============================] - 33s 212us/step - loss: 0.4833 - acc: 0.7764 - precision: 0.7838 - recall: 0.9516 - val_loss: 0.4991 - val_acc: 0.7692 - val_precision: 0.7986 - val_recall: 0.9114\n",
      "Epoch 4/10\n",
      "158011/158011 [==============================] - 33s 209us/step - loss: 0.4702 - acc: 0.7841 - precision: 0.7911 - recall: 0.9510 - val_loss: 0.4923 - val_acc: 0.7733 - val_precision: 0.7792 - val_recall: 0.9589\n",
      "Epoch 5/10\n",
      "158011/158011 [==============================] - 33s 209us/step - loss: 0.4592 - acc: 0.7906 - precision: 0.7976 - recall: 0.9500 - val_loss: 0.4894 - val_acc: 0.7756 - val_precision: 0.7923 - val_recall: 0.9357\n",
      "Epoch 6/10\n",
      "158011/158011 [==============================] - 51s 325us/step - loss: 0.4508 - acc: 0.7955 - precision: 0.8025 - recall: 0.9495 - val_loss: 0.4856 - val_acc: 0.7777 - val_precision: 0.7855 - val_recall: 0.9536\n",
      "Epoch 7/10\n",
      "158011/158011 [==============================] - 69s 435us/step - loss: 0.4422 - acc: 0.8010 - precision: 0.8075 - recall: 0.9498 - val_loss: 0.4957 - val_acc: 0.7748 - val_precision: 0.7850 - val_recall: 0.9492\n",
      "Epoch 8/10\n",
      "158011/158011 [==============================] - 69s 434us/step - loss: 0.4349 - acc: 0.8055 - precision: 0.8123 - recall: 0.9490 - val_loss: 0.4922 - val_acc: 0.7735 - val_precision: 0.7880 - val_recall: 0.9405\n",
      "Epoch 9/10\n",
      "158011/158011 [==============================] - 69s 434us/step - loss: 0.4287 - acc: 0.8072 - precision: 0.8139 - recall: 0.9489 - val_loss: 0.4934 - val_acc: 0.7800 - val_precision: 0.7897 - val_recall: 0.9492\n",
      "Epoch 10/10\n",
      "158011/158011 [==============================] - 69s 436us/step - loss: 0.4239 - acc: 0.8111 - precision: 0.8174 - recall: 0.9494 - val_loss: 0.4993 - val_acc: 0.7803 - val_precision: 0.7923 - val_recall: 0.9444\n",
      "197514/197514 [==============================] - 14s 68us/step\n"
     ]
    }
   ],
   "source": [
    "X = claim_train\n",
    "Y = y_train\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(101, input_dim=101, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras_metrics.precision(), keras_metrics.recall()])\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=10, batch_size=10, validation_split=0.2)\n",
    "# evaluate the model\n",
    "scores_train = model.evaluate(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197514/197514 [==============================] - 14s 70us/step\n",
      "22556/22556 [==============================] - 2s 70us/step\n"
     ]
    }
   ],
   "source": [
    "scores_train = model.evaluate(X, Y)\n",
    "scores_test = model.evaluate(claim_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43222101540628366, 0.8079781686361474, 0.810673254359896, 0.956980331254993]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7722183164499143,\n",
       " 0.6130076254496529,\n",
       " 0.5678718337271711,\n",
       " 0.9337011657832721]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
